## Contents

move to sidebar

hide

- [(Top)](#)
- [1 List](#List)
- [2 Timeline](#Timeline)
- [3 See also](#See_also)
- [4 Notes](#Notes)
- [5 References](#References)

Toggle the table of contents

# List of large language models

4 languages

- [العربية](https://ar.wikipedia.org/wiki/%D9%82%D8%A7%D8%A6%D9%85%D8%A9_%D8%A7%D9%84%D9%86%D9%85%D8%A7%D8%B0%D8%AC_%D8%A7%D9%84%D9%84%D8%BA%D9%88%D9%8A%D8%A9_%D8%A7%D9%84%D9%83%D8%A8%D9%8A%D8%B1%D8%A9)
- [Français](https://fr.wikipedia.org/wiki/Liste_de_grands_mod%C3%A8les_de_langage)
- [Српски / srpski](https://sr.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B8%D1%81%D0%B0%D0%BA_%D0%B2%D0%B5%D0%BB%D0%B8%D0%BA%D0%B8%D1%85_%D1%98%D0%B5%D0%B7%D0%B8%D1%87%D0%BA%D0%B8%D1%85_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B0)
- [中文](https://zh.wikipedia.org/wiki/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%88%97%E8%A1%A8)

[Edit links](https://www.wikidata.org/wiki/Special:EntityPage/Q131598147#sitelinks-wikipedia)

- [Article](/wiki/List_of_large_language_models)
- [Talk](/wiki/Talk:List_of_large_language_models)

English

- [Read](/wiki/List_of_large_language_models)
- [Edit](/w/index.php?title=List_of_large_language_models&action=edit)
- [View history](/w/index.php?title=List_of_large_language_models&action=history)

Tools

Tools

move to sidebar

hide

Actions

- [Read](/wiki/List_of_large_language_models)
- [Edit](/w/index.php?title=List_of_large_language_models&action=edit)
- [View history](/w/index.php?title=List_of_large_language_models&action=history)

General

- [What links here](/wiki/Special:WhatLinksHere/List_of_large_language_models)
- [Related changes](/wiki/Special:RecentChangesLinked/List_of_large_language_models)
- [Upload file](https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard)
- [Permanent link](/w/index.php?title=List_of_large_language_models&oldid=1325091032)
- [Page information](/w/index.php?title=List_of_large_language_models&action=info)
- [Cite this page](/w/index.php?title=Special:CiteThisPage&page=List_of_large_language_models&id=1325091032&wpFormIdentifier=titleform)
- [Get shortened URL](/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FList_of_large_language_models)
- [Download QR code](/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FList_of_large_language_models)

Print/export

- [Download as PDF](/w/index.php?title=Special:DownloadAsPdf&page=List_of_large_language_models&action=show-download-screen)
- [Printable version](/w/index.php?title=List_of_large_language_models&printable=yes)

In other projects

- [Wikidata item](https://www.wikidata.org/wiki/Special:EntityPage/Q131598147)

Appearance

move to sidebar

hide

From Wikipedia, the free encyclopedia

This is a [dynamic list](/wiki/Wikipedia:WikiProject_Lists#Dynamic_lists) and may never be able to satisfy particular standards for completeness. You can help by [editing the page](/wiki/Special:EditPage/List_of_large_language_models) to add missing items, with references to [reliable sources](/wiki/Wikipedia:Reliable_sources) .

A [large language model](/wiki/Large_language_model) (LLM) is a type of [machine learning](/wiki/Machine_learning) [model](/wiki/Model#Conceptual_model) designed for [natural language processing](/wiki/Natural_language_processing) tasks such as language [generation](/wiki/Generative_artificial_intelligence) . LLMs are [language models](/wiki/Language_model) with many parameters, and are trained with [self-supervised learning](/wiki/Self-supervised_learning) on a vast amount of text.

## List

[ [edit](/w/index.php?title=List_of_large_language_models&action=edit&section=1) ]

For the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.

| Name                                                                   | Release date  [[ a ]](#cite_note-1)         | Developer                                                                                                          | Number of parameters (billion)  [[ b ]](#cite_note-2)                                 | Corpus size                                                                                                                                                | Training cost (petaFLOP- day)           | License  [[ c ]](#cite_note-3)                                                        | Notes                                                                                                                                                                                                                                                                                                           |
|------------------------------------------------------------------------|---------------------------------------------|--------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------|---------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [GPT-1](/wiki/GPT-1)                                                   | June 11, 2018                               | [OpenAI](/wiki/OpenAI)                                                                                             | 0.117                                                                                 | Unknown                                                                                                                                                    | 1  [[ 1 ]](#cite_note-oai-unsup-4)      | [MIT](/wiki/MIT_License)  [[ 2 ]](#cite_note-5)                                       | [GPUs](/wiki/Graphics_processing_unit)  First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 .  [[ 3 ]](#cite_note-6)                                                                                                                                                                       |
| [BERT](/wiki/BERT_(language_model))                                    | October 2018                                | [Google](/wiki/Google)                                                                                             | 0.340  [[ 4 ]](#cite_note-bert-paper-7)                                               | 3.3 billion words  [[ 4 ]](#cite_note-bert-paper-7)                                                                                                        | 9  [[ 5 ]](#cite_note-bHZJ2-8)          | [Apache 2.0](/wiki/Apache_2.0)  [[ 6 ]](#cite_note-bert-web-9)                        | An early and influential language model.  [[ 7 ]](#cite_note-Manning-2022-10)  [Encoder-only](/wiki/Transformer_(deep_learning_architecture)#encoder-only)  and thus not built to be prompted or generative.  [[ 8 ]](#cite_note-Ir545-11)  Training took 4 days on 64 TPUv2 chips.  [[ 9 ]](#cite_note-:02-12) |
| [T5](/wiki/T5_(language_model))                                        | October 2019                                | [Google](/wiki/Google)                                                                                             | 11  [[ 10 ]](#cite_note-:6-13)                                                        | 34 billion tokens  [[ 10 ]](#cite_note-:6-13)                                                                                                              |                                         | [Apache 2.0](/wiki/Apache_2.0)  [[ 11 ]](#cite_note-14)                               | Base model for many Google projects, such as Imagen.  [[ 12 ]](#cite_note-15)                                                                                                                                                                                                                                   |
| [XLNet](/wiki/XLNet)                                                   | June 2019                                   | [Google](/wiki/Google)                                                                                             | 0.340  [[ 13 ]](#cite_note-16)                                                        | 33 billion words                                                                                                                                           | 330                                     | [Apache 2.0](/wiki/Apache_2.0)  [[ 14 ]](#cite_note-xlnet-17)                         | An alternative to BERT; designed as encoder-only. Trained on 512 TPU v3 chips for 5.5 days.  [[ 15 ]](#cite_note-LX3rI-18)                                                                                                                                                                                      |
| [GPT-2](/wiki/GPT-2)                                                   | February 2019                               | [OpenAI](/wiki/OpenAI)                                                                                             | 1.5  [[ 16 ]](#cite_note-15Brelease-19)                                               | 40GB  [[ 17 ]](#cite_note-5T8u5-20)  (~ 10 billion tokens)  [[ 18 ]](#cite_note-LambdaLabs-21)                                                             | 28  [[ 19 ]](#cite_note-:10-22)         | [MIT](/wiki/MIT_License)  [[ 20 ]](#cite_note-Sudbe-23)                               | Trained on 32 TPUv3 chips for 1 week.  [[ 19 ]](#cite_note-:10-22)                                                                                                                                                                                                                                              |
| [GPT-3](/wiki/GPT-3)                                                   | May 2020                                    | [OpenAI](/wiki/OpenAI)                                                                                             | 175  [[ 21 ]](#cite_note-Wiggers-24)                                                  | 300 billion tokens  [[ 18 ]](#cite_note-LambdaLabs-21)                                                                                                     | 3640  [[ 22 ]](#cite_note-:2-25)        | [Proprietary](/wiki/Proprietary_software)                                             | [ChatGPT](/wiki/ChatGPT)  A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called in 2022.  [[ 23 ]](#cite_note-chatgpt-blog-26)                                                                                                                         |
| GPT-Neo                                                                | March 2021                                  | [EleutherAI](/wiki/EleutherAI)                                                                                     | 2.7  [[ 24 ]](#cite_note-gpt-neo-27)                                                  | 825 GiB  [[ 25 ]](#cite_note-Pile-28)                                                                                                                      | Unknown                                 | [MIT](/wiki/MIT_License)  [[ 26 ]](#cite_note-vb-gpt-neo-29)                          | [a series of free GPT-3 alternatives](/wiki/EleutherAI#GPT_models)  The first of released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.  [[ 26 ]](#cite_note-vb-gpt-neo-29)                                        |
| [GPT-J](/wiki/GPT-J)                                                   | June 2021                                   | [EleutherAI](/wiki/EleutherAI)                                                                                     | 6  [[ 27 ]](#cite_note-JxohJ-30)                                                      | 825 GiB  [[ 25 ]](#cite_note-Pile-28)                                                                                                                      | 200  [[ 28 ]](#cite_note-:3-31)         | [Apache 2.0](/wiki/Apache_2.0)                                                        | GPT-3-style language model                                                                                                                                                                                                                                                                                      |
| Megatron-Turing NLG                                                    | October 2021  [[ 29 ]](#cite_note-BwnW5-32) | [Microsoft](/wiki/Microsoft)  [Nvidia](/wiki/Nvidia)  and                                                          | 530  [[ 30 ]](#cite_note-mtnlg-preprint-33)                                           | 338.6 billion tokens  [[ 30 ]](#cite_note-mtnlg-preprint-33)                                                                                               | 38000  [[ 31 ]](#cite_note-:11-34)      | Unreleased                                                                            | [Selene Supercomputer](/wiki/Selene_(supercomputer))  Trained for 3 months on over 2000 A100 GPUs on the NVIDIA , for over 3 million GPU-hours  [[ 31 ]](#cite_note-:11-34)                                                                                                                                     |
| Ernie 3.0 Titan                                                        | December 2021                               | [Baidu](/wiki/Baidu)                                                                                               | 260  [[ 32 ]](#cite_note-qeOB8-35)                                                    | 4TB                                                                                                                                                        | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | [Ernie Bot](/wiki/Ernie_Bot)  Chinese-language LLM. is based on this model.                                                                                                                                                                                                                                     |
| [Claude](/wiki/Claude_(language_model))  [[ 33 ]](#cite_note-i8jc4-36) | December 2021                               | [Anthropic](/wiki/Anthropic)                                                                                       | 52  [[ 34 ]](#cite_note-AnthroArch-37)                                                | 400 billion tokens  [[ 34 ]](#cite_note-AnthroArch-37)                                                                                                     | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Fine-tuned for desirable behavior in conversations.  [[ 35 ]](#cite_note-RZqhw-38)                                                                                                                                                                                                                              |
| GLaM (Generalist Language Model)                                       | December 2021                               | [Google](/wiki/Google)                                                                                             | 1200  [[ 36 ]](#cite_note-glam-blog-39)                                               | 1.6 trillion tokens  [[ 36 ]](#cite_note-glam-blog-39)                                                                                                     | 5600  [[ 36 ]](#cite_note-glam-blog-39) | [Proprietary](/wiki/Proprietary_software)                                             | [mixture of experts](/wiki/Mixture_of_experts)  Sparse model, making it more expensive to train but cheaper to run inference compared to GPT-3.                                                                                                                                                                 |
| Gopher                                                                 | December 2021                               | [DeepMind](/wiki/DeepMind)                                                                                         | 280  [[ 37 ]](#cite_note-mD5eE-40)                                                    | 300 billion tokens  [[ 38 ]](#cite_note-hoffman-41)                                                                                                        | 5833  [[ 39 ]](#cite_note-:4-42)        | [Proprietary](/wiki/Proprietary_software)                                             | Later developed into the Chinchilla model.                                                                                                                                                                                                                                                                      |
| [LaMDA](/wiki/LaMDA)  (Language Models for Dialog Applications)        | January 2022                                | [Google](/wiki/Google)                                                                                             | 137  [[ 40 ]](#cite_note-lamda-blog-43)                                               | 1.56T words,  [[ 40 ]](#cite_note-lamda-blog-43)  168 billion tokens  [[ 38 ]](#cite_note-hoffman-41)                                                      | 4110  [[ 41 ]](#cite_note-DMs9Z-44)     | [Proprietary](/wiki/Proprietary_software)                                             | Specialized for response generation in conversations.                                                                                                                                                                                                                                                           |
| GPT-NeoX                                                               | February 2022                               | [EleutherAI](/wiki/EleutherAI)                                                                                     | 20  [[ 42 ]](#cite_note-gpt-neox-20b-45)                                              | 825 GiB  [[ 25 ]](#cite_note-Pile-28)                                                                                                                      | 740  [[ 28 ]](#cite_note-:3-31)         | [Apache 2.0](/wiki/Apache_2.0)                                                        | based on the Megatron architecture                                                                                                                                                                                                                                                                              |
| [Chinchilla](/wiki/Chinchilla_AI)                                      | March 2022                                  | [DeepMind](/wiki/DeepMind)                                                                                         | 70  [[ 43 ]](#cite_note-chinchilla-blog-46)                                           | 1.4 trillion tokens  [[ 43 ]](#cite_note-chinchilla-blog-46)  [[ 38 ]](#cite_note-hoffman-41)                                                              | 6805  [[ 39 ]](#cite_note-:4-42)        | [Proprietary](/wiki/Proprietary_software)                                             | [Sparrow](/wiki/Sparrow_(bot))  [neural scaling law](/wiki/Neural_scaling_law)  Reduced-parameter model trained on more data. Used in the bot. Often cited for its .                                                                                                                                            |
| [PaLM](/wiki/PaLM)  (Pathways Language Model)                          | April 2022                                  | [Google](/wiki/Google)                                                                                             | 540  [[ 44 ]](#cite_note-palm-blog-47)                                                | 768 billion tokens  [[ 43 ]](#cite_note-chinchilla-blog-46)                                                                                                | 29,250  [[ 39 ]](#cite_note-:4-42)      | [Proprietary](/wiki/Proprietary_software)                                             | [TPU v4](/wiki/Tensor_Processing_Unit)  Trained for ~60 days on ~6000 chips.  [[ 39 ]](#cite_note-:4-42)                                                                                                                                                                                                        |
| OPT (Open Pretrained Transformer)                                      | May 2022                                    | [Meta](/wiki/Meta_Platforms)                                                                                       | 175  [[ 45 ]](#cite_note-jlof8-48)                                                    | 180 billion tokens  [[ 46 ]](#cite_note-QjTIc-49)                                                                                                          | 310  [[ 28 ]](#cite_note-:3-31)         | Non-commercial research  [[ d ]](#cite_note-50)                                       | GPT-3 architecture with some adaptations from Megatron. Uniquely, the training logbook written by the team was published.  [[ 47 ]](#cite_note-51)                                                                                                                                                              |
| YaLM 100B                                                              | June 2022                                   | [Yandex](/wiki/Yandex)                                                                                             | 100  [[ 48 ]](#cite_note-yalm-repo-52)                                                | 1.7TB  [[ 48 ]](#cite_note-yalm-repo-52)                                                                                                                   | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        | English-Russian model based on Microsoft's Megatron-LM                                                                                                                                                                                                                                                          |
| Minerva                                                                | June 2022                                   | [Google](/wiki/Google)                                                                                             | 540  [[ 49 ]](#cite_note-minerva-paper-53)                                            | 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server  [[ 49 ]](#cite_note-minerva-paper-53) | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | For solving "mathematical and scientific questions using step-by-step reasoning".  [[ 50 ]](#cite_note-FfCNK-54)  Initialized from PaLM models, then finetuned on mathematical and scientific data.                                                                                                             |
| [BLOOM](/wiki/BLOOM_(language_model))                                  | July 2022                                   | [Hugging Face](/wiki/Hugging_Face)  Large collaboration led by                                                     | 175  [[ 51 ]](#cite_note-bigger-better-55)                                            | 350 billion tokens (1.6TB)  [[ 52 ]](#cite_note-B8wB2-56)                                                                                                  | Unknown                                 | Responsible AI                                                                        | Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)                                                                                                                                                                                                           |
| Galactica                                                              | November 2022                               | [Meta](/wiki/Meta_Platforms)                                                                                       | 120                                                                                   | 106 billion tokens  [[ 53 ]](#cite_note-37sY6-57)                                                                                                          | Unknown                                 | CC-BY-NC-4.0                                                                          | Trained on scientific text and modalities.                                                                                                                                                                                                                                                                      |
| AlexaTM (Teacher Models)                                               | November 2022                               | [Amazon](/wiki/Amazon_(company))                                                                                   | 20  [[ 54 ]](#cite_note-u5szh-58)                                                     | 1.3 trillion  [[ 55 ]](#cite_note-HaA7l-59)                                                                                                                | Unknown                                 | [Proprietary](/wiki/Proprietary_software)  [[ 56 ]](#cite_note-rpehM-60)              | Bidirectional sequence-to-sequence architecture                                                                                                                                                                                                                                                                 |
| [Llama](/wiki/Llama_(language_model))                                  | February 2023                               | [Meta AI](/wiki/Meta_AI)                                                                                           | 65  [[ 57 ]](#cite_note-llama-blog-61)                                                | 1.4 trillion  [[ 57 ]](#cite_note-llama-blog-61)                                                                                                           | 6300  [[ 58 ]](#cite_note-:5-62)        | Non-commercial research  [[ e ]](#cite_note-63)                                       | [Chinchilla scaling law](/wiki/Chinchilla_(language_model))  Corpus has 20 languages. "Overtrained" (compared to ) for better performance with fewer parameters.  [[ 57 ]](#cite_note-llama-blog-61)                                                                                                            |
| [GPT-4](/wiki/GPT-4)                                                   | March 2023                                  | [OpenAI](/wiki/OpenAI)                                                                                             | Unknown  [[ f ]](#cite_note-65)  (According to rumors: 1760)  [[ 60 ]](#cite_note-66) | Unknown                                                                                                                                                    | Unknown,  estimated 230,000             | [Proprietary](/wiki/Proprietary_software)                                             | [several products](/wiki/GPT-4#Usage)  Available for all ChatGPT users now and used in .                                                                                                                                                                                                                        |
| Cerebras-GPT                                                           | March 2023                                  | [Cerebras](/wiki/Cerebras)                                                                                         | 13  [[ 61 ]](#cite_note-D0k2a-67)                                                     |                                                                                                                                                            | 270  [[ 28 ]](#cite_note-:3-31)         | [Apache 2.0](/wiki/Apache_2.0)                                                        | [Chinchilla formula](/wiki/Chinchilla_(language_model))  Trained with .                                                                                                                                                                                                                                         |
| Falcon                                                                 | March 2023                                  | [Technology Innovation Institute](/wiki/Technology_Innovation_Institute)                                           | 40  [[ 62 ]](#cite_note-falcon-68)                                                    | 1 trillion tokens, from RefinedWeb (filtered web text corpus)  [[ 63 ]](#cite_note-Xb1gq-69)  plus some "curated corpora".  [[ 64 ]](#cite_note-gzTNw-70)  | 2800  [[ 58 ]](#cite_note-:5-62)        | [Apache 2.0](/wiki/Apache_2.0)  [[ 65 ]](#cite_note-Wmlcs-71)                         |                                                                                                                                                                                                                                                                                                                 |
| BloombergGPT                                                           | March 2023                                  | [Bloomberg L.P.](/wiki/Bloomberg_L.P.)                                                                             | 50                                                                                    | 363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets  [[ 66 ]](#cite_note-nGOSu-72)          | Unknown                                 | Unreleased                                                                            | Trained on financial data from proprietary sources, for financial tasks                                                                                                                                                                                                                                         |
| [PanGu-Σ](/wiki/Huawei_PanGu)                                          | March 2023                                  | [Huawei](/wiki/Huawei)                                                                                             | 1085                                                                                  | 329 billion tokens  [[ 67 ]](#cite_note-9WSFw-73)                                                                                                          | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             |                                                                                                                                                                                                                                                                                                                 |
| OpenAssistant  [[ 68 ]](#cite_note-JiOl8-74)                           | March 2023                                  | [LAION](/wiki/LAION)                                                                                               | 17                                                                                    | 1.5 trillion tokens                                                                                                                                        | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        | Trained on crowdsourced open data                                                                                                                                                                                                                                                                               |
| Jurassic-2  [[ 69 ]](#cite_note-75)                                    | March 2023                                  | [AI21 Labs](/wiki/AI21_Labs)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Multilingual  [[ 70 ]](#cite_note-76)                                                                                                                                                                                                                                                                           |
| [PaLM 2](/wiki/PaLM)  (Pathways Language Model 2)                      | May 2023                                    | [Google](/wiki/Google)                                                                                             | 340  [[ 71 ]](#cite_note-cnbc-20230516-77)                                            | 3.6 trillion tokens  [[ 71 ]](#cite_note-cnbc-20230516-77)                                                                                                 | 85,000  [[ 58 ]](#cite_note-:5-62)      | [Proprietary](/wiki/Proprietary_software)                                             | [Bard chatbot](/wiki/Bard_(chatbot))  Was used in .  [[ 72 ]](#cite_note-pWyLA-78)                                                                                                                                                                                                                              |
| [Llama 2](/wiki/Llama_(language_model)#Llama_2)                        | July 2023                                   | [Meta AI](/wiki/Meta_AI)                                                                                           | 70  [[ 73 ]](#cite_note-meta-20230719-79)                                             | 2 trillion tokens  [[ 73 ]](#cite_note-meta-20230719-79)                                                                                                   | 21,000                                  | [Llama 2 license](/wiki/Llama_(language_model)#Licensing)                             | 1.7 million A100-hours.  [[ 74 ]](#cite_note-80)                                                                                                                                                                                                                                                                |
| [Claude 2](/wiki/Claude_(language_model))                              | July 2023                                   | [Anthropic](/wiki/Anthropic)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Used in Claude chatbot.  [[ 75 ]](#cite_note-81)                                                                                                                                                                                                                                                                |
| [Granite 13b](/wiki/IBM_Granite)                                       | July 2023                                   | [IBM](/wiki/IBM)                                                                                                   | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | [IBM Watsonx](/wiki/IBM_Watsonx)  Used in .  [[ 76 ]](#cite_note-82)                                                                                                                                                                                                                                            |
| [Mistral 7B](/wiki/Mistral_7B)                                         | September 2023                              | [Mistral AI](/wiki/Mistral_AI)                                                                                     | 7.3  [[ 77 ]](#cite_note-mistral-20230927-83)                                         | Unknown                                                                                                                                                    | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        |                                                                                                                                                                                                                                                                                                                 |
| [Claude 2.1](/wiki/Claude_2.1)                                         | November 2023                               | [Anthropic](/wiki/Anthropic)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.  [[ 78 ]](#cite_note-84)                                                                                                                                                                                                         |
| [Grok 1](/wiki/Grok_1)  [[ 79 ]](#cite_note-85)                        | November 2023                               | [xAI](/wiki/XAI_(company))                                                                                         | 314                                                                                   | Unknown                                                                                                                                                    | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        | [Grok](/wiki/Grok_(chatbot))  Used in chatbot. Grok 1 has a context length of 8,192 tokens and has access to X (Twitter).  [[ 80 ]](#cite_note-86)                                                                                                                                                              |
| [Gemini 1.0](/wiki/Gemini_(language_model))                            | December 2023                               | [Google DeepMind](/wiki/Google_DeepMind)                                                                           | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | [the chatbot of the same name](/wiki/Gemini_(chatbot))  Multimodal model, comes in three sizes. Used in .  [[ 81 ]](#cite_note-87)                                                                                                                                                                              |
| [Mixtral 8x7B](/wiki/Mixtral_8x7B)                                     | December 2023                               | [Mistral AI](/wiki/Mistral_AI)                                                                                     | 46.7                                                                                  | Unknown                                                                                                                                                    | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        | Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.  [[ 82 ]](#cite_note-88)  [Mixture of experts](/wiki/Mixture_of_experts)  model, with 12.9 billion parameters activated per token.  [[ 83 ]](#cite_note-89)                                                                                             |
| [DeepSeek-LLM](/wiki/DeepSeek-LLM)                                     | November 29, 2023                           | [DeepSeek](/wiki/DeepSeek)                                                                                         | 67                                                                                    | 2T tokens  [[ 84 ]](#cite_note-:1-90)  : table 2                                                                                                           | 12,000                                  | DeepSeek License                                                                      | Trained on English and Chinese text. 1e24 FLOPs for 67B. 1e23 FLOPs for 7B  [[ 84 ]](#cite_note-:1-90)  : figure 5                                                                                                                                                                                              |
| [Phi-2](/w/index.php?title=Phi_(LLM)&action=edit&redlink=1)            | December 2023                               | [Microsoft](/wiki/Microsoft)                                                                                       | 2.7                                                                                   | 1.4T tokens                                                                                                                                                | 419  [[ 85 ]](#cite_note-:9-91)         | [MIT](/wiki/MIT_License)                                                              | Trained on real and synthetic "textbook-quality" data, for 14 days on 96 A100 GPUs.  [[ 85 ]](#cite_note-:9-91)                                                                                                                                                                                                 |
| [Gemini 1.5](/wiki/Gemini_(language_model))                            | February 2024                               | [Google DeepMind](/wiki/Google_DeepMind)                                                                           | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | [Mixture-of-Experts](/wiki/Mixture_of_experts)  Multimodal model, based on a (MoE) architecture. Context window above 1 million tokens.  [[ 86 ]](#cite_note-92)                                                                                                                                                |
| [Gemini Ultra](/wiki/Gemini_(language_model))                          | February 2024                               | [Google DeepMind](/wiki/Google_DeepMind)                                                                           | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             |                                                                                                                                                                                                                                                                                                                 |
| Gemma                                                                  | February 2024                               | [Google DeepMind](/wiki/Google_DeepMind)                                                                           | 7                                                                                     | 6T tokens                                                                                                                                                  | Unknown                                 | Gemma Terms of Use  [[ 87 ]](#cite_note-gemma-93)                                     |                                                                                                                                                                                                                                                                                                                 |
| [Claude 3](/wiki/Claude_(language_model))                              | March 2024                                  | [Anthropic](/wiki/Anthropic)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Includes three models, Haiku, Sonnet, and Opus.  [[ 88 ]](#cite_note-94)                                                                                                                                                                                                                                        |
| [DBRX](/wiki/DBRX)                                                     | March 2024                                  | [Databricks](/wiki/Databricks)  [Mosaic ML](/wiki/Mosaic_ML)  and                                                  | 136                                                                                   | 12T tokens                                                                                                                                                 | Unknown                                 | Databricks Open Model License  [[ 89 ]](#cite_note-95)  [[ 90 ]](#cite_note-96)       | Training cost 10 million USD                                                                                                                                                                                                                                                                                    |
| Fugaku-LLM                                                             | May 2024                                    | [Fujitsu](/wiki/Fujitsu)  [Tokyo Institute of Technology](/wiki/Tokyo_Institute_of_Technology)  , , etc.           | 13                                                                                    | 380B tokens                                                                                                                                                | Unknown                                 | Fugaku-LLM Terms of Use  [[ 91 ]](#cite_note-97)                                      | [Fugaku](/wiki/Fugaku_(supercomputer))  The largest model ever trained on CPU-only, on the  [[ 92 ]](#cite_note-98)                                                                                                                                                                                             |
| Chameleon                                                              | May 2024                                    | [Meta AI](/wiki/Meta_AI)                                                                                           | 34  [[ 93 ]](#cite_note-99)                                                           | 4.4 trillion                                                                                                                                               | Unknown                                 | Non-commercial research  [[ 94 ]](#cite_note-100)                                     |                                                                                                                                                                                                                                                                                                                 |
| [Mixtral 8x22B](/wiki/Mixtral_8x22B)                                   | April 17, 2024                              | [Mistral AI](/wiki/Mistral_AI)                                                                                     | 141                                                                                   | Unknown                                                                                                                                                    | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        | [[ 95 ]](#cite_note-101)                                                                                                                                                                                                                                                                                        |
| [Phi-3](/w/index.php?title=Phi_(LLM)&action=edit&redlink=1)            | April 23, 2024                              | [Microsoft](/wiki/Microsoft)                                                                                       | 14  [[ 96 ]](#cite_note-102)                                                          | 4.8T tokens                                                                                                                                                | Unknown                                 | [MIT](/wiki/MIT_License)                                                              | Microsoft markets them as "small language model".  [[ 97 ]](#cite_note-103)                                                                                                                                                                                                                                     |
| [Granite Code Models](/wiki/IBM_Granite)                               | May 2024                                    | [IBM](/wiki/IBM)                                                                                                   | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        |                                                                                                                                                                                                                                                                                                                 |
| [Qwen2](/wiki/Qwen2)                                                   | June 2024                                   | [Alibaba Cloud](/wiki/Alibaba_Cloud)                                                                               | 72  [[ 98 ]](#cite_note-104)                                                          | 3T tokens                                                                                                                                                  | Unknown                                 | Qwen License                                                                          | Multiple sizes, the smallest being 0.5B.                                                                                                                                                                                                                                                                        |
| [DeepSeek-V2](/wiki/DeepSeek-V2)                                       | June 2024                                   | [DeepSeek](/wiki/DeepSeek)                                                                                         | 236                                                                                   | 8.1T tokens                                                                                                                                                | 28,000                                  | DeepSeek License                                                                      | 1.4M hours on H800.  [[ 99 ]](#cite_note-105)                                                                                                                                                                                                                                                                   |
| Nemotron-4                                                             | June 2024                                   | [Nvidia](/wiki/Nvidia)                                                                                             | 340                                                                                   | 9T tokens                                                                                                                                                  | 200,000                                 | NVIDIA Open Model License  [[ 100 ]](#cite_note-106)  [[ 101 ]](#cite_note-107)       | Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.  [[ 102 ]](#cite_note-108)  [[ 103 ]](#cite_note-109)                                                                                                                                                                        |
| [Claude 3.5](/wiki/Claude_(language_model))                            | June 2024                                   | [Anthropic](/wiki/Anthropic)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Initially, only one model, Sonnet, was released.  [[ 104 ]](#cite_note-110)  In October 2024, Sonnet 3.5 was upgraded, and Haiku 3.5 became available.  [[ 105 ]](#cite_note-111)                                                                                                                               |
| [Llama 3.1](/wiki/Llama_3.1)                                           | July 2024                                   | [Meta AI](/wiki/Meta_AI)                                                                                           | 405                                                                                   | 15.6T tokens                                                                                                                                               | 440,000                                 | [Llama 3 license](/wiki/Llama_(language_model)#Licensing)                             | [H100](/wiki/Hopper_(microarchitecture))  405B version took 31 million hours on -80GB, at 3.8E25 FLOPs.  [[ 106 ]](#cite_note-112)  [[ 107 ]](#cite_note-113)                                                                                                                                                   |
| [Grok-2](/wiki/Grok-2)                                                 | August 14, 2024                             | [xAI](/wiki/XAI_(company))                                                                                         | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | xAI Community License Agreement  [[ 108 ]](#cite_note-114)  [[ 109 ]](#cite_note-115) | Originally closed-source, then re-released as "Grok 2.5" under a source-available license in August 2025.  [[ 110 ]](#cite_note-116)  [[ 111 ]](#cite_note-117)                                                                                                                                                 |
| [OpenAI o1](/wiki/OpenAI_o1)                                           | September 12, 2024                          | [OpenAI](/wiki/OpenAI)                                                                                             | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Reasoning model.  [[ 112 ]](#cite_note-118)                                                                                                                                                                                                                                                                     |
| [Mistral Large](/wiki/Mistral_Large)                                   | November 2024                               | [Mistral AI](/wiki/Mistral_AI)                                                                                     | 123                                                                                   | Unknown                                                                                                                                                    | Unknown                                 | Mistral Research License                                                              | Upgraded over time. The latest version is 24.11.  [[ 113 ]](#cite_note-Mistral_models_overview-119)                                                                                                                                                                                                             |
| [Pixtral](/wiki/Pixtral)                                               | November 2024                               | [Mistral AI](/wiki/Mistral_AI)                                                                                     | 123                                                                                   | Unknown                                                                                                                                                    | Unknown                                 | Mistral Research License                                                              | Multimodal. There is also a 12B version which is under Apache 2 license.  [[ 113 ]](#cite_note-Mistral_models_overview-119)                                                                                                                                                                                     |
| [Phi-4](/w/index.php?title=Phi_(LLM)&action=edit&redlink=1)            | December 12, 2024                           | [Microsoft](/wiki/Microsoft)                                                                                       | 14  [[ 114 ]](#cite_note-120)                                                         | 9.8T tokens                                                                                                                                                | Unknown                                 | [MIT](/wiki/MIT_License)                                                              | Microsoft markets them as "small language model".  [[ 115 ]](#cite_note-121)                                                                                                                                                                                                                                    |
| [DeepSeek-V3](/wiki/DeepSeek-V3)                                       | December 2024                               | [DeepSeek](/wiki/DeepSeek)                                                                                         | 671                                                                                   | 14.8T tokens                                                                                                                                               | 56,000                                  | [MIT](/wiki/MIT_License)                                                              | 2.788M hours on H800 GPUs.  [[ 116 ]](#cite_note-122)  Originally released under the DeepSeek License, then re-released under the MIT License as "DeepSeek-V3-0324" in March 2025.  [[ 117 ]](#cite_note-123)                                                                                                   |
| Amazon Nova                                                            | December 2024                               | [Amazon](/wiki/Amazon_(company))                                                                                   | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Includes three models, Nova Micro, Nova Lite, and Nova Pro  [[ 118 ]](#cite_note-124)                                                                                                                                                                                                                           |
| [DeepSeek-R1](/wiki/DeepSeek-R1)                                       | January 2025                                | [DeepSeek](/wiki/DeepSeek)                                                                                         | 671                                                                                   | Not applicable                                                                                                                                             | Unknown                                 | [MIT](/wiki/MIT_License)                                                              | No pretraining. Reinforcement-learned upon V3-Base.  [[ 119 ]](#cite_note-125)  [[ 120 ]](#cite_note-126)                                                                                                                                                                                                       |
| [Qwen2.5](/wiki/Qwen2.5)                                               | January 2025                                | [Alibaba](/wiki/Alibaba_Group)                                                                                     | 72                                                                                    | 18T tokens                                                                                                                                                 | Unknown                                 | Qwen License                                                                          | 7 dense models, with parameter count from 0.5B to 72B. They also released 2 MoE variants.  [[ 121 ]](#cite_note-127)                                                                                                                                                                                            |
| MiniMax-Text-01                                                        | January 2025                                | [Minimax](/wiki/MiniMax_(company))                                                                                 | 456                                                                                   | 4.7T tokens  [[ 122 ]](#cite_note-:0-128)                                                                                                                  | Unknown                                 | Minimax Model license                                                                 | [[ 123 ]](#cite_note-129)  [[ 122 ]](#cite_note-:0-128)                                                                                                                                                                                                                                                         |
| [Gemini 2.0](/wiki/Gemini_(language_model))                            | February 2025                               | [Google DeepMind](/wiki/Google_DeepMind)                                                                           | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Three models released: Flash, Flash-Lite and Pro  [[ 124 ]](#cite_note-130)  [[ 125 ]](#cite_note-131)  [[ 126 ]](#cite_note-132)                                                                                                                                                                               |
| [Claude 3.7](/wiki/Claude_(language_model))                            | February 24, 2025                           | [Anthropic](/wiki/Anthropic)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | One model, Sonnet 3.7.  [[ 127 ]](#cite_note-133)                                                                                                                                                                                                                                                               |
| [GPT-4.5](/wiki/GPT-4.5)                                               | February 27, 2025                           | [OpenAI](/wiki/OpenAI)                                                                                             | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Largest non-reasoning model.  [[ 128 ]](#cite_note-134)                                                                                                                                                                                                                                                         |
| [Grok 3](/wiki/Grok_3)                                                 | February 2025                               | [xAI](/wiki/XAI_(company))                                                                                         | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown,  estimated 5,800,000           | [Proprietary](/wiki/Proprietary_software)                                             | Training cost claimed "10x the compute of previous state-of-the-art models".  [[ 129 ]](#cite_note-135)                                                                                                                                                                                                         |
| [Gemini 2.5](/wiki/Gemini_(language_model))                            | March 25, 2025                              | [Google DeepMind](/wiki/Google_DeepMind)                                                                           | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Three models released: Flash, Flash-Lite and Pro  [[ 130 ]](#cite_note-136)                                                                                                                                                                                                                                     |
| [Llama 4](/wiki/Llama_4)                                               | April 5, 2025                               | [Meta AI](/wiki/Meta_AI)                                                                                           | 400                                                                                   | 40T tokens                                                                                                                                                 | Unknown                                 | [Llama 4 license](/wiki/Llama_(language_model)#Licensing)                             | [[ 131 ]](#cite_note-137)  [[ 132 ]](#cite_note-138)                                                                                                                                                                                                                                                            |
| [OpenAI o3](/wiki/OpenAI_o3)  and o4-mini                              | April 16, 2025                              | [OpenAI](/wiki/OpenAI)                                                                                             | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Reasoning models.  [[ 133 ]](#cite_note-139)                                                                                                                                                                                                                                                                    |
| [Qwen3](/wiki/Qwen3)                                                   | April 2025                                  | [Alibaba Cloud](/wiki/Alibaba_Cloud)                                                                               | 235                                                                                   | 36T tokens                                                                                                                                                 | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        | Multiple sizes, the smallest being 0.6B.  [[ 134 ]](#cite_note-140)                                                                                                                                                                                                                                             |
| [Claude 4](/wiki/Claude_(language_model))                              | May 22, 2025                                | [Anthropic](/wiki/Anthropic)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Includes two models, Sonnet and Opus.  [[ 135 ]](#cite_note-141)                                                                                                                                                                                                                                                |
| [Grok 4](/wiki/Grok_4)                                                 | July 9, 2025                                | [xAI](/wiki/XAI_(company))                                                                                         | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             |                                                                                                                                                                                                                                                                                                                 |
| GLM-4.5                                                                | July 29, 2025                               | [Zhipu AI](/wiki/Zhipu_AI)                                                                                         | 355                                                                                   | 22T tokens                                                                                                                                                 | Unknown                                 | [MIT](/wiki/MIT_License)                                                              | Released in 335B and 106B sizes.  [[ 136 ]](#cite_note-142)  Corpus size was calculated by combining the 15 trillion tokens and the 7 trillion tokens pre-training mix.  [[ 137 ]](#cite_note-143)                                                                                                              |
| [GPT-OSS](/wiki/GPT-OSS)                                               | August 5, 2025                              | [OpenAI](/wiki/OpenAI)                                                                                             | 117                                                                                   | Unknown                                                                                                                                                    | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        | Released in 20B and 120B sizes.  [[ 138 ]](#cite_note-144)                                                                                                                                                                                                                                                      |
| [Claude 4.1](/wiki/Claude_(language_model))                            | August 5, 2025                              | [Anthropic](/wiki/Anthropic)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Includes one model, Opus.  [[ 139 ]](#cite_note-145)                                                                                                                                                                                                                                                            |
| [GPT-5](/wiki/GPT-5)                                                   | August 7, 2025                              | [OpenAI](/wiki/OpenAI)                                                                                             | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Includes three models, GPT-5, GPT-5 mini, and GPT-5 nano. GPT-5 is available in ChatGPT and API. It includes thinking abilities.  [[ 140 ]](#cite_note-146)  [[ 141 ]](#cite_note-147)                                                                                                                          |
| DeepSeek-V3.1                                                          | August 21, 2025                             | [DeepSeek](/wiki/DeepSeek)                                                                                         | 671                                                                                   | 15.639T                                                                                                                                                    |                                         | [MIT](/wiki/MIT_License)                                                              | Training size: 14.8T tokens, of DeepSeek V3 plus 839B tokens from the extension phases (630B + 209B)  [[ 142 ]](#cite_note-148)  It is a hybrid model that can switch between thinking and non-thinking modes.  [[ 143 ]](#cite_note-149)                                                                       |
| [Apertus](/wiki/Apertus_(LLM))                                         | September 2, 2025                           | [ETH Zurich](/wiki/ETH_Zurich)  [EPF Lausanne](/wiki/%C3%89cole_Polytechnique_F%C3%A9d%C3%A9rale_de_Lausanne)  and | 70                                                                                    | 15 trillion  [[ 144 ]](#cite_note-150)                                                                                                                     | Unknown                                 | [Apache 2.0](/wiki/Apache_2.0)                                                        | [EU's](/wiki/European_Union)  [Artificial Intelligence Act](/wiki/Artificial_Intelligence_Act)  It's said to be the first LLM to be compliant with .  [[ 145 ]](#cite_note-151)                                                                                                                                 |
| [Claude 4.5](/wiki/Claude_(language_model))                            | September 29, 2025                          | [Anthropic](/wiki/Anthropic)                                                                                       | Unknown                                                                               | Unknown                                                                                                                                                    | Unknown                                 | [Proprietary](/wiki/Proprietary_software)                                             | Only one variant is available, Sonnet.  [[ 146 ]](#cite_note-152)                                                                                                                                                                                                                                               |
| DeepSeek-V3.2-Exp                                                      | September 29, 2025                          | [DeepSeek](/wiki/DeepSeek)                                                                                         | 685                                                                                   |                                                                                                                                                            |                                         | [MIT](/wiki/MIT_License)                                                              | This experimental model built upon v3.1-Terminus uses a custom efficient mechanism tagged DeepSeek Sparse Attention (DSA).  [[ 147 ]](#cite_note-153)  [[ 148 ]](#cite_note-154)  [[ 149 ]](#cite_note-155)                                                                                                     |
| GLM-4.6                                                                | September 30, 2025                          | [Zhipu AI](/wiki/Zhipu_AI)                                                                                         | 357                                                                                   |                                                                                                                                                            |                                         | [Apache 2.0](/wiki/Apache_2.0)                                                        | [[ 150 ]](#cite_note-156)  [[ 151 ]](#cite_note-157)  [[ 152 ]](#cite_note-158)                                                                                                                                                                                                                                 |

## Timeline

[ [edit](/w/index.php?title=List_of_large_language_models&action=edit&section=2) ]

| Timeline of major LLM releases (2024-present)                                                   |
|-------------------------------------------------------------------------------------------------|
| <!-- 🖼️❌ Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` --> |

<!-- 🖼️❌ Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->

## See also

[ [edit](/w/index.php?title=List_of_large_language_models&action=edit&section=3) ]

- [List of chatbots](/wiki/List_of_chatbots)
- [List of language model benchmarks](/wiki/List_of_language_model_benchmarks)

## Notes

[ [edit](/w/index.php?title=List_of_large_language_models&action=edit&section=4) ]

1. [**^**](#cite_ref-1) This is the date that documentation describing the model's architecture was first released.
2. [**^**](#cite_ref-2) In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.
3. [**^**](#cite_ref-3) This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated. LLMs may be licensed differently from the chatbots that use them; for the licenses of chatbots, see [List of chatbots](/wiki/List_of_chatbots) .
4. [**^**](#cite_ref-50) The smaller models including 66B are publicly available, while the 175B model is available on request.
5. [**^**](#cite_ref-63) Facebook's license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.
6. [**^**](#cite_ref-65) As stated in Technical report: "Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ..." [[ 59 ]](#cite_note-GPT4Tech-64)

## References

[ [edit](/w/index.php?title=List_of_large_language_models&action=edit&section=5) ]

1. [**^**](#cite_ref-oai-unsup_4-0) ["Improving language understanding with unsupervised learning"](https://openai.com/research/language-unsupervised) . *openai.com* . June 11, 2018. [Archived](https://web.archive.org/web/20230318210736/https://openai.com/research/language-unsupervised) from the original on 2023-03-18 . Retrieved 2023-03-18 .
2. [**^**](#cite_ref-5) ["finetune-transformer-lm"](https://github.com/openai/finetune-transformer-lm) . *GitHub* . [Archived](https://web.archive.org/web/20230519062127/https://github.com/openai/finetune-transformer-lm) from the original on 19 May 2023 . Retrieved 2 January 2024 .
3. [**^**](#cite_ref-6) Radford, Alec (11 June 2018). ["Improving language understanding with unsupervised learning"](https://openai.com/index/language-unsupervised/) . [*OpenAI*](/wiki/OpenAI) . Retrieved 18 November 2025 .
4. ^ [***a***](#cite_ref-bert-paper_7-0) [***b***](#cite_ref-bert-paper_7-1) Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". [arXiv](/wiki/ArXiv_(identifier)) : [1810.04805v2](https://arxiv.org/abs/1810.04805v2) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
5. [**^**](#cite_ref-bHZJ2_8-0) Prickett, Nicole Hemsoth (2021-08-24). ["Cerebras Shifts Architecture To Meet Massive AI/ML Models"](https://www.nextplatform.com/2021/08/24/cerebras-shifts-architecture-to-meet-massive-ai-ml-models/) . *The Next Platform* . [Archived](https://web.archive.org/web/20230620151619/https://www.nextplatform.com/2021/08/24/cerebras-shifts-architecture-to-meet-massive-ai-ml-models/) from the original on 2023-06-20 . Retrieved 2023-06-20 .
6. [**^**](#cite_ref-bert-web_9-0) ["BERT"](https://github.com/google-research/bert) . March 13, 2023. [Archived](https://web.archive.org/web/20210113211317/https://github.com/google-research/bert) from the original on January 13, 2021 . Retrieved March 13, 2023 - via GitHub.
7. [**^**](#cite_ref-Manning-2022_10-0) [Manning, Christopher D.](/wiki/Christopher_D._Manning) (2022). ["Human Language Understanding &amp; Reasoning"](https://www.amacad.org/publication/human-language-understanding-reasoning) . *Daedalus* . **151** (2): 127- 138. [doi](/wiki/Doi_(identifier)) : [10.1162/daed\_a\_01905](https://doi.org/10.1162%2Fdaed_a_01905) . [S2CID](/wiki/S2CID_(identifier)) [248377870](https://api.semanticscholar.org/CorpusID:248377870) . [Archived](https://web.archive.org/web/20231117205531/https://www.amacad.org/publication/human-language-understanding-reasoning) from the original on 2023-11-17 . Retrieved 2023-03-09 .
8. [**^**](#cite_ref-Ir545_11-0) Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). "Bidirectional Language Models Are Also Few-shot Learners". [arXiv](/wiki/ArXiv_(identifier)) : [2209.14500](https://arxiv.org/abs/2209.14500) [ [cs.LG](https://arxiv.org/archive/cs.LG) ].
9. [**^**](#cite_ref-:02_12-0) Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". [arXiv](/wiki/ArXiv_(identifier)) : [1810.04805v2](https://arxiv.org/abs/1810.04805v2) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
10. ^ [***a***](#cite_ref-:6_13-0) [***b***](#cite_ref-:6_13-1) Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). ["Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"](http://jmlr.org/papers/v21/20-074.html) . *Journal of Machine Learning Research* . **21** (140): 1- 67. [arXiv](/wiki/ArXiv_(identifier)) : [1910.10683](https://arxiv.org/abs/1910.10683) . [ISSN](/wiki/ISSN_(identifier)) [1533-7928](https://search.worldcat.org/issn/1533-7928) .
11. [**^**](#cite_ref-14) [*google-research/text-to-text-transfer-transformer*](https://github.com/google-research/text-to-text-transfer-transformer) , Google Research, 2024-04-02, [archived](https://web.archive.org/web/20240329112957/https://github.com/google-research/text-to-text-transfer-transformer) from the original on 2024-03-29 , retrieved 2024-04-04
12. [**^**](#cite_ref-15) ["Imagen: Text-to-Image Diffusion Models"](https://imagen.research.google/) . *imagen.research.google* . [Archived](https://web.archive.org/web/20240327201713/https://imagen.research.google/) from the original on 2024-03-27 . Retrieved 2024-04-04 .
13. [**^**](#cite_ref-16) ["Pretrained models - transformers 2.0.0 documentation"](https://huggingface.co/transformers/v2.0.0/pretrained_models.html) . *huggingface.co* . [Archived](https://web.archive.org/web/20240805032110/https://huggingface.co/transformers/v2.0.0/pretrained_models.html) from the original on 2024-08-05 . Retrieved 2024-08-05 .
14. [**^**](#cite_ref-xlnet_17-0) ["xlnet"](https://github.com/zihangdai/xlnet/) . *GitHub* . [Archived](https://web.archive.org/web/20240102191842/https://github.com/zihangdai/xlnet/) from the original on 2 January 2024 . Retrieved 2 January 2024 .
15. [**^**](#cite_ref-LX3rI_18-0) Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V. (2 January 2020). "XLNet: Generalized Autoregressive Pretraining for Language Understanding". [arXiv](/wiki/ArXiv_(identifier)) : [1906.08237](https://arxiv.org/abs/1906.08237) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
16. [**^**](#cite_ref-15Brelease_19-0) ["GPT-2: 1.5B Release"](https://openai.com/blog/gpt-2-1-5b-release/) . *OpenAI* . 2019-11-05. [Archived](https://web.archive.org/web/20191114074358/https://openai.com/blog/gpt-2-1-5b-release/) from the original on 2019-11-14 . Retrieved 2019-11-14 .
17. [**^**](#cite_ref-5T8u5_20-0) ["Better language models and their implications"](https://openai.com/research/better-language-models) . *openai.com* . [Archived](https://web.archive.org/web/20230316160730/https://openai.com/research/better-language-models) from the original on 2023-03-16 . Retrieved 2023-03-13 .
18. ^ [***a***](#cite_ref-LambdaLabs_21-0) [***b***](#cite_ref-LambdaLabs_21-1) ["OpenAI's GPT-3 Language Model: A Technical Overview"](https://lambdalabs.com/blog/demystifying-gpt-3) . *lambdalabs.com* . 3 June 2020. [Archived](https://web.archive.org/web/20230327213811/https://lambdalabs.com/blog/demystifying-gpt-3) from the original on 27 March 2023 . Retrieved 13 March 2023 .
19. ^ [***a***](#cite_ref-:10_22-0) [***b***](#cite_ref-:10_22-1) ["openai-community/gpt2-xl · Hugging Face"](https://huggingface.co/openai-community/gpt2-xl) . *huggingface.co* . [Archived](https://web.archive.org/web/20240724041702/https://huggingface.co/openai-community/gpt2-xl) from the original on 2024-07-24 . Retrieved 2024-07-24 .
20. [**^**](#cite_ref-Sudbe_23-0) ["gpt-2"](https://github.com/openai/gpt-2) . *GitHub* . [Archived](https://web.archive.org/web/20230311154936/https://github.com/openai/gpt-2) from the original on 11 March 2023 . Retrieved 13 March 2023 .
21. [**^**](#cite_ref-Wiggers_24-0) Wiggers, Kyle (28 April 2022). ["The emerging types of language models and why they matter"](https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/) . *TechCrunch* . [Archived](https://web.archive.org/web/20230316072443/https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/) from the original on 16 March 2023 . Retrieved 9 March 2023 .
22. [**^**](#cite_ref-:2_25-0) Table D.1 in Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (May 28, 2020). "Language Models are Few-Shot Learners". [arXiv](/wiki/ArXiv_(identifier)) : [2005.14165v4](https://arxiv.org/abs/2005.14165v4) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
23. [**^**](#cite_ref-chatgpt-blog_26-0) ["ChatGPT: Optimizing Language Models for Dialogue"](https://openai.com/blog/chatgpt/) . *OpenAI* . 2022-11-30. [Archived](https://web.archive.org/web/20221130180912/https://openai.com/blog/chatgpt/) from the original on 2022-11-30 . Retrieved 2023-01-13 .
24. [**^**](#cite_ref-gpt-neo_27-0) ["GPT Neo"](https://github.com/EleutherAI/gpt-neo) . March 15, 2023. [Archived](https://web.archive.org/web/20230312225202/https://github.com/EleutherAI/gpt-neo) from the original on March 12, 2023 . Retrieved March 12, 2023 - via GitHub.
25. ^ [***a***](#cite_ref-Pile_28-0) [***b***](#cite_ref-Pile_28-1) [***c***](#cite_ref-Pile_28-2) Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (31 December 2020). "The Pile: An 800GB Dataset of Diverse Text for Language Modeling". [arXiv](/wiki/ArXiv_(identifier)) : [2101.00027](https://arxiv.org/abs/2101.00027) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
26. ^ [***a***](#cite_ref-vb-gpt-neo_29-0) [***b***](#cite_ref-vb-gpt-neo_29-1) Iyer, Abhishek (15 May 2021). ["GPT-3's free alternative GPT-Neo is something to be excited about"](https://venturebeat.com/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/) . *VentureBeat* . [Archived](https://web.archive.org/web/20230309012717/https://venturebeat.com/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/) from the original on 9 March 2023 . Retrieved 13 March 2023 .
27. [**^**](#cite_ref-JxohJ_30-0) ["GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront"](https://web.archive.org/web/20230309205439/https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model) . *www.forefront.ai* . Archived from [the original](https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model) on 2023-03-09 . Retrieved 2023-02-28 .
28. ^ [***a***](#cite_ref-:3_31-0) [***b***](#cite_ref-:3_31-1) [***c***](#cite_ref-:3_31-2) [***d***](#cite_ref-:3_31-3) Dey, Nolan; Gosal, Gurpreet; Zhiming; Chen; Khachane, Hemant; Marshall, William; Pathria, Ribhu; Tom, Marvin; Hestness, Joel (2023-04-01). "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster". [arXiv](/wiki/ArXiv_(identifier)) : [2304.03208](https://arxiv.org/abs/2304.03208) [ [cs.LG](https://arxiv.org/archive/cs.LG) ].
29. [**^**](#cite_ref-BwnW5_32-0) Alvi, Ali; Kharya, Paresh (11 October 2021). ["Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model"](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) . *Microsoft Research* . [Archived](https://web.archive.org/web/20230313180531/https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) from the original on 13 March 2023 . Retrieved 13 March 2023 .
30. ^ [***a***](#cite_ref-mtnlg-preprint_33-0) [***b***](#cite_ref-mtnlg-preprint_33-1) Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022-02-04). "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model". [arXiv](/wiki/ArXiv_(identifier)) : [2201.11990](https://arxiv.org/abs/2201.11990) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
31. ^ [***a***](#cite_ref-:11_34-0) [***b***](#cite_ref-:11_34-1) Rajbhandari, Samyam; Li, Conglong; Yao, Zhewei; Zhang, Minjia; Aminabadi, Reza Yazdani; Awan, Ammar Ahmad; Rasley, Jeff; He, Yuxiong (2022-07-21), *DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale* , [arXiv](/wiki/ArXiv_(identifier)) : [2201.05596](https://arxiv.org/abs/2201.05596)
32. [**^**](#cite_ref-qeOB8_35-0) Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (December 23, 2021). "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation". [arXiv](/wiki/ArXiv_(identifier)) : [2112.12731](https://arxiv.org/abs/2112.12731) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
33. [**^**](#cite_ref-i8jc4_36-0) ["Product"](https://www.anthropic.com/product) . *Anthropic* . [Archived](https://web.archive.org/web/20230316145444/https://www.anthropic.com/product) from the original on 16 March 2023 . Retrieved 14 March 2023 .
34. ^ [***a***](#cite_ref-AnthroArch_37-0) [***b***](#cite_ref-AnthroArch_37-1) Askell, Amanda; Bai, Yuntao; Chen, Anna; et al. (9 December 2021). "A General Language Assistant as a Laboratory for Alignment". [arXiv](/wiki/ArXiv_(identifier)) : [2112.00861](https://arxiv.org/abs/2112.00861) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
35. [**^**](#cite_ref-RZqhw_38-0) Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et al. (15 December 2022). "Constitutional AI: Harmlessness from AI Feedback". [arXiv](/wiki/ArXiv_(identifier)) : [2212.08073](https://arxiv.org/abs/2212.08073) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
36. ^ [***a***](#cite_ref-glam-blog_39-0) [***b***](#cite_ref-glam-blog_39-1) [***c***](#cite_ref-glam-blog_39-2) Dai, Andrew M; Du, Nan (December 9, 2021). ["More Efficient In-Context Learning with GLaM"](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html) . *ai.googleblog.com* . [Archived](https://web.archive.org/web/20230312072042/https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html) from the original on 2023-03-12 . Retrieved 2023-03-09 .
37. [**^**](#cite_ref-mD5eE_40-0) ["Language modelling at scale: Gopher, ethical considerations, and retrieval"](https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval) . *www.deepmind.com* . 8 December 2021. [Archived](https://web.archive.org/web/20230320082323/https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval) from the original on 20 March 2023 . Retrieved 20 March 2023 .
38. ^ [***a***](#cite_ref-hoffman_41-0) [***b***](#cite_ref-hoffman_41-1) [***c***](#cite_ref-hoffman_41-2) Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et al. (29 March 2022). "Training Compute-Optimal Large Language Models". [arXiv](/wiki/ArXiv_(identifier)) : [2203.15556](https://arxiv.org/abs/2203.15556) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
39. ^ [***a***](#cite_ref-:4_42-0) [***b***](#cite_ref-:4_42-1) [***c***](#cite_ref-:4_42-2) [***d***](#cite_ref-:4_42-3) Table 20 and page 66 of [*PaLM: Scaling Language Modeling with Pathways*](https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf) [*Archived*](https://web.archive.org/web/20230610040050/https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf) *2023-06-10 at the* [*Wayback Machine*](/wiki/Wayback_Machine)
40. ^ [***a***](#cite_ref-lamda-blog_43-0) [***b***](#cite_ref-lamda-blog_43-1) Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). ["LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything"](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html) . *ai.googleblog.com* . [Archived](https://web.archive.org/web/20220325014118/https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html) from the original on 2022-03-25 . Retrieved 2023-03-09 .
41. [**^**](#cite_ref-DMs9Z_44-0) Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv; Cheng, Heng-Tze; Jin, Alicia; Bos, Taylor; Baker, Leslie; Du, Yu; Li, YaGuang; Lee, Hongrae; Zheng, Huaixiu Steven; Ghafouri, Amin; Menegali, Marcelo (2022-01-01). "LaMDA: Language Models for Dialog Applications". [arXiv](/wiki/ArXiv_(identifier)) : [2201.08239](https://arxiv.org/abs/2201.08239) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
42. [**^**](#cite_ref-gpt-neox-20b_45-0) Black, Sidney; Biderman, Stella; Hallahan, Eric; et al. (2022-05-01). [*GPT-NeoX-20B: An Open-Source Autoregressive Language Model*](https://aclanthology.org/2022.bigscience-1.9/) . Proceedings of BigScience Episode #5 - Workshop on Challenges &amp; Perspectives in Creating Large Language Models. Vol. Proceedings of BigScience Episode #5 - Workshop on Challenges &amp; Perspectives in Creating Large Language Models. pp. 95- 136. [Archived](https://web.archive.org/web/20221210082456/https://aclanthology.org/2022.bigscience-1.9/) from the original on 2022-12-10 . Retrieved 2022-12-19 .
43. ^ [***a***](#cite_ref-chinchilla-blog_46-0) [***b***](#cite_ref-chinchilla-blog_46-1) [***c***](#cite_ref-chinchilla-blog_46-2) Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). ["An empirical analysis of compute-optimal large language model training"](https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training) . *Deepmind Blog* . [Archived](https://web.archive.org/web/20220413014510/https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training) from the original on 13 April 2022 . Retrieved 9 March 2023 .
44. [**^**](#cite_ref-palm-blog_47-0) Narang, Sharan; Chowdhery, Aakanksha (April 4, 2022). ["Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance"](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) . *ai.googleblog.com* . [Archived](https://web.archive.org/web/20220404161447/https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) from the original on 2022-04-04 . Retrieved 2023-03-09 .
45. [**^**](#cite_ref-jlof8_48-0) Susan Zhang; Mona Diab; Luke Zettlemoyer. ["Democratizing access to large-scale language models with OPT-175B"](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) . *ai.facebook.com* . [Archived](https://web.archive.org/web/20230312231820/https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) from the original on 2023-03-12 . Retrieved 2023-03-12 .
46. [**^**](#cite_ref-QjTIc_49-0) Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (21 June 2022). "OPT: Open Pre-trained Transformer Language Models". [arXiv](/wiki/ArXiv_(identifier)) : [2205.01068](https://arxiv.org/abs/2205.01068) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
47. [**^**](#cite_ref-51) ["metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq"](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles) . *GitHub* . Retrieved 2024-10-18 .
48. ^ [***a***](#cite_ref-yalm-repo_52-0) [***b***](#cite_ref-yalm-repo_52-1) Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022-06-22), [*YaLM 100B*](https://github.com/yandex/YaLM-100B) , [archived](https://web.archive.org/web/20230616050056/https://github.com/yandex/YaLM-100B) from the original on 2023-06-16 , retrieved 2023-03-18
49. ^ [***a***](#cite_ref-minerva-paper_53-0) [***b***](#cite_ref-minerva-paper_53-1) Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (30 June 2022). "Solving Quantitative Reasoning Problems with Language Models". [arXiv](/wiki/ArXiv_(identifier)) : [2206.14858](https://arxiv.org/abs/2206.14858) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
50. [**^**](#cite_ref-FfCNK_54-0) ["Minerva: Solving Quantitative Reasoning Problems with Language Models"](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html) . *ai.googleblog.com* . 30 June 2022 . Retrieved 20 March 2023 .
51. [**^**](#cite_ref-bigger-better_55-0) Ananthaswamy, Anil (8 March 2023). ["In AI, is bigger always better?"](https://www.nature.com/articles/d41586-023-00641-w) . *Nature* . **615** (7951): 202- 205. [Bibcode](/wiki/Bibcode_(identifier)) : [2023Natur.615..202A](https://ui.adsabs.harvard.edu/abs/2023Natur.615..202A) . [doi](/wiki/Doi_(identifier)) : [10.1038/d41586-023-00641-w](https://doi.org/10.1038%2Fd41586-023-00641-w) . [PMID](/wiki/PMID_(identifier)) [36890378](https://pubmed.ncbi.nlm.nih.gov/36890378) . [S2CID](/wiki/S2CID_(identifier)) [257380916](https://api.semanticscholar.org/CorpusID:257380916) . [Archived](https://web.archive.org/web/20230316181013/https://www.nature.com/articles/d41586-023-00641-w) from the original on 16 March 2023 . Retrieved 9 March 2023 .
52. [**^**](#cite_ref-B8wB2_56-0) ["bigscience/bloom · Hugging Face"](https://huggingface.co/bigscience/bloom) . *huggingface.co* . [Archived](https://web.archive.org/web/20230412002547/https://huggingface.co/bigscience/bloom) from the original on 2023-04-12 . Retrieved 2023-03-13 .
53. [**^**](#cite_ref-37sY6_57-0) Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). "Galactica: A Large Language Model for Science". [arXiv](/wiki/ArXiv_(identifier)) : [2211.09085](https://arxiv.org/abs/2211.09085) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
54. [**^**](#cite_ref-u5szh_58-0) ["20B-parameter Alexa model sets new marks in few-shot learning"](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning) . *Amazon Science* . 2 August 2022. [Archived](https://web.archive.org/web/20230315190223/https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning) from the original on 15 March 2023 . Retrieved 12 March 2023 .
55. [**^**](#cite_ref-HaA7l_59-0) Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et al. (3 August 2022). "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model". [arXiv](/wiki/ArXiv_(identifier)) : [2208.01448](https://arxiv.org/abs/2208.01448) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
56. [**^**](#cite_ref-rpehM_60-0) ["AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog"](https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/) . *aws.amazon.com* . 17 November 2022. [Archived](https://web.archive.org/web/20230313163933/https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/) from the original on 13 March 2023 . Retrieved 13 March 2023 .
57. ^ [***a***](#cite_ref-llama-blog_61-0) [***b***](#cite_ref-llama-blog_61-1) [***c***](#cite_ref-llama-blog_61-2) ["Introducing LLaMA: A foundational, 65-billion-parameter large language model"](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) . *Meta AI* . 24 February 2023. [Archived](https://web.archive.org/web/20230303112302/https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) from the original on 3 March 2023 . Retrieved 9 March 2023 .
58. ^ [***a***](#cite_ref-:5_62-0) [***b***](#cite_ref-:5_62-1) [***c***](#cite_ref-:5_62-2) ["The Falcon has landed in the Hugging Face ecosystem"](https://huggingface.co/blog/falcon) . *huggingface.co* . [Archived](https://web.archive.org/web/20230620002832/https://huggingface.co/blog/falcon) from the original on 2023-06-20 . Retrieved 2023-06-20 .
59. [**^**](#cite_ref-GPT4Tech_64-0) ["GPT-4 Technical Report"](https://cdn.openai.com/papers/gpt-4.pdf) (PDF) . [*OpenAI*](/wiki/OpenAI) . 2023. [Archived](https://web.archive.org/web/20230314190904/https://cdn.openai.com/papers/gpt-4.pdf) (PDF) from the original on March 14, 2023 . Retrieved March 14, 2023 .
60. [**^**](#cite_ref-66) Schreiner, Maximilian (2023-07-11). ["GPT-4 architecture, datasets, costs and more leaked"](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/) . *THE DECODER* . [Archived](https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/) from the original on 2023-07-12 . Retrieved 2024-07-26 .
61. [**^**](#cite_ref-D0k2a_67-0) Dey, Nolan (March 28, 2023). ["Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models"](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) . *Cerebras* . [Archived](https://web.archive.org/web/20230328213339/https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) from the original on March 28, 2023 . Retrieved March 28, 2023 .
62. [**^**](#cite_ref-falcon_68-0) ["Abu Dhabi-based TII launches its own version of ChatGPT"](https://fastcompanyme.com/news/abu-dhabi-based-tii-launches-its-own-version-of-chatgpt/) . *tii.ae* . [Archived](https://web.archive.org/web/20230403021729/https://fastcompanyme.com/news/abu-dhabi-based-tii-launches-its-own-version-of-chatgpt/) from the original on 2023-04-03 . Retrieved 2023-04-03 .
63. [**^**](#cite_ref-Xb1gq_69-0) Penedo, Guilherme; Malartic, Quentin; Hesslow, Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza; Pannier, Baptiste; Almazrouei, Ebtesam; Launay, Julien (2023-06-01). "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only". [arXiv](/wiki/ArXiv_(identifier)) : [2306.01116](https://arxiv.org/abs/2306.01116) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
64. [**^**](#cite_ref-gzTNw_70-0) ["tiiuae/falcon-40b · Hugging Face"](https://huggingface.co/tiiuae/falcon-40b) . *huggingface.co* . 2023-06-09 . Retrieved 2023-06-20 .
65. [**^**](#cite_ref-Wmlcs_71-0) [UAE's Falcon 40B, World's Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free](https://www.businesswire.com/news/home/20230531005608/en/UAE's-Falcon-40B-World's-Top-Ranked-AI-Model-from-Technology-Innovation-Institute-is-Now-Royalty-Free) [Archived](https://web.archive.org/web/20240208133040/https://www.businesswire.com/news/home/20230531005608/en/UAE%27s-Falcon-40B-World%27s-Top-Ranked-AI-Model-from-Technology-Innovation-Institute-is-Now-Royalty-Free) 2024-02-08 at the [Wayback Machine](/wiki/Wayback_Machine) , 31 May 2023
66. [**^**](#cite_ref-nGOSu_72-0) Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). "BloombergGPT: A Large Language Model for Finance". [arXiv](/wiki/ArXiv_(identifier)) : [2303.17564](https://arxiv.org/abs/2303.17564) [ [cs.LG](https://arxiv.org/archive/cs.LG) ].
67. [**^**](#cite_ref-9WSFw_73-0) Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). "PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing". [arXiv](/wiki/ArXiv_(identifier)) : [2303.10845](https://arxiv.org/abs/2303.10845) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
68. [**^**](#cite_ref-JiOl8_74-0) Köpf, Andreas; Kilcher, Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). "OpenAssistant Conversations - Democratizing Large Language Model Alignment". [arXiv](/wiki/ArXiv_(identifier)) : [2304.07327](https://arxiv.org/abs/2304.07327) [ [cs.CL](https://arxiv.org/archive/cs.CL) ].
69. [**^**](#cite_ref-75) Wrobel, Sharon. ["Tel Aviv startup rolls out new advanced AI language model to rival OpenAI"](https://www.timesofisrael.com/ai21-labs-rolls-out-new-advanced-ai-language-model-to-rival-openai/) . [*The Times of Israel*](/wiki/The_Times_of_Israel) . [ISSN](/wiki/ISSN_(identifier)) [0040-7909](https://search.worldcat.org/issn/0040-7909) . [Archived](https://web.archive.org/web/20230724191823/https://www.timesofisrael.com/ai21-labs-rolls-out-new-advanced-ai-language-model-to-rival-openai/) from the original on 2023-07-24 . Retrieved 2023-07-24 .
70. [**^**](#cite_ref-76) Wiggers, Kyle (2023-04-13). ["With Bedrock, Amazon enters the generative AI race"](https://techcrunch.com/2023/04/13/with-bedrock-amazon-enters-the-generative-ai-race/) . *TechCrunch* . [Archived](https://web.archive.org/web/20230724102458/https://techcrunch.com/2023/04/13/with-bedrock-amazon-enters-the-generative-ai-race/) from the original on 2023-07-24 . Retrieved 2023-07-24 .
71. ^ [***a***](#cite_ref-cnbc-20230516_77-0) [***b***](#cite_ref-cnbc-20230516_77-1) Elias, Jennifer (16 May 2023). ["Google's newest A.I. model uses nearly five times more text data for training than its predecessor"](https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html) . [*CNBC*](/wiki/CNBC) . [Archived](https://web.archive.org/web/20230516225326/https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html) from the original on 16 May 2023 . Retrieved 18 May 2023 .
72. [**^**](#cite_ref-pWyLA_78-0) ["Introducing PaLM 2"](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) . *Google* . May 10, 2023. [Archived](https://web.archive.org/web/20230518213209/https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) from the original on May 18, 2023 . Retrieved May 18, 2023 .
73. ^ [***a***](#cite_ref-meta-20230719_79-0) [***b***](#cite_ref-meta-20230719_79-1) ["Introducing Llama 2: The Next Generation of Our Open Source Large Language Model"](https://ai.meta.com/llama/) . *Meta AI* . 2023. [Archived](https://web.archive.org/web/20240105234629/https://ai.meta.com/llama/) from the original on 2024-01-05 . Retrieved 2023-07-19 .
74. [**^**](#cite_ref-80) ["llama/MODEL\_CARD.md at main · meta-llama/llama"](https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md) . *GitHub* . [Archived](https://web.archive.org/web/20240528090541/https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md) from the original on 2024-05-28 . Retrieved 2024-05-28 .
75. [**^**](#cite_ref-81) ["Claude 2"](https://www.anthropic.com/index/claude-2) . *anthropic.com* . [Archived](https://web.archive.org/web/20231215212208/https://www.anthropic.com/index/claude-2) from the original on 15 December 2023 . Retrieved 12 December 2023 .
76. [**^**](#cite_ref-82) Nirmal, Dinesh (2023-09-07). ["Building AI for business: IBM's Granite foundation models"](https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models) . *IBM Blog* . [Archived](https://web.archive.org/web/20240722083855/https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models/) from the original on 2024-07-22 . Retrieved 2024-08-11 .
77. [**^**](#cite_ref-mistral-20230927_83-0) ["Announcing Mistral 7B"](https://mistral.ai/news/announcing-mistral-7b/) . *Mistral* . 2023. [Archived](https://web.archive.org/web/20240106051047/https://mistral.ai/news/announcing-mistral-7b/) from the original on 2024-01-06 . Retrieved 2023-10-06 .
78. [**^**](#cite_ref-84) ["Introducing Claude 2.1"](https://www.anthropic.com/index/claude-2-1) . *anthropic.com* . [Archived](https://web.archive.org/web/20231215201726/https://www.anthropic.com/index/claude-2-1) from the original on 15 December 2023 . Retrieved 12 December 2023 .
79. [**^**](#cite_ref-85) [*xai-org/grok-1*](https://github.com/xai-org/grok-1) , xai-org, 2024-03-19, [archived](https://web.archive.org/web/20240528170731/https://github.com/xai-org/grok-1) from the original on 2024-05-28 , retrieved 2024-03-19
80. [**^**](#cite_ref-86) ["Grok-1 model card"](https://x.ai/model-card/) . *x.ai* . Retrieved 12 December 2023 .
81. [**^**](#cite_ref-87) ["Gemini - Google DeepMind"](https://deepmind.google/technologies/gemini/#capabilities) . *deepmind.google* . [Archived](https://web.archive.org/web/20231208015607/https://deepmind.google/technologies/gemini/#capabilities) from the original on 8 December 2023 . Retrieved 12 December 2023 .
82. [**^**](#cite_ref-88) Franzen, Carl (11 December 2023). ["Mistral shocks AI community as latest open source model eclipses GPT-3.5 performance"](https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/) . *VentureBeat* . [Archived](https://web.archive.org/web/20231211213640/https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/) from the original on 11 December 2023 . Retrieved 12 December 2023 .
83. [**^**](#cite_ref-89) ["Mixtral of experts"](https://mistral.ai/news/mixtral-of-experts/) . *mistral.ai* . 11 December 2023. [Archived](https://web.archive.org/web/20240213104049/https://mistral.ai/news/mixtral-of-experts/) from the original on 13 February 2024 . Retrieved 12 December 2023 .
84. ^ [***a***](#cite_ref-:1_90-0) [***b***](#cite_ref-:1_90-1) DeepSeek-AI; Bi, Xiao; Chen, Deli; Chen, Guanting; Chen, Shanhuang; Dai, Damai; Deng, Chengqi; Ding, Honghui; Dong, Kai (2024-01-05), *DeepSeek LLM: Scaling Open-Source Language Models with Longtermism* , [arXiv](/wiki/ArXiv_(identifier)) : [2401.02954](https://arxiv.org/abs/2401.02954)
85. ^ [***a***](#cite_ref-:9_91-0) [***b***](#cite_ref-:9_91-1) Hughes, Alyssa (12 December 2023). ["Phi-2: The surprising power of small language models"](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) . *Microsoft Research* . [Archived](https://web.archive.org/web/20231212232647/https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) from the original on 12 December 2023 . Retrieved 13 December 2023 .
86. [**^**](#cite_ref-92) ["Our next-generation model: Gemini 1.5"](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window) . *Google* . 15 February 2024. [Archived](https://web.archive.org/web/20240216003052/https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window) from the original on 16 February 2024 . Retrieved 16 February 2024 . This means 1.5 Pro can process vast amounts of information in one go - including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we've also successfully tested up to 10 million tokens.
87. [**^**](#cite_ref-gemma_93-0) ["Gemma"](https://ai.google.dev/gemma/terms) - via GitHub.
88. [**^**](#cite_ref-94) ["Introducing the next generation of Claude"](https://www.anthropic.com/news/claude-3-family) . *www.anthropic.com* . [Archived](https://web.archive.org/web/20240304143650/https://www.anthropic.com/news/claude-3-family) from the original on 2024-03-04 . Retrieved 2024-03-04 .
89. [**^**](#cite_ref-95) ["Databricks Open Model License"](https://www.databricks.com/legal/open-model-license) . [*Databricks*](/wiki/Databricks) . 27 March 2024 . Retrieved 6 August 2025 .
90. [**^**](#cite_ref-96) ["Databricks Open Model Acceptable Use Policy"](https://www.databricks.com/legal/acceptable-use-policy-open-model) . [*Databricks*](/wiki/Databricks) . 27 March 2024 . Retrieved 6 August 2025 .
91. [**^**](#cite_ref-97) ["Fugaku-LLM Terms of Use"](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B/blob/main/LICENSE) . 23 April 2024 . Retrieved 6 August 2025 - via [Hugging Face](/wiki/Hugging_Face) .
92. [**^**](#cite_ref-98) ["Fugaku-LLM/Fugaku-LLM-13B · Hugging Face"](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B) . *huggingface.co* . [Archived](https://web.archive.org/web/20240517135225/https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B) from the original on 2024-05-17 . Retrieved 2024-05-17 .
93. [**^**](#cite_ref-99) Dickson, Ben (22 May 2024). ["Meta introduces Chameleon, a state-of-the-art multimodal model"](https://venturebeat.com/ai/meta-introduces-chameleon-a-state-of-the-art-multimodal-model/) . *VentureBeat* .
94. [**^**](#cite_ref-100) ["chameleon/LICENSE at e3b711ef63b0bb3a129cf0cf0918e36a32f26e2c · facebookresearch/chameleon"](https://github.com/facebookresearch/chameleon/blob/e3b711ef63b0bb3a129cf0cf0918e36a32f26e2c/LICENSE) . Meta Research . Retrieved 6 August 2025 - via [GitHub](/wiki/GitHub) .
95. [**^**](#cite_ref-101) AI, Mistral (2024-04-17). ["Cheaper, Better, Faster, Stronger"](https://mistral.ai/news/mixtral-8x22b/) . *mistral.ai* . [Archived](https://web.archive.org/web/20240505023828/https://mistral.ai/news/mixtral-8x22b/) from the original on 2024-05-05 . Retrieved 2024-05-05 .
96. [**^**](#cite_ref-102) ["Phi-3"](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms) . *azure.microsoft.com* . 23 April 2024. [Archived](https://web.archive.org/web/20240427043835/https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/) from the original on 2024-04-27 . Retrieved 2024-04-28 .
97. [**^**](#cite_ref-103) ["Phi-3 Model Documentation"](https://huggingface.co/docs/transformers/main/en/model_doc/phi3) . *huggingface.co* . [Archived](https://web.archive.org/web/20240513141513/https://huggingface.co/docs/transformers/main/en/model_doc/phi3) from the original on 2024-05-13 . Retrieved 2024-04-28 .
98. [**^**](#cite_ref-104) ["Qwen2"](https://github.com/QwenLM/Qwen2?spm=a3c0i.28768018.7084722650.1.5cd35c10NEqBXm&file=Qwen1.5) . [*GitHub*](/wiki/GitHub) . [Archived](https://web.archive.org/web/20240617072401/https://github.com/QwenLM/Qwen2?spm=a3c0i.28768018.7084722650.1.5cd35c10NEqBXm&file=Qwen1.5) from the original on 2024-06-17 . Retrieved 2024-06-17 .
99. [**^**](#cite_ref-105) DeepSeek-AI; Liu, Aixin; Feng, Bei; Wang, Bin; Wang, Bingxuan; Liu, Bo; Zhao, Chenggang; Dengr, Chengqi; Ruan, Chong (2024-06-19), *DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model* , [arXiv](/wiki/ArXiv_(identifier)) : [2405.04434](https://arxiv.org/abs/2405.04434)
100. [**^**](#cite_ref-106) ["NVIDIA Open Models License"](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) . [*Nvidia*](/wiki/Nvidia) . 16 June 2025 . Retrieved 6 August 2025 .
101. [**^**](#cite_ref-107) ["Trustworthy AI"](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/) . [*Nvidia*](/wiki/Nvidia) . 27 June 2024 . Retrieved 6 August 2025 .
102. [**^**](#cite_ref-108) ["nvidia/Nemotron-4-340B-Base · Hugging Face"](https://huggingface.co/nvidia/Nemotron-4-340B-Base) . *huggingface.co* . 2024-06-14. [Archived](https://web.archive.org/web/20240615010323/https://huggingface.co/nvidia/Nemotron-4-340B-Base) from the original on 2024-06-15 . Retrieved 2024-06-15 .
103. [**^**](#cite_ref-109) ["Nemotron-4 340B | Research"](https://research.nvidia.com/publication/2024-06_nemotron-4-340b) . *research.nvidia.com* . [Archived](https://web.archive.org/web/20240615010323/https://research.nvidia.com/publication/2024-06_nemotron-4-340b) from the original on 2024-06-15 . Retrieved 2024-06-15 .
104. [**^**](#cite_ref-110) ["Introducing Claude 3.5 Sonnet"](https://www.anthropic.com/news/claude-3-5-sonnet) . *www.anthropic.com* . Retrieved 8 August 2025 .
105. [**^**](#cite_ref-111) ["Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku"](https://www.anthropic.com/news/3-5-models-and-computer-use) . *www.anthropic.com* . Retrieved 8 August 2025 .
106. [**^**](#cite_ref-112) ["The Llama 3 Herd of Models" (July 23, 2024) Llama Team, AI @ Meta](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
107. [**^**](#cite_ref-113) ["llama-models/models/llama3\_1/MODEL\_CARD.md at main · meta-llama/llama-models"](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) . *GitHub* . [Archived](https://web.archive.org/web/20240723151851/https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) from the original on 2024-07-23 . Retrieved 2024-07-23 .
108. [**^**](#cite_ref-114) ["LICENSE · xai-org/grok-2 at main"](https://huggingface.co/xai-org/grok-2/blob/main/LICENSE) . 5 November 2025 . Retrieved 18 November 2025 - via [Hugging Face](/wiki/Hugging_Face) .
109. [**^**](#cite_ref-115) ["xAI Acceptable Use Policy"](https://x.ai/legal/acceptable-use-policy) . [*xAI*](/wiki/XAI_(company)) . 2 January 2025 . Retrieved 18 November 2025 .
110. [**^**](#cite_ref-116) Weatherbed, Jess (14 August 2024). ["xAI's new Grok-2 chatbots bring AI image generation to X"](https://www.theverge.com/2024/8/14/24220127/grok-ai-chatbot-beta-image-generation-x-xai-update) . [*The Verge*](/wiki/The_Verge) . Retrieved 18 November 2025 .
111. [**^**](#cite_ref-117) Ha, Anthony (24 August 2025). ["Elon Musk says xAI has open sourced Grok 2.5"](https://techcrunch.com/2025/08/24/elon-musk-says-xai-has-open-sourced-grok-2-5/) . [*TechCrunch*](/wiki/TechCrunch) . Retrieved 18 November 2025 .
112. [**^**](#cite_ref-118) ["Introducing OpenAI o1"](https://openai.com/o1/) . *openai.com* . Retrieved 8 August 2025 .
113. ^ [***a***](#cite_ref-Mistral_models_overview_119-0) [***b***](#cite_ref-Mistral_models_overview_119-1) ["Models Overview"](https://docs.mistral.ai/getting-started/models/models_overview/) . *mistral.ai* . Retrieved 2025-03-03 .
114. [**^**](#cite_ref-120) ["Phi-4 Model Card"](https://huggingface.co/microsoft/phi-4) . *huggingface.co* . Retrieved 2025-11-11 . `{{` [`cite web`](/wiki/Template:Cite_web) `}}` : CS1 maint: url-status ( [link](/wiki/Category:CS1_maint:_url-status) )
115. [**^**](#cite_ref-121) ["Introducing Phi-4: Microsoft's Newest Small Language Model Specializing in Complex Reasoning"](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090) . *techcommunity.microsoft.com* . Retrieved 2025-11-11 . `{{` [`cite web`](/wiki/Template:Cite_web) `}}` : CS1 maint: url-status ( [link](/wiki/Category:CS1_maint:_url-status) )
116. [**^**](#cite_ref-122) [*deepseek-ai/DeepSeek-V3*](https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file) , DeepSeek, 2024-12-26 , retrieved 2024-12-26
117. [**^**](#cite_ref-123) Feng, Coco (25 March 2025). ["DeepSeek wows coders with more powerful open-source V3 model"](https://www.scmp.com/tech/big-tech/article/3303798/deepseeks-upgraded-foundational-model-excels-coding-and-maths) . [*South China Morning Post*](/wiki/South_China_Morning_Post) . Retrieved 6 April 2025 .
118. [**^**](#cite_ref-124) [*Amazon Nova Micro, Lite, and Pro - AWS AI Service Cards3*](https://docs.aws.amazon.com/ai/responsible-ai/nova-micro-lite-pro/overview.html) , Amazon, 2024-12-27 , retrieved 2024-12-27
119. [**^**](#cite_ref-125) [*deepseek-ai/DeepSeek-R1*](https://github.com/deepseek-ai/DeepSeek-R1) , DeepSeek, 2025-01-21 , retrieved 2025-01-21
120. [**^**](#cite_ref-126) DeepSeek-AI; Guo, Daya; Yang, Dejian; Zhang, Haowei; Song, Junxiao; Zhang, Ruoyu; Xu, Runxin; Zhu, Qihao; Ma, Shirong (2025-01-22), *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning* , [arXiv](/wiki/ArXiv_(identifier)) : [2501.12948](https://arxiv.org/abs/2501.12948)
121. [**^**](#cite_ref-127) Qwen; Yang, An; Yang, Baosong; Zhang, Beichen; Hui, Binyuan; Zheng, Bo; Yu, Bowen; Li, Chengyuan; Liu, Dayiheng (2025-01-03), *Qwen2.5 Technical Report* , [arXiv](/wiki/ArXiv_(identifier)) : [2412.15115](https://arxiv.org/abs/2412.15115)
122. ^ [***a***](#cite_ref-:0_128-0) [***b***](#cite_ref-:0_128-1) MiniMax; Li, Aonian; Gong, Bangwei; Yang, Bo; Shan, Boji; Liu, Chang; Zhu, Cheng; Zhang, Chunhao; Guo, Congchao (2025-01-14), *MiniMax-01: Scaling Foundation Models with Lightning Attention* , [arXiv](/wiki/ArXiv_(identifier)) : [2501.08313](https://arxiv.org/abs/2501.08313)
123. [**^**](#cite_ref-129) [*MiniMax-AI/MiniMax-01*](https://github.com/MiniMax-AI/MiniMax-01?tab=readme-ov-file) , MiniMax, 2025-01-26 , retrieved 2025-01-26
124. [**^**](#cite_ref-130) Kavukcuoglu, Koray (5 February 2025). ["Gemini 2.0 is now available to everyone"](https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/) . *Google* . Retrieved 6 February 2025 .
125. [**^**](#cite_ref-131) ["Gemini 2.0: Flash, Flash-Lite and Pro"](https://developers.googleblog.com/en/gemini-2-family-expands/) . *Google for Developers* . Retrieved 6 February 2025 .
126. [**^**](#cite_ref-132) Franzen, Carl (5 February 2025). ["Google launches Gemini 2.0 Pro, Flash-Lite and connects reasoning model Flash Thinking to YouTube, Maps and Search"](https://venturebeat.com/ai/google-launches-gemini-2-0-pro-flash-lite-and-connects-reasoning-model-flash-thinking-to-youtube-maps-and-search/) . *VentureBeat* . Retrieved 6 February 2025 .
127. [**^**](#cite_ref-133) ["Claude 3.7 Sonnet and Claude Code"](https://www.anthropic.com/news/claude-3-7-sonnet) . *www.anthropic.com* . Retrieved 8 August 2025 .
128. [**^**](#cite_ref-134) ["Introducing GPT-4.5"](https://openai.com/index/introducing-gpt-4-5/) . *openai.com* . Retrieved 8 August 2025 .
129. [**^**](#cite_ref-135) ["Grok 3 Beta - The Age of Reasoning Agents"](https://x.ai/blog/grok-3) . *x.ai* . Retrieved 2025-02-22 .
130. [**^**](#cite_ref-136) Kavukcuoglu, Koray (25 March 2025). ["Gemini 2.5: Our most intelligent AI model"](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) . *Google* . Retrieved 23 September 2025 .
131. [**^**](#cite_ref-137) ["meta-llama/Llama-4-Maverick-17B-128E · Hugging Face"](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E) . *huggingface.co* . 2025-04-05 . Retrieved 2025-04-06 .
132. [**^**](#cite_ref-138) ["The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation"](http://web.archive.org/web/20250405185132/https://ai.meta.com/blog/llama-4-multimodal-intelligence/) . *ai.meta.com* . Archived from [the original](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) on 2025-04-05 . Retrieved 2025-04-05 .
133. [**^**](#cite_ref-139) ["Introducing OpenAI o3 and o4-mini"](https://openai.com/index/introducing-o3-and-o4-mini/) . *openai.com* . Retrieved 8 August 2025 .
134. [**^**](#cite_ref-140) Team, Qwen (2025-04-29). ["Qwen3: Think Deeper, Act Faster"](https://qwenlm.github.io/blog/qwen3/) . *Qwen* . Retrieved 2025-04-29 .
135. [**^**](#cite_ref-141) ["Introducing Claude 4"](https://www.anthropic.com/news/claude-4) . *www.anthropic.com* . Retrieved 8 August 2025 .
136. [**^**](#cite_ref-142) ["zai-org/GLM-4.5 · Hugging Face"](https://huggingface.co/zai-org/GLM-4.5) . *huggingface.co* . 2025-08-04 . Retrieved 2025-08-06 .
137. [**^**](#cite_ref-143) ["GLM-4.5: Reasoning, Coding, and Agentic Abililties"](https://z.ai/blog/glm-4.5) . *z.ai* . Retrieved 2025-08-06 .
138. [**^**](#cite_ref-144) Whitwam, Ryan (5 August 2025). ["OpenAI announces two "gpt-oss" open AI models, and you can download them today"](https://arstechnica.com/ai/2025/08/openai-releases-its-first-open-source-models-since-2019/) . [*Ars Technica*](/wiki/Ars_Technica) . Retrieved 6 August 2025 .
139. [**^**](#cite_ref-145) ["Claude Opus 4.1"](https://www.anthropic.com/news/claude-opus-4-1) . *www.anthropic.com* . Retrieved 8 August 2025 .
140. [**^**](#cite_ref-146) ["Introducing GPT-5"](https://openai.com/index/introducing-gpt-5/) . *openai.com* . 7 August 2025 . Retrieved 8 August 2025 .
141. [**^**](#cite_ref-147) ["OpenAI Platform: GPT-5 Model Documentation"](https://platform.openai.com/docs/models/gpt-5) . *openai.com* . Retrieved 18 August 2025 .
142. [**^**](#cite_ref-148) ["deepseek-ai/DeepSeek-V3.1 · Hugging Face"](https://huggingface.co/deepseek-ai/DeepSeek-V3.1) . *huggingface.co* . 2025-08-21 . Retrieved 2025-08-25 .
143. [**^**](#cite_ref-149) ["DeepSeek-V3.1 Release | DeepSeek API Docs"](https://api-docs.deepseek.com/news/news250821) . *api-docs.deepseek.com* . Retrieved 2025-08-25 .
144. [**^**](#cite_ref-150) ["Apertus: Ein vollständig offenes, transparentes und mehrsprachiges Sprachmodell"](https://ethz.ch/de/news-und-veranstaltungen/eth-news/news/2025/09/medienmitteilung-apertus-ein-vollstaendig-offenes-transparentes-und-mehrsprachiges-sprachmodell.html) (in German). Zürich: ETH Zürich. 2025-09-02 . Retrieved 2025-11-07 .
145. [**^**](#cite_ref-151) Kirchner, Malte (2025-09-02). ["Apertus: Schweiz stellt erstes offenes und mehrsprachiges KI-Modell vor"](https://www.heise.de/news/Apertus-Schweiz-stellt-erstes-offenes-und-mehrsprachiges-KI-Modell-vor-10629412.html) . *heise online* (in German) . Retrieved 2025-11-07 .
146. [**^**](#cite_ref-152) ["Introducing Claude Sonnet 4.5"](https://www.anthropic.com/news/claude-sonnet-4-5) . *www.anthropic.com* . Retrieved 29 September 2025 .
147. [**^**](#cite_ref-153) ["Introducing DeepSeek-V3.2-Exp | DeepSeek API Docs"](https://api-docs.deepseek.com/news/news250929) . *api-docs.deepseek.com* . Retrieved 2025-10-01 .
148. [**^**](#cite_ref-154) ["deepseek-ai/DeepSeek-V3.2-Exp · Hugging Face"](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp) . *huggingface.co* . 2025-09-29 . Retrieved 2025-10-01 .
149. [**^**](#cite_ref-155) ["DeepSeek-V3.2-Exp/DeepSeek\_V3\_2.pdf at main · deepseek-ai/DeepSeek-V3.2-Exp"](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf) (PDF) . *GitHub* . Retrieved 2025-10-01 .
150. [**^**](#cite_ref-156) ["GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities"](https://z.ai/blog/glm-4.6) . *z.ai* . Retrieved 2025-10-01 .
151. [**^**](#cite_ref-157) ["zai-org/GLM-4.6 · Hugging Face"](https://huggingface.co/zai-org/GLM-4.6) . *huggingface.co* . 2025-09-30 . Retrieved 2025-10-01 .
152. [**^**](#cite_ref-158) ["GLM-4.6"](https://modelscope.cn/models/ZhipuAI/GLM-4.6) . *modelscope.cn* . Retrieved 2025-10-01 .

| - [v](/wiki/Template:Natural_language_processing) - [t](/wiki/Template_talk:Natural_language_processing) - [e](/wiki/Special:EditPage/Template:Natural_language_processing)  [Natural language processing](/wiki/Natural_language_processing)   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| General terms                                                                                                                                                                                                                                   | - [AI-complete](/wiki/AI-complete) - [Bag-of-words](/wiki/Bag-of-words_model) - [*n*](/wiki/N-gram) [-gram](/wiki/N-gram)     - [Bigram](/wiki/Bigram)     - [Trigram](/wiki/Trigram) - [Computational linguistics](/wiki/Computational_linguistics) - [Natural language understanding](/wiki/Natural_language_understanding) - [Stop words](/wiki/Stop_word) - [Text processing](/wiki/Text_processing)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| [Text analysis](/wiki/Text_mining)                                                                                                                                                                                                              | - [Argument mining](/wiki/Argument_mining) - [Collocation extraction](/wiki/Collocation_extraction) - [Concept mining](/wiki/Concept_mining) - [Coreference resolution](/wiki/Coreference#Coreference_resolution) - [Deep linguistic processing](/wiki/Deep_linguistic_processing) - [Distant reading](/wiki/Distant_reading) - [Information extraction](/wiki/Information_extraction) - [Named-entity recognition](/wiki/Named-entity_recognition) - [Ontology learning](/wiki/Ontology_learning) - [Parsing](/wiki/Parsing)     - [semantic](/wiki/Semantic_parsing)     - [syntactic](/wiki/Syntactic_parsing_(computational_linguistics)) - [Part-of-speech tagging](/wiki/Part-of-speech_tagging) - [Semantic analysis](/wiki/Semantic_analysis_(machine_learning)) - [Semantic role labeling](/wiki/Semantic_role_labeling) - [Semantic decomposition](/wiki/Semantic_decomposition_(natural_language_processing)) - [Semantic similarity](/wiki/Semantic_similarity) - [Sentiment analysis](/wiki/Sentiment_analysis)  - [Terminology extraction](/wiki/Terminology_extraction) - [Text mining](/wiki/Text_mining) - [Textual entailment](/wiki/Textual_entailment) - [Truecasing](/wiki/Truecasing) - [Word-sense disambiguation](/wiki/Word-sense_disambiguation) - [Word-sense induction](/wiki/Word-sense_induction)  | [Text segmentation](/wiki/Text_segmentation)   | - [Compound-term processing](/wiki/Compound-term_processing) - [Lemmatisation](/wiki/Lemmatisation) - [Lexical analysis](/wiki/Lexical_analysis) - [Text chunking](/wiki/Shallow_parsing) - [Stemming](/wiki/Stemming) - [Sentence segmentation](/wiki/Sentence_boundary_disambiguation) - [Word segmentation](/wiki/Word#Word_boundaries)   | |------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|                                    |
| [Automatic summarization](/wiki/Automatic_summarization)                                                                                                                                                                                        | - [Multi-document summarization](/wiki/Multi-document_summarization) - [Sentence extraction](/wiki/Sentence_extraction) - [Text simplification](/wiki/Text_simplification)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| [Machine translation](/wiki/Machine_translation)                                                                                                                                                                                                | - [Computer-assisted](/wiki/Computer-assisted_translation) - [Example-based](/wiki/Example-based_machine_translation) - [Rule-based](/wiki/Rule-based_machine_translation) - [Statistical](/wiki/Statistical_machine_translation) - [Transfer-based](/wiki/Transfer-based_machine_translation) - [Neural](/wiki/Neural_machine_translation)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| [Distributional semantics](/wiki/Distributional_semantics)  models                                                                                                                                                                              | - [BERT](/wiki/BERT_(language_model)) - [Document-term matrix](/wiki/Document-term_matrix) - [Explicit semantic analysis](/wiki/Explicit_semantic_analysis) - [fastText](/wiki/FastText) - [GloVe](/wiki/GloVe) - [Language model](/wiki/Language_model)     - [large](/wiki/Large_language_model)     - [small](/wiki/Small_language_model) - [Latent semantic analysis](/wiki/Latent_semantic_analysis) - [Long short-term memory](/wiki/Long_short-term_memory) - [Seq2seq](/wiki/Seq2seq) - [Transformer](/wiki/Transformer_(deep_learning_architecture)) - [Word embedding](/wiki/Word_embedding) - [Word2vec](/wiki/Word2vec)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [Language resources](/wiki/Language_resource)  ,  datasets and corpora                                                                                                                                                                          | | Types and  standards   | - [Corpus linguistics](/wiki/Corpus_linguistics) - [Lexical resource](/wiki/Lexical_resource) - [Linguistic Linked Open Data](/wiki/Linguistic_Linked_Open_Data) - [Machine-readable dictionary](/wiki/Machine-readable_dictionary) - [Parallel text](/wiki/Parallel_text) - [PropBank](/wiki/PropBank) - [Semantic network](/wiki/Semantic_network) - [Simple Knowledge Organization System](/wiki/Simple_Knowledge_Organization_System) - [Speech corpus](/wiki/Speech_corpus) - [Text corpus](/wiki/Text_corpus) - [Thesaurus (information retrieval)](/wiki/Thesaurus_(information_retrieval)) - [Treebank](/wiki/Treebank) - [Universal Dependencies](/wiki/Universal_Dependencies)   | |------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Data                   | - [BabelNet](/wiki/BabelNet) - [Bank of English](/wiki/Bank_of_English) - [DBpedia](/wiki/DBpedia) - [FrameNet](/wiki/FrameNet) - [Google Ngram Viewer](/wiki/Google_Ngram_Viewer) - [UBY](/wiki/UBY) - [WordNet](/wiki/WordNet) - [Wikidata](/wiki/Wikidata)                                                                                                                                                                                                                                                                                                                                                                                                                              | |
| [Automatic identification](/wiki/Automatic_identification_and_data_capture)  [and data capture](/wiki/Automatic_identification_and_data_capture)                                                                                                | - [Speech recognition](/wiki/Speech_recognition) - [Speech segmentation](/wiki/Speech_segmentation) - [Speech synthesis](/wiki/Speech_synthesis) - [Natural language generation](/wiki/Natural_language_generation) - [Optical character recognition](/wiki/Optical_character_recognition)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| [Topic model](/wiki/Topic_model)                                                                                                                                                                                                                | - [Document classification](/wiki/Document_classification) - [Latent Dirichlet allocation](/wiki/Latent_Dirichlet_allocation) - [Pachinko allocation](/wiki/Pachinko_allocation)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| [Computer-assisted](/wiki/Computer-assisted_reviewing)  [reviewing](/wiki/Computer-assisted_reviewing)                                                                                                                                          | - [Automated essay scoring](/wiki/Automated_essay_scoring) - [Concordancer](/wiki/Concordancer) - [Grammar checker](/wiki/Grammar_checker) - [Predictive text](/wiki/Predictive_text) - [Pronunciation assessment](/wiki/Pronunciation_assessment) - [Spell checker](/wiki/Spell_checker)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| [Natural language](/wiki/Natural-language_user_interface)  [user interface](/wiki/Natural-language_user_interface)                                                                                                                              | - [Chatbot](/wiki/Chatbot) - [Interactive fiction](/wiki/Interactive_fiction) - [Question answering](/wiki/Question_answering) - [Virtual assistant](/wiki/Virtual_assistant) - [Voice user interface](/wiki/Voice_user_interface)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Related                                                                                                                                                                                                                                         | - [Formal semantics](/wiki/Formal_semantics_(natural_language)) - [Hallucination](/wiki/Hallucination_(artificial_intelligence)) - [Natural Language Toolkit](/wiki/Natural_Language_Toolkit) - [spaCy](/wiki/SpaCy)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |

[Portal](/wiki/Wikipedia:Contents/Portals) :

- [Language](/wiki/Portal:Language)
icon

<!-- 🖼️❌ Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->

Retrieved from " [https://en.wikipedia.org/w/index.php?title=List\_of\_large\_language\_models&amp;oldid=1325091032](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&oldid=1325091032) "

[Categories](/wiki/Help:Category)

:

- [Software comparisons](/wiki/Category:Software_comparisons)
- [Large language models](/wiki/Category:Large_language_models)

Hidden categories:

- [Pages using the EasyTimeline extension](/wiki/Category:Pages_using_the_EasyTimeline_extension)
- [Webarchive template wayback links](/wiki/Category:Webarchive_template_wayback_links)
- [CS1: long volume value](/wiki/Category:CS1:_long_volume_value)
- [CS1 maint: url-status](/wiki/Category:CS1_maint:_url-status)
- [CS1 German-language sources (de)](/wiki/Category:CS1_German-language_sources_(de))
- [Articles with short description](/wiki/Category:Articles_with_short_description)
- [Short description is different from Wikidata](/wiki/Category:Short_description_is_different_from_Wikidata)
- [Dynamic lists](/wiki/Category:Dynamic_lists)

Search

Search

Toggle the table of contents

List of large language models

4 languages

[Add topic](#)