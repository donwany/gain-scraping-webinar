[Jump to content](https://en.wikipedia.org/wiki/List_of_large_language_models#bodyContent)
Main menu
Main menu
move to sidebar hide
Navigation 
  * [Main page](https://en.wikipedia.org/wiki/Main_Page "Visit the main page \[alt-z\]")
  * [Contents](https://en.wikipedia.org/wiki/Wikipedia:Contents "Guides to browsing Wikipedia")
  * [Current events](https://en.wikipedia.org/wiki/Portal:Current_events "Articles related to current events")
  * [Random article](https://en.wikipedia.org/wiki/Special:Random "Visit a randomly selected article \[alt-x\]")
  * [About Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:About "Learn about Wikipedia and how it works")
  * [Contact us](https://en.wikipedia.org/wiki/Wikipedia:Contact_us "How to contact Wikipedia")


Contribute 
  * [Help](https://en.wikipedia.org/wiki/Help:Contents "Guidance on how to use and edit Wikipedia")
  * [Learn to edit](https://en.wikipedia.org/wiki/Help:Introduction "Learn how to edit Wikipedia")
  * [Community portal](https://en.wikipedia.org/wiki/Wikipedia:Community_portal "The hub for editors")
  * [Recent changes](https://en.wikipedia.org/wiki/Special:RecentChanges "A list of recent changes to Wikipedia \[alt-r\]")
  * [Upload file](https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard "Add images or other media for use on Wikipedia")
  * [Special pages](https://en.wikipedia.org/wiki/Special:SpecialPages)


[ ![](https://en.wikipedia.org/static/images/icons/wikipedia.png) ![Wikipedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg) ![The Free Encyclopedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-tagline-en.svg) ](https://en.wikipedia.org/wiki/Main_Page)
[Search ](https://en.wikipedia.org/wiki/Special:Search "Search Wikipedia \[alt-f\]")
Search
Appearance
Appearance
move to sidebar hide
Text
  * Small
Standard
Large

This page always uses small font size
Width
  * Standard
Wide

The content is as wide as possible for your browser window.
Color (beta)
  * Automatic
Light
Dark

This page is always in light mode.
  * [Donate](https://donate.wikimedia.org/?wmf_source=donate&wmf_medium=sidebar&wmf_campaign=en.wikipedia.org&uselang=en)
  * [Create account](https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=List+of+large+language+models "You are encouraged to create an account and log in; however, it is not mandatory")
  * [Log in](https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=List+of+large+language+models "You're encouraged to log in; however, it's not mandatory. \[alt-o\]")


Personal tools
  * [Donate](https://donate.wikimedia.org/?wmf_source=donate&wmf_medium=sidebar&wmf_campaign=en.wikipedia.org&uselang=en)
  * [Create account](https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=List+of+large+language+models "You are encouraged to create an account and log in; however, it is not mandatory")
  * [Log in](https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=List+of+large+language+models "You're encouraged to log in; however, it's not mandatory. \[alt-o\]")


December 2: Readers in the United States deserve an explanation. 
Please don't skip this 1-minute read. It's Tuesday, December 2, and if you're like us, you've used Wikipedia countless times. To settle an argument with a friend. To satisfy a curiosity. Whether it's 3 in the morning or afternoon, Wikipedia is useful in your life. Please give $2.75. 
Wikipedia's been around since 2001. Back then, it was just a wildly ambitious, probably impossible dream. But it came together piece by piece‚Äîcreated by people, not machines. Wikipedia's not perfect, but it's always been free thanks to everyday readers. 
Only 2% ever donate. But that small group makes a big difference. When you support Wikipedia, you're standing up for the internet we were promised, where knowledge belongs to everyone. 
If you agree, then this is your moment to give back. Even $2.75 makes a difference. Help keep it going‚Äîfor you, for the next reader, and for the next generation. 
25 years of the internet at its **best**
How often would you like to donate? 
  * Once
  * Monthly
  * Annual

Please select an amount (USD)  
The average donation in the United States is around $13.
  * $2.75
  * $15
  * $25
Celebrate & Give üéâ
  * $50
  * $100
  * $250
  * $500
  * Other amount Other


I'll generously add a little to cover the transaction fees so you can keep 100% of my donation.
Please select a payment method 
![](https://upload.wikimedia.org/wikipedia/donate/b/be/Trustly_logos_only.png) Online Banking
Credit / Debit Card Visa Mastercard American Express JCB Discover
PayPal
Venmo
Apple Pay
Google Pay
Continue Donate  Donate  Donate 
Please select an amount (minimum $1)
We cannot accept donations greater than 25000 USD through our website. Please contact our major gifts staff at benefactors@wikimedia.org.
Please select a payment method
Maybe later 
Can we follow up and let you know if we need your help again? The support and advice we get from donors in the United States is priceless, but many donors don't let us stay in touch. Will you commit today, this Tuesday, to staying in touch with the Wikimedia Foundation?
  * Yes
  * No


Sorry to hear that. We don't email often; would you consider changing your mind?
Thanks for changing your mind! We‚Äôll respect your inbox.
Your information is handled in accordance with our [donor privacy policy](https://foundation.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_privacy_policy&language=en), and each email you receive will include easy unsubscribe options. 
Continue 
Please select an email option
Monthly support is the best way to ensure that Wikipedia keeps thriving.
No thanks! I'll make a one-time donation of  Yes, I'll donate  Yes, I'll donate monthly, but for a different amount 
Thank you for your support!
Enter your monthly donation amount
Please select an amount (minimum $1)
We cannot accept donations greater than 25000 USD through our website. Please contact our major gifts staff at benefactors@wikimedia.org.
Donate 
[Problems donating?](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Problems_donating&country=US&language=en&uselang=en&wmf_medium=sitenotice&wmf_campaign=WMF_FR_C2526_en6C_dsk_1202&wmf_source=B2526_120220_en6C_dsk_p1_lg_cvt_cnt) | [Frequently asked questions](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=FAQ&country=US&language=en&uselang=en&wmf_medium=sitenotice&wmf_campaign=WMF_FR_C2526_en6C_dsk_1202&wmf_source=B2526_120220_en6C_dsk_p1_lg_cvt_cnt) | Where your donation goes | [Other ways to give](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Ways_to_Give&country=US&language=en&uselang=en&wmf_medium=sitenotice&wmf_campaign=WMF_FR_C2526_en6C_dsk_1202&wmf_source=B2526_120220_en6C_dsk_p1_lg_cvt_cnt) |  I already donated
We never sell your information. By submitting, you are agreeing to our [donor privacy policy](https://foundation.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_privacy_policy&language=en) and to sharing your information with the [Wikimedia Foundation](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_deductibility&language=en) and its service providers in the USA and elsewhere. Donations to the Wikimedia Foundation are likely not tax-deductible outside the USA. We never sell your information. By submitting, you are agreeing to our [donor privacy policy](https://foundation.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_privacy_policy&language=en). The Wikimedia Foundation is a nonprofit, [tax-exempt organization](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_deductibility&language=en). We never sell your information. By submitting, you are agreeing to our [donor privacy policy](https://foundation.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_privacy_policy&language=en) and to sharing your information with the Wikimedia Foundation and its service providers in the U.S. and elsewhere. The Wikimedia Foundation is a recognized [public welfare institution (ANBI)](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_deductibility&language=en). If you make a recurring donation, you will be debited by the Wikimedia Foundation until you notify us to stop. We‚Äôll send you an email which will include a link to [easy cancellation instructions](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Cancel_or_change_recurring_giving&language=en).
$2.75? 
No, but maybe later when I have more time Yes, I'll donate $2.75
How would you like to be reminded?
Whether you give now or later, any contribution helps. We can send you an email or text message reminder to join the 2% of readers who donate.
Send me an email Send me a text message
Send me an email reminder 
We‚Äôll gladly send you an email reminder and get out of your way so you can get back to reading. 
Email address Submit
Please enter a valid email address i.e. name@domain.com 
We never sell your information. By submitting, you are agreeing to our [donor privacy policy](https://foundation.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_privacy_policy&language=en). The Wikimedia Foundation is a nonprofit, [tax-exempt organization](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_deductibility&language=en). We never sell your information. By submitting, you are agreeing to our [donor privacy policy](https://foundation.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_privacy_policy&language=en) and to sharing your information with the Wikimedia Foundation and its service providers in the U.S. and elsewhere. The Wikimedia Foundation is a recognized [public welfare institution (ANBI)](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_deductibility&language=en). We never sell your information. By submitting, you are agreeing to our [donor privacy policy](https://foundation.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Donor_privacy_policy&language=en) and to sharing your information with the [Wikimedia Foundation](https://donate.wikimedia.org/wiki/Special:LandingCheck?basic=true&landing_page=Tax_deductibility&language=en) and its service providers in the USA and elsewhere. Donations to the Wikimedia Foundation are likely not tax-deductible outside the USA.
Send me a reminder
We‚Äôll gladly send you a reminder and get out of your way so you can get back to reading.
Mobile phone number I would like to receive text messages such as donation reminders and appeals from Wikimedia at the number I have provided.  Submit
Please enter a valid phone number e.g. (201) 555-0123 
Please check the box to consent to receive messages. 
By participating, you consent to receive recurring updates through automated text messages from Wikimedia to the phone number you provide. Message frequency varies. For text messages, Msg&Data rates may apply. Text STOP to cancel or HELP for help. [Terms of Service and Privacy Policy.](https://foundation.wikimedia.org/wiki/Policy:Donor_SMS_Supplementary_Terms)
Thank you! We will send you a reminder. 
Thank you for donating recently! 
Your support means the world to us. We'll hide banners in this browser for the rest of our campaign. 
Want to show off your support? **Donors get 20% off Wikipedia Store merchandise automatically applied at checkout.** Click the button below to shop hats, tees, and more! 
Go to the Wikipedia Store 
Close 
Other ways to give
[ ![](https://upload.wikimedia.org/wikipedia/donate/7/73/Badge-daf.png) Donor-Advised Fund (DAF) Unlock tax benefits by directing your donation via your Donor-Advised Fund (DAF) ](https://donate.wikimedia.org/wiki/Donor_advised_funds) [ ![](https://upload.wikimedia.org/wikipedia/donate/a/ad/Badge-ira.png) Individual Retirement Account (IRA) Qualified Charitable Distributions from a tax efficient eligible IRA ](https://donate.wikimedia.org/wiki/IRA_Charitable_Rollover) [ ![](https://upload.wikimedia.org/wikipedia/donate/d/de/Badge-workplace-giving.png) Workplace Giving Involve your employer and increase the impact of your donation ](https://donate.wikimedia.org/wiki/Workplace_giving)
[More ways to give](https://donate.wikimedia.org/wiki/Ways_to_Give)
Where your donation goes
Accountability and transparency are core values at the Wikimedia Foundation. We manage funds and resources to ensure that every contribution supports our mission. We have earned the Platinum Seal of Transparency from Candid (formerly GuideStar), and Charity Navigator awarded us its highest rating. You can [read our most recent annual report](https://annual.wikimedia.org/) for more information about our financial health. 
45% $80.3M Technology 34% $60.7M Support 12% $21.4M General 11% $19.6M Fundraising $178.4M Total Funding
Investment in Technology 45%
Nearly half of our budget goes toward supporting the technology that powers Wikipedia and other Wikimedia projects. We are constantly working to enhance the user experience for both contributors and readers, improve site security, and ensure reliable access to our websites globally. This infrastructure and product support sustain one of the top ten most visited websites in the world, all at a fraction of the cost of popular for-profit websites. 
Support for Volunteers 34%
The global reach of Wikimedia projects is made possible by the hard work of volunteers from across the globe. We provide grants, legal support, and other resources to help build vibrant volunteer communities. Additionally, we promote community engagement through outreach initiatives and advocate for the growth and protection of free knowledge. 
General and Administrative Expenses 12%
Operational costs are essential for the smooth management and governance of the Wikimedia Foundation. These expenses help us recruit top talent and support staff around the world, empowering them to carry out the mission of the Wikimedia Foundation. 
Allocation to Fundraising Efforts 11%
Donor support is crucial to sustaining Wikipedia and our other free knowledge endeavors. Our team is committed to efficient and effective fundraising throughout the year, ensuring that every contribution helps advance our mission. 
Toggle the table of contents
## Contents
move to sidebar hide
  * [ (Top) ](https://en.wikipedia.org/wiki/List_of_large_language_models)
  * [ 1 List ](https://en.wikipedia.org/wiki/List_of_large_language_models#List)
  * [ 2 Timeline ](https://en.wikipedia.org/wiki/List_of_large_language_models#Timeline)
  * [ 3 See also ](https://en.wikipedia.org/wiki/List_of_large_language_models#See_also)
  * [ 4 Notes ](https://en.wikipedia.org/wiki/List_of_large_language_models#Notes)
  * [ 5 References ](https://en.wikipedia.org/wiki/List_of_large_language_models#References)


# List of large language models
4 languages
  * [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://ar.wikipedia.org/wiki/%D9%82%D8%A7%D8%A6%D9%85%D8%A9_%D8%A7%D9%84%D9%86%D9%85%D8%A7%D8%B0%D8%AC_%D8%A7%D9%84%D9%84%D8%BA%D9%88%D9%8A%D8%A9_%D8%A7%D9%84%D9%83%D8%A8%D9%8A%D8%B1%D8%A9 "ŸÇÿßÿ¶ŸÖÿ© ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑŸÑÿ∫ŸàŸäÿ© ÿßŸÑŸÉÿ®Ÿäÿ±ÿ© ‚Äì Arabic")
  * [Fran√ßais](https://fr.wikipedia.org/wiki/Liste_de_grands_mod%C3%A8les_de_langage "Liste de grands mod√®les de langage ‚Äì French")
  * [–°—Ä–ø—Å–∫–∏ / srpski](https://sr.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B8%D1%81%D0%B0%D0%BA_%D0%B2%D0%B5%D0%BB%D0%B8%D0%BA%D0%B8%D1%85_%D1%98%D0%B5%D0%B7%D0%B8%D1%87%D0%BA%D0%B8%D1%85_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B0 "–°–ø–∏—Å–∞–∫ –≤–µ–ª–∏–∫–∏—Ö —ò–µ–∑–∏—á–∫–∏—Ö –º–æ–¥–µ–ª–∞ ‚Äì Serbian")
  * [‰∏≠Êñá](https://zh.wikipedia.org/wiki/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%88%97%E8%A1%A8 "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂàóË°® ‚Äì Chinese")


[Edit links](https://www.wikidata.org/wiki/Special:EntityPage/Q131598147#sitelinks-wikipedia "Edit interlanguage links")
  * [Article](https://en.wikipedia.org/wiki/List_of_large_language_models "View the content page \[alt-c\]")
  * [Talk](https://en.wikipedia.org/wiki/Talk:List_of_large_language_models "Discuss improvements to the content page \[alt-t\]")


English
  * [Read](https://en.wikipedia.org/wiki/List_of_large_language_models)
  * [Edit](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=edit "Edit this page \[alt-e\]")
  * [View history](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=history "Past revisions of this page \[alt-h\]")


Tools
Tools
move to sidebar hide
Actions 
  * [Read](https://en.wikipedia.org/wiki/List_of_large_language_models)
  * [Edit](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=edit "Edit this page \[alt-e\]")
  * [View history](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=history)


General 
  * [What links here](https://en.wikipedia.org/wiki/Special:WhatLinksHere/List_of_large_language_models "List of all English Wikipedia pages containing links to this page \[alt-j\]")
  * [Related changes](https://en.wikipedia.org/wiki/Special:RecentChangesLinked/List_of_large_language_models "Recent changes in pages linked from this page \[alt-k\]")
  * [Upload file](https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard "Upload files \[alt-u\]")
  * [Permanent link](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&oldid=1325091032 "Permanent link to this revision of this page")
  * [Page information](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=info "More information about this page")
  * [Cite this page](https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=List_of_large_language_models&id=1325091032&wpFormIdentifier=titleform "Information on how to cite this page")
  * [Get shortened URL](https://en.wikipedia.org/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FList_of_large_language_models)
  * [Download QR code](https://en.wikipedia.org/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FList_of_large_language_models)
  * [Expand all](https://en.wikipedia.org/wiki/List_of_large_language_models "Expand all collapsible elements on the current page")
  * [Edit interlanguage links](https://www.wikidata.org/wiki/Special:EntityPage/Q131598147#sitelinks-wikipedia "Edit interlanguage links")


Print/export 
  * [Download as PDF](https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf&page=List_of_large_language_models&action=show-download-screen "Download this page as a PDF file")
  * [Printable version](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&printable=yes "Printable version of this page \[alt-p\]")


In other projects 
  * [Wikidata item](https://www.wikidata.org/wiki/Special:EntityPage/Q131598147 "Structured data on this page hosted by Wikidata \[alt-g\]")


From Wikipedia, the free encyclopedia
This is a [dynamic list](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Lists#Dynamic_lists "Wikipedia:WikiProject Lists") and may never be able to satisfy particular standards for completeness. You can help by [editing the page](https://en.wikipedia.org/wiki/Special:EditPage/List_of_large_language_models "Special:EditPage/List of large language models") to add missing items, with references to [reliable sources](https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources "Wikipedia:Reliable sources").
A [large language model](https://en.wikipedia.org/wiki/Large_language_model "Large language model") (LLM) is a type of [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning") [model](https://en.wikipedia.org/wiki/Model#Conceptual_model "Model") designed for [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing "Natural language processing") tasks such as language [generation](https://en.wikipedia.org/wiki/Generative_artificial_intelligence "Generative artificial intelligence"). LLMs are [language models](https://en.wikipedia.org/wiki/Language_model "Language model") with many parameters, and are trained with [self-supervised learning](https://en.wikipedia.org/wiki/Self-supervised_learning "Self-supervised learning") on a vast amount of text. 
## List
[[edit](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=edit&section=1 "Edit section: List")]
For the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec √ó 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written. 
Name | Release date[[a]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-1) | Developer | Number of parameters (billion) [[b]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-2) | Corpus size  | Training cost (petaFLOP-day) | License[[c]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-3) | Notes   
---|---|---|---|---|---|---|---  
[GPT-1](https://en.wikipedia.org/wiki/GPT-1 "GPT-1") |  June 11, 2018 |  [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") |  0.117 | Unknown  | 1[[1]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-oai-unsup-4) |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License")[[2]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-5) | First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit "Graphics processing unit").[[3]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-6)  
[BERT](https://en.wikipedia.org/wiki/BERT_\(language_model\) "BERT \(language model\)") | October 2018 | [Google](https://en.wikipedia.org/wiki/Google "Google") |  0.340[[4]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-bert-paper-7) |  3.3 billion words[[4]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-bert-paper-7) |  9[[5]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-bHZJ2-8) |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0")[[6]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-bert-web-9) | An early and influential language model.[[7]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Manning-2022-10)[Encoder-only](https://en.wikipedia.org/wiki/Transformer_\(deep_learning_architecture\)#encoder-only "Transformer \(deep learning architecture\)") and thus not built to be prompted or generative.[[8]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Ir545-11) Training took 4 days on 64 TPUv2 chips.[[9]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:02-12)  
[T5](https://en.wikipedia.org/wiki/T5_\(language_model\) "T5 \(language model\)") |  October 2019 |  [Google](https://en.wikipedia.org/wiki/Google "Google") |  11[[10]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:6-13) | 34 billion tokens[[10]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:6-13) |  |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0")[[11]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-14) | Base model for many Google projects, such as Imagen.[[12]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-15)  
[XLNet](https://en.wikipedia.org/wiki/XLNet "XLNet") | June 2019 | [Google](https://en.wikipedia.org/wiki/Google "Google") |  0.340[[13]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-16) |  33 billion words  | 330 |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0")[[14]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-xlnet-17) | An alternative to BERT; designed as encoder-only. Trained on 512 TPU v3 chips for 5.5 days.[[15]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-LX3rI-18)  
[GPT-2](https://en.wikipedia.org/wiki/GPT-2 "GPT-2") | February 2019 | [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") |  1.5[[16]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-15Brelease-19) | 40GB[[17]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-5T8u5-20) (~10 billion tokens)[[18]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-LambdaLabs-21) | 28[[19]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:10-22) |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License")[[20]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Sudbe-23) | Trained on 32 TPUv3 chips for 1 week.[[19]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:10-22)  
[GPT-3](https://en.wikipedia.org/wiki/GPT-3 "GPT-3") | May 2020 | [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") |  175[[21]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Wiggers-24) |  300 billion tokens[[18]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-LambdaLabs-21) | 3640[[22]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:2-25) |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT "ChatGPT") in 2022.[[23]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-chatgpt-blog-26)  
GPT-Neo  |  March 2021 |  [EleutherAI](https://en.wikipedia.org/wiki/EleutherAI "EleutherAI") |  2.7[[24]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-gpt-neo-27) | 825 GiB[[25]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Pile-28) | Unknown  |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License")[[26]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-vb-gpt-neo-29) | The first of [a series of free GPT-3 alternatives](https://en.wikipedia.org/wiki/EleutherAI#GPT_models "EleutherAI") released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.[[26]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-vb-gpt-neo-29)  
[GPT-J](https://en.wikipedia.org/wiki/GPT-J "GPT-J") | June 2021 | [EleutherAI](https://en.wikipedia.org/wiki/EleutherAI "EleutherAI") |  6[[27]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-JxohJ-30) | 825 GiB[[25]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Pile-28) | 200[[28]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:3-31) |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | GPT-3-style language model   
Megatron-Turing NLG  |  October 2021[[29]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-BwnW5-32) |  [Microsoft](https://en.wikipedia.org/wiki/Microsoft "Microsoft") and [Nvidia](https://en.wikipedia.org/wiki/Nvidia "Nvidia") |  530[[30]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-mtnlg-preprint-33) |  338.6 billion tokens[[30]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-mtnlg-preprint-33) | 38000[[31]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:11-34) | Unreleased  | Trained for 3 months on over 2000 A100 GPUs on the NVIDIA [Selene Supercomputer](https://en.wikipedia.org/wiki/Selene_\(supercomputer\) "Selene \(supercomputer\)"), for over 3 million GPU-hours[[31]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:11-34)  
Ernie 3.0 Titan  |  December 2021 |  [Baidu](https://en.wikipedia.org/wiki/Baidu "Baidu") |  260[[32]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-qeOB8-35) | 4TB  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Chinese-language LLM. [Ernie Bot](https://en.wikipedia.org/wiki/Ernie_Bot "Ernie Bot") is based on this model.   
[Claude](https://en.wikipedia.org/wiki/Claude_\(language_model\) "Claude \(language model\)")[[33]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-i8jc4-36) |  December 2021 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") |  52[[34]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-AnthroArch-37) |  400 billion tokens[[34]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-AnthroArch-37) | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Fine-tuned for desirable behavior in conversations.[[35]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-RZqhw-38)  
GLaM (Generalist Language Model) | December 2021 | [Google](https://en.wikipedia.org/wiki/Google "Google") |  1200[[36]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-glam-blog-39) |  1.6 trillion tokens[[36]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-glam-blog-39) | 5600[[36]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-glam-blog-39) |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Sparse [mixture of experts](https://en.wikipedia.org/wiki/Mixture_of_experts "Mixture of experts") model, making it more expensive to train but cheaper to run inference compared to GPT-3.   
Gopher | December 2021 | [DeepMind](https://en.wikipedia.org/wiki/DeepMind "DeepMind") |  280[[37]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-mD5eE-40) |  300 billion tokens[[38]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-hoffman-41) | 5833[[39]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:4-42) |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Later developed into the Chinchilla model.   
[LaMDA](https://en.wikipedia.org/wiki/LaMDA "LaMDA") (Language Models for Dialog Applications) | January 2022 | [Google](https://en.wikipedia.org/wiki/Google "Google") |  137[[40]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-lamda-blog-43) | 1.56T words,[[40]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-lamda-blog-43) 168 billion tokens[[38]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-hoffman-41) | 4110[[41]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-DMs9Z-44) |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Specialized for response generation in conversations.   
GPT-NeoX | February 2022 | [EleutherAI](https://en.wikipedia.org/wiki/EleutherAI "EleutherAI") |  20[[42]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-gpt-neox-20b-45) | 825 GiB[[25]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Pile-28) | 740[[28]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:3-31) |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | based on the Megatron architecture   
[Chinchilla](https://en.wikipedia.org/wiki/Chinchilla_AI "Chinchilla AI") | March 2022 | [DeepMind](https://en.wikipedia.org/wiki/DeepMind "DeepMind") |  70[[43]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-chinchilla-blog-46) |  1.4 trillion tokens[[43]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-chinchilla-blog-46)[[38]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-hoffman-41) | 6805[[39]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:4-42) |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Reduced-parameter model trained on more data. Used in the [Sparrow](https://en.wikipedia.org/wiki/Sparrow_\(bot\) "Sparrow \(bot\)") bot. Often cited for its [neural scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law "Neural scaling law").   
[PaLM](https://en.wikipedia.org/wiki/PaLM "PaLM") (Pathways Language Model) | April 2022 | [Google](https://en.wikipedia.org/wiki/Google "Google") |  540[[44]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-palm-blog-47) |  768 billion tokens[[43]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-chinchilla-blog-46) |  29,250[[39]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:4-42) |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Trained for ~60 days on ~6000 [TPU v4](https://en.wikipedia.org/wiki/Tensor_Processing_Unit "Tensor Processing Unit") chips.[[39]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:4-42)  
OPT (Open Pretrained Transformer) | May 2022 | [Meta](https://en.wikipedia.org/wiki/Meta_Platforms "Meta Platforms") |  175[[45]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-jlof8-48) |  180 billion tokens[[46]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-QjTIc-49) | 310[[28]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:3-31) | Non-commercial research[[d]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-50) | GPT-3 architecture with some adaptations from Megatron. Uniquely, the training logbook written by the team was published.[[47]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-51)  
YaLM 100B  |  June 2022 |  [Yandex](https://en.wikipedia.org/wiki/Yandex "Yandex") |  100[[48]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-yalm-repo-52) | 1.7TB[[48]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-yalm-repo-52) | Unknown  |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | English-Russian model based on Microsoft's Megatron-LM   
Minerva  |  June 2022 |  [Google](https://en.wikipedia.org/wiki/Google "Google") |  540[[49]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-minerva-paper-53) | 38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server[[49]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-minerva-paper-53) | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | For solving "mathematical and scientific questions using step-by-step reasoning".[[50]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-FfCNK-54) Initialized from PaLM models, then finetuned on mathematical and scientific data.   
[BLOOM](https://en.wikipedia.org/wiki/BLOOM_\(language_model\) "BLOOM \(language model\)") |  July 2022 | Large collaboration led by [Hugging Face](https://en.wikipedia.org/wiki/Hugging_Face "Hugging Face") |  175[[51]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-bigger-better-55) |  350 billion tokens (1.6TB)[[52]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-B8wB2-56) | Unknown  | Responsible AI  | Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)   
Galactica | November 2022 | [Meta](https://en.wikipedia.org/wiki/Meta_Platforms "Meta Platforms") | 120 |  106 billion tokens[[53]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-37sY6-57) | Unknown | CC-BY-NC-4.0  | Trained on scientific text and modalities.   
AlexaTM (Teacher Models)  |  November 2022 |  [Amazon](https://en.wikipedia.org/wiki/Amazon_\(company\) "Amazon \(company\)") |  20[[54]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-u5szh-58) |  1.3 trillion[[55]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-HaA7l-59) | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software")[[56]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-rpehM-60) | Bidirectional sequence-to-sequence architecture   
[Llama](https://en.wikipedia.org/wiki/Llama_\(language_model\) "Llama \(language model\)") | February 2023 | [Meta AI](https://en.wikipedia.org/wiki/Meta_AI "Meta AI") |  65[[57]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-llama-blog-61) |  1.4 trillion[[57]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-llama-blog-61) | 6300[[58]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:5-62) | Non-commercial research[[e]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-63) | Corpus has 20 languages. "Overtrained" (compared to [Chinchilla scaling law](https://en.wikipedia.org/wiki/Chinchilla_\(language_model\) "Chinchilla \(language model\)")) for better performance with fewer parameters.[[57]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-llama-blog-61)  
[GPT-4](https://en.wikipedia.org/wiki/GPT-4 "GPT-4") | March 2023 | [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") | Unknown[[f]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-65)  
(According to rumors: 1760)[[60]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-66) | Unknown  | Unknown,  
estimated 230,000  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Available for all ChatGPT users now and used in [several products](https://en.wikipedia.org/wiki/GPT-4#Usage "GPT-4").   
Cerebras-GPT  |  March 2023 |  [Cerebras](https://en.wikipedia.org/wiki/Cerebras "Cerebras") |  13[[61]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-D0k2a-67) |  | 270[[28]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:3-31) |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | Trained with [Chinchilla formula](https://en.wikipedia.org/wiki/Chinchilla_\(language_model\) "Chinchilla \(language model\)").   
Falcon | March 2023 | [Technology Innovation Institute](https://en.wikipedia.org/wiki/Technology_Innovation_Institute "Technology Innovation Institute") |  40[[62]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-falcon-68) | 1 trillion tokens, from RefinedWeb (filtered web text corpus)[[63]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Xb1gq-69) plus some "curated corpora".[[64]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-gzTNw-70) | 2800[[58]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:5-62) |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0")[[65]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Wmlcs-71) |   
BloombergGPT  |  March 2023 |  [Bloomberg L.P.](https://en.wikipedia.org/wiki/Bloomberg_L.P. "Bloomberg L.P.") | 50 | 363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets[[66]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-nGOSu-72) | Unknown  | Unreleased  | Trained on financial data from proprietary sources, for financial tasks   
[PanGu-Œ£](https://en.wikipedia.org/wiki/Huawei_PanGu "Huawei PanGu") |  March 2023 |  [Huawei](https://en.wikipedia.org/wiki/Huawei "Huawei") |  1085 | 329 billion tokens[[67]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-9WSFw-73) | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") |   
OpenAssistant[[68]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-JiOl8-74) |  March 2023 |  [LAION](https://en.wikipedia.org/wiki/LAION "LAION") |  17 | 1.5 trillion tokens  | Unknown  |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | Trained on crowdsourced open data   
Jurassic-2[[69]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-75) |  March 2023 |  [AI21 Labs](https://en.wikipedia.org/wiki/AI21_Labs "AI21 Labs") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Multilingual[[70]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-76)  
[PaLM 2](https://en.wikipedia.org/wiki/PaLM "PaLM") (Pathways Language Model 2) | May 2023 | [Google](https://en.wikipedia.org/wiki/Google "Google") |  340[[71]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-cnbc-20230516-77) |  3.6 trillion tokens[[71]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-cnbc-20230516-77) |  85,000[[58]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:5-62) |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Was used in [Bard chatbot](https://en.wikipedia.org/wiki/Bard_\(chatbot\) "Bard \(chatbot\)").[[72]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-pWyLA-78)  
[Llama 2](https://en.wikipedia.org/wiki/Llama_\(language_model\)#Llama_2 "Llama \(language model\)") | July 2023 | [Meta AI](https://en.wikipedia.org/wiki/Meta_AI "Meta AI") |  70[[73]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-meta-20230719-79) |  2 trillion tokens[[73]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-meta-20230719-79) | 21,000 |  [Llama 2 license](https://en.wikipedia.org/wiki/Llama_\(language_model\)#Licensing "Llama \(language model\)") | 1.7 million A100-hours.[[74]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-80)  
[Claude 2](https://en.wikipedia.org/wiki/Claude_\(language_model\) "Claude \(language model\)") |  July 2023 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") | Unknown  | Unknown  | Unknown |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Used in Claude chatbot.[[75]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-81)  
[Granite 13b](https://en.wikipedia.org/wiki/IBM_Granite "IBM Granite") |  July 2023 |  [IBM](https://en.wikipedia.org/wiki/IBM "IBM") | Unknown  | Unknown  | Unknown |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Used in [IBM Watsonx](https://en.wikipedia.org/wiki/IBM_Watsonx "IBM Watsonx").[[76]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-82)  
[Mistral 7B](https://en.wikipedia.org/wiki/Mistral_7B "Mistral 7B") |  September 2023 |  [Mistral AI](https://en.wikipedia.org/wiki/Mistral_AI "Mistral AI") |  7.3[[77]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-mistral-20230927-83) | Unknown  | Unknown  |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") |   
[Claude 2.1](https://en.wikipedia.org/wiki/Claude_2.1 "Claude 2.1") |  November 2023 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") | Unknown  | Unknown  | Unknown |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.[[78]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-84)  
[Grok 1](https://en.wikipedia.org/wiki/Grok_1 "Grok 1")[[79]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-85) |  November 2023 |  [xAI](https://en.wikipedia.org/wiki/XAI_\(company\) "XAI \(company\)") | 314  | Unknown  | Unknown |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | Used in [Grok](https://en.wikipedia.org/wiki/Grok_\(chatbot\) "Grok \(chatbot\)") chatbot. Grok 1 has a context length of 8,192 tokens and has access to X (Twitter).[[80]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-86)  
[Gemini 1.0](https://en.wikipedia.org/wiki/Gemini_\(language_model\) "Gemini \(language model\)") |  December 2023 |  [Google DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind "Google DeepMind") | Unknown  | Unknown  | Unknown |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Multimodal model, comes in three sizes. Used in [the chatbot of the same name](https://en.wikipedia.org/wiki/Gemini_\(chatbot\) "Gemini \(chatbot\)").[[81]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-87)  
[Mixtral 8x7B](https://en.wikipedia.org/wiki/Mixtral_8x7B "Mixtral 8x7B") |  December 2023 |  [Mistral AI](https://en.wikipedia.org/wiki/Mistral_AI "Mistral AI") | 46.7  | Unknown  | Unknown |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.[[82]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-88) [Mixture of experts](https://en.wikipedia.org/wiki/Mixture_of_experts "Mixture of experts") model, with 12.9 billion parameters activated per token.[[83]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-89)  
[DeepSeek-LLM](https://en.wikipedia.org/wiki/DeepSeek-LLM "DeepSeek-LLM") |  November 29, 2023 |  [DeepSeek](https://en.wikipedia.org/wiki/DeepSeek "DeepSeek") | 67  | 2T tokens[[84]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:1-90): table 2  |  12,000 | DeepSeek License  | Trained on English and Chinese text. 1e24 FLOPs for 67B. 1e23 FLOPs for 7B[[84]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:1-90): figure 5   
[Phi-2](https://en.wikipedia.org/w/index.php?title=Phi_\(LLM\)&action=edit&redlink=1 "Phi \(LLM\) \(page does not exist\)") |  December 2023 |  [Microsoft](https://en.wikipedia.org/wiki/Microsoft "Microsoft") | 2.7  | 1.4T tokens  | 419[[85]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:9-91) |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License") | Trained on real and synthetic "textbook-quality" data, for 14 days on 96 A100 GPUs.[[85]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:9-91)  
[Gemini 1.5](https://en.wikipedia.org/wiki/Gemini_\(language_model\) "Gemini \(language model\)") |  February 2024 |  [Google DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind "Google DeepMind") | Unknown  | Unknown  | Unknown |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Multimodal model, based on a [Mixture-of-Experts](https://en.wikipedia.org/wiki/Mixture_of_experts "Mixture of experts") (MoE) architecture. Context window above 1 million tokens.[[86]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-92)  
[Gemini Ultra](https://en.wikipedia.org/wiki/Gemini_\(language_model\) "Gemini \(language model\)") |  February 2024 |  [Google DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind "Google DeepMind") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") |   
Gemma | February 2024 | [Google DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind "Google DeepMind") | 7 | 6T tokens | Unknown | Gemma Terms of Use[[87]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-gemma-93) |   
[Claude 3](https://en.wikipedia.org/wiki/Claude_\(language_model\) "Claude \(language model\)") |  March 2024 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Includes three models, Haiku, Sonnet, and Opus.[[88]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-94)  
[DBRX](https://en.wikipedia.org/wiki/DBRX "DBRX") |  March 2024 |  [Databricks](https://en.wikipedia.org/wiki/Databricks "Databricks") and [Mosaic ML](https://en.wikipedia.org/wiki/Mosaic_ML "Mosaic ML") |  136 | 12T tokens  | Unknown  | Databricks Open Model License[[89]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-95)[[90]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-96) | Training cost 10 million USD   
Fugaku-LLM  |  May 2024 |  [Fujitsu](https://en.wikipedia.org/wiki/Fujitsu "Fujitsu"), [Tokyo Institute of Technology](https://en.wikipedia.org/wiki/Tokyo_Institute_of_Technology "Tokyo Institute of Technology"), etc.  |  13 | 380B tokens  | Unknown  | Fugaku-LLM Terms of Use[[91]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-97) | The largest model ever trained on CPU-only, on the [Fugaku](https://en.wikipedia.org/wiki/Fugaku_\(supercomputer\) "Fugaku \(supercomputer\)")[[92]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-98)  
Chameleon  |  May 2024 |  [Meta AI](https://en.wikipedia.org/wiki/Meta_AI "Meta AI") |  34[[93]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-99) |  4.4 trillion | Unknown  | Non-commercial research[[94]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-100)  
[Mixtral 8x22B](https://en.wikipedia.org/wiki/Mixtral_8x22B "Mixtral 8x22B") |  April 17, 2024 |  [Mistral AI](https://en.wikipedia.org/wiki/Mistral_AI "Mistral AI") | 141  | Unknown  | Unknown |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") |  [[95]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-101)  
[Phi-3](https://en.wikipedia.org/w/index.php?title=Phi_\(LLM\)&action=edit&redlink=1 "Phi \(LLM\) \(page does not exist\)") |  April 23, 2024 |  [Microsoft](https://en.wikipedia.org/wiki/Microsoft "Microsoft") | 14[[96]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-102) | 4.8T tokens  | Unknown  |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License") | Microsoft markets them as "small language model".[[97]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-103)  
[Granite Code Models](https://en.wikipedia.org/wiki/IBM_Granite "IBM Granite") |  May 2024 |  [IBM](https://en.wikipedia.org/wiki/IBM "IBM") | Unknown  | Unknown  | Unknown |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") |   
[Qwen2](https://en.wikipedia.org/wiki/Qwen2 "Qwen2") |  June 2024 |  [Alibaba Cloud](https://en.wikipedia.org/wiki/Alibaba_Cloud "Alibaba Cloud") | 72[[98]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-104) | 3T tokens  | Unknown  | Qwen License  | Multiple sizes, the smallest being 0.5B.   
[DeepSeek-V2](https://en.wikipedia.org/wiki/DeepSeek-V2 "DeepSeek-V2") |  June 2024 |  [DeepSeek](https://en.wikipedia.org/wiki/DeepSeek "DeepSeek") | 236  | 8.1T tokens  |  28,000 | DeepSeek License  | 1.4M hours on H800.[[99]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-105)  
Nemotron-4  |  June 2024 |  [Nvidia](https://en.wikipedia.org/wiki/Nvidia "Nvidia") |  340 | 9T tokens  |  200,000 | NVIDIA Open Model License[[100]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-106)[[101]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-107) | Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.[[102]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-108)[[103]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-109)  
[Claude 3.5](https://en.wikipedia.org/wiki/Claude_\(language_model\) "Claude \(language model\)") |  June 2024 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Initially, only one model, Sonnet, was released.[[104]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-110) In October 2024, Sonnet 3.5 was upgraded, and Haiku 3.5 became available.[[105]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-111)  
[Llama 3.1](https://en.wikipedia.org/wiki/Llama_3.1 "Llama 3.1") |  July 2024 |  [Meta AI](https://en.wikipedia.org/wiki/Meta_AI "Meta AI") | 405  | 15.6T tokens  |  440,000 |  [Llama 3 license](https://en.wikipedia.org/wiki/Llama_\(language_model\)#Licensing "Llama \(language model\)") | 405B version took 31 million hours on [H100](https://en.wikipedia.org/wiki/Hopper_\(microarchitecture\) "Hopper \(microarchitecture\)")-80GB, at 3.8E25 FLOPs.[[106]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-112)[[107]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-113)  
[Grok-2](https://en.wikipedia.org/wiki/Grok-2 "Grok-2") |  August 14, 2024 |  [xAI](https://en.wikipedia.org/wiki/XAI_\(company\) "XAI \(company\)") | Unknown  | Unknown  | Unknown  | xAI Community License Agreement[[108]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-114)[[109]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-115) | Originally closed-source, then re-released as "Grok 2.5" under a source-available license in August 2025.[[110]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-116)[[111]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-117)  
[OpenAI o1](https://en.wikipedia.org/wiki/OpenAI_o1 "OpenAI o1") |  September 12, 2024 |  [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Reasoning model.[[112]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-118)  
[Mistral Large](https://en.wikipedia.org/wiki/Mistral_Large "Mistral Large") |  November 2024 |  [Mistral AI](https://en.wikipedia.org/wiki/Mistral_AI "Mistral AI") | 123  | Unknown  | Unknown  | Mistral Research License  | Upgraded over time. The latest version is 24.11.[[113]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Mistral_models_overview-119)  
[Pixtral](https://en.wikipedia.org/wiki/Pixtral "Pixtral") |  November 2024 |  [Mistral AI](https://en.wikipedia.org/wiki/Mistral_AI "Mistral AI") | 123  | Unknown  | Unknown  | Mistral Research License  | Multimodal. There is also a 12B version which is under Apache 2 license.[[113]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-Mistral_models_overview-119)  
[Phi-4](https://en.wikipedia.org/w/index.php?title=Phi_\(LLM\)&action=edit&redlink=1 "Phi \(LLM\) \(page does not exist\)") |  December 12, 2024 |  [Microsoft](https://en.wikipedia.org/wiki/Microsoft "Microsoft") | 14[[114]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-120) |  9.8T tokens  | Unknown  |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License") | Microsoft markets them as "small language model".[[115]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-121)  
[DeepSeek-V3](https://en.wikipedia.org/wiki/DeepSeek-V3 "DeepSeek-V3") |  December 2024 |  [DeepSeek](https://en.wikipedia.org/wiki/DeepSeek "DeepSeek") | 671  | 14.8T tokens  |  56,000 |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License") | 2.788M hours on H800 GPUs.[[116]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-122) Originally released under the DeepSeek License, then re-released under the MIT License as "DeepSeek-V3-0324" in March 2025.[[117]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-123)  
Amazon Nova  |  December 2024 |  [Amazon](https://en.wikipedia.org/wiki/Amazon_\(company\) "Amazon \(company\)") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Includes three models, Nova Micro, Nova Lite, and Nova Pro[[118]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-124)  
[DeepSeek-R1](https://en.wikipedia.org/wiki/DeepSeek-R1 "DeepSeek-R1") |  January 2025 |  [DeepSeek](https://en.wikipedia.org/wiki/DeepSeek "DeepSeek") | 671  | Not applicable  | Unknown  |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License") | No pretraining. Reinforcement-learned upon V3-Base.[[119]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-125)[[120]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-126)  
[Qwen2.5](https://en.wikipedia.org/wiki/Qwen2.5 "Qwen2.5") |  January 2025 |  [Alibaba](https://en.wikipedia.org/wiki/Alibaba_Group "Alibaba Group") | 72  | 18T tokens  | Unknown  | Qwen License  | 7 dense models, with parameter count from 0.5B to 72B. They also released 2 MoE variants.[[121]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-127)  
MiniMax-Text-01  |  January 2025 |  [Minimax](https://en.wikipedia.org/wiki/MiniMax_\(company\) "MiniMax \(company\)") | 456  | 4.7T tokens[[122]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:0-128) | Unknown  | Minimax Model license  |  [[123]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-129)[[122]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-:0-128)  
[Gemini 2.0](https://en.wikipedia.org/wiki/Gemini_\(language_model\) "Gemini \(language model\)") |  February 2025 |  [Google DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind "Google DeepMind") | Unknown  | Unknown  | Unknown |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Three models released: Flash, Flash-Lite and Pro[[124]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-130)[[125]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-131)[[126]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-132)  
[Claude 3.7](https://en.wikipedia.org/wiki/Claude_\(language_model\) "Claude \(language model\)") |  February 24, 2025 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | One model, Sonnet 3.7.[[127]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-133)  
[GPT-4.5](https://en.wikipedia.org/wiki/GPT-4.5 "GPT-4.5") |  February 27, 2025 |  [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Largest non-reasoning model.[[128]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-134)  
[Grok 3](https://en.wikipedia.org/wiki/Grok_3 "Grok 3") |  February 2025 |  [xAI](https://en.wikipedia.org/wiki/XAI_\(company\) "XAI \(company\)") | Unknown  | Unknown  | Unknown,  
estimated 5,800,000  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Training cost claimed "10x the compute of previous state-of-the-art models".[[129]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-135)  
[Gemini 2.5](https://en.wikipedia.org/wiki/Gemini_\(language_model\) "Gemini \(language model\)") |  March 25, 2025 |  [Google DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind "Google DeepMind") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Three models released: Flash, Flash-Lite and Pro[[130]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-136)  
[Llama 4](https://en.wikipedia.org/wiki/Llama_4 "Llama 4") |  April 5, 2025 |  [Meta AI](https://en.wikipedia.org/wiki/Meta_AI "Meta AI") |  400 |  40T tokens | Unknown  |  [Llama 4 license](https://en.wikipedia.org/wiki/Llama_\(language_model\)#Licensing "Llama \(language model\)") |  [[131]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-137)[[132]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-138)  
[OpenAI o3](https://en.wikipedia.org/wiki/OpenAI_o3 "OpenAI o3") and o4-mini  |  April 16, 2025 |  [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Reasoning models.[[133]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-139)  
[Qwen3](https://en.wikipedia.org/wiki/Qwen3 "Qwen3") |  April 2025 |  [Alibaba Cloud](https://en.wikipedia.org/wiki/Alibaba_Cloud "Alibaba Cloud") | 235  |  36T tokens | Unknown  |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | Multiple sizes, the smallest being 0.6B.[[134]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-140)  
[Claude 4](https://en.wikipedia.org/wiki/Claude_\(language_model\) "Claude \(language model\)") |  May 22, 2025 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Includes two models, Sonnet and Opus.[[135]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-141)  
[Grok 4](https://en.wikipedia.org/wiki/Grok_4 "Grok 4") |  July 9, 2025 |  [xAI](https://en.wikipedia.org/wiki/XAI_\(company\) "XAI \(company\)") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") |   
GLM-4.5  |  July 29, 2025 |  [Zhipu AI](https://en.wikipedia.org/wiki/Zhipu_AI "Zhipu AI") | 355  | 22T tokens  | Unknown  |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License") | Released in 335B and 106B sizes.[[136]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-142) Corpus size was calculated by combining the 15 trillion tokens and the 7 trillion tokens pre-training mix.[[137]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-143)  
[GPT-OSS](https://en.wikipedia.org/wiki/GPT-OSS "GPT-OSS") |  August 5, 2025 |  [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") | 117  | Unknown  | Unknown  |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | Released in 20B and 120B sizes.[[138]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-144)  
[Claude 4.1](https://en.wikipedia.org/wiki/Claude_\(language_model\) "Claude \(language model\)") |  August 5, 2025 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Includes one model, Opus.[[139]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-145)  
[GPT-5](https://en.wikipedia.org/wiki/GPT-5 "GPT-5") |  August 7, 2025 |  [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Includes three models, GPT-5, GPT-5 mini, and GPT-5 nano. GPT-5 is available in ChatGPT and API. It includes thinking abilities. [[140]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-146)[[141]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-147)  
DeepSeek-V3.1  | August 21, 2025  |  [DeepSeek](https://en.wikipedia.org/wiki/DeepSeek "DeepSeek") | 671  | 15.639T  |  |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License") | Training size: 14.8T tokens, of DeepSeek V3 plus 839B tokens from the extension phases (630B + 209B)[[142]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-148)It is a hybrid model that can switch between thinking and non-thinking modes.[[143]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-149)  
[Apertus](https://en.wikipedia.org/wiki/Apertus_\(LLM\) "Apertus \(LLM\)") |  September 2, 2025 |  [ETH Zurich](https://en.wikipedia.org/wiki/ETH_Zurich "ETH Zurich") and [EPF Lausanne](https://en.wikipedia.org/wiki/%C3%89cole_Polytechnique_F%C3%A9d%C3%A9rale_de_Lausanne "√âcole Polytechnique F√©d√©rale de Lausanne") | 70  |  15 trillion[[144]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-150) | Unknown  |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") | It's said to be the first LLM to be compliant with [EU's](https://en.wikipedia.org/wiki/European_Union "European Union") [Artificial Intelligence Act](https://en.wikipedia.org/wiki/Artificial_Intelligence_Act "Artificial Intelligence Act").[[145]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-151)  
[Claude 4.5](https://en.wikipedia.org/wiki/Claude_\(language_model\) "Claude \(language model\)") |  September 29, 2025 |  [Anthropic](https://en.wikipedia.org/wiki/Anthropic "Anthropic") | Unknown  | Unknown  | Unknown  |  [Proprietary](https://en.wikipedia.org/wiki/Proprietary_software "Proprietary software") | Only one variant is available, Sonnet.[[146]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-152)  
DeepSeek-V3.2-Exp  |  September 29, 2025 |  [DeepSeek](https://en.wikipedia.org/wiki/DeepSeek "DeepSeek") | 685  |  |  |  [MIT](https://en.wikipedia.org/wiki/MIT_License "MIT License") | This experimental model built upon v3.1-Terminus uses a custom efficient mechanism tagged DeepSeek Sparse Attention (DSA).[[147]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-153)[[148]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-154)[[149]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-155)  
GLM-4.6  |  September 30, 2025 |  [Zhipu AI](https://en.wikipedia.org/wiki/Zhipu_AI "Zhipu AI") | 357  |  |  |  [Apache 2.0](https://en.wikipedia.org/wiki/Apache_2.0 "Apache 2.0") |  [[150]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-156)[[151]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-157)[[152]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-158)  
## Timeline
[[edit](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=edit&section=2 "Edit section: Timeline")]
Timeline of major LLM releases (2024‚Äìpresent)   
---  
![](https://upload.wikimedia.org/wikipedia/en/timeline/t7m673fka9gr7p45ohexoe5d9ree4b8.png)  
  

## See also
[[edit](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=edit&section=3 "Edit section: See also")]
  * [List of chatbots](https://en.wikipedia.org/wiki/List_of_chatbots "List of chatbots")
  * [List of language model benchmarks](https://en.wikipedia.org/wiki/List_of_language_model_benchmarks "List of language model benchmarks")


## Notes
[[edit](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=edit&section=4 "Edit section: Notes")]
  1. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-1 "Jump up")** This is the date that documentation describing the model's architecture was first released.
  2. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-2 "Jump up")** In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.
  3. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-3 "Jump up")** This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated. LLMs may be licensed differently from the chatbots that use them; for the licenses of chatbots, see [List of chatbots](https://en.wikipedia.org/wiki/List_of_chatbots "List of chatbots").
  4. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-50 "Jump up")** The smaller models including 66B are publicly available, while the 175B model is available on request.
  5. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-63 "Jump up")** Facebook's license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.
  6. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-65 "Jump up")** As stated in Technical report: "Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ..."[[59]](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_note-GPT4Tech-64)


## References
[[edit](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&action=edit&section=5 "Edit section: References")]
  1. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-oai-unsup_4-0 "Jump up")** ["Improving language understanding with unsupervised learning"](https://openai.com/research/language-unsupervised). _openai.com_. June 11, 2018. [Archived](https://web.archive.org/web/20230318210736/https://openai.com/research/language-unsupervised) from the original on 2023-03-18. Retrieved 2023-03-18.
  2. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-5 "Jump up")** ["finetune-transformer-lm"](https://github.com/openai/finetune-transformer-lm). _GitHub_. [Archived](https://web.archive.org/web/20230519062127/https://github.com/openai/finetune-transformer-lm) from the original on 19 May 2023. Retrieved 2 January 2024.
  3. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-6 "Jump up")** Radford, Alec (11 June 2018). ["Improving language understanding with unsupervised learning"](https://openai.com/index/language-unsupervised/). _[OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI")_. Retrieved 18 November 2025.
  4. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-bert-paper_7-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-bert-paper_7-1) Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[1810.04805v2](https://arxiv.org/abs/1810.04805v2) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  5. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-bHZJ2_8-0 "Jump up")** Prickett, Nicole Hemsoth (2021-08-24). ["Cerebras Shifts Architecture To Meet Massive AI/ML Models"](https://www.nextplatform.com/2021/08/24/cerebras-shifts-architecture-to-meet-massive-ai-ml-models/). _The Next Platform_. [Archived](https://web.archive.org/web/20230620151619/https://www.nextplatform.com/2021/08/24/cerebras-shifts-architecture-to-meet-massive-ai-ml-models/) from the original on 2023-06-20. Retrieved 2023-06-20.
  6. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-bert-web_9-0 "Jump up")** ["BERT"](https://github.com/google-research/bert). March 13, 2023. [Archived](https://web.archive.org/web/20210113211317/https://github.com/google-research/bert) from the original on January 13, 2021. Retrieved March 13, 2023 ‚Äì via GitHub.
  7. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Manning-2022_10-0 "Jump up")** [Manning, Christopher D.](https://en.wikipedia.org/wiki/Christopher_D._Manning "Christopher D. Manning") (2022). ["Human Language Understanding & Reasoning"](https://www.amacad.org/publication/human-language-understanding-reasoning). _Daedalus_. **151** (2): 127‚Äì138. [doi](https://en.wikipedia.org/wiki/Doi_\(identifier\) "Doi \(identifier\)"):[10.1162/daed_a_01905](https://doi.org/10.1162%2Fdaed_a_01905). [S2CID](https://en.wikipedia.org/wiki/S2CID_\(identifier\) "S2CID \(identifier\)") [248377870](https://api.semanticscholar.org/CorpusID:248377870). [Archived](https://web.archive.org/web/20231117205531/https://www.amacad.org/publication/human-language-understanding-reasoning) from the original on 2023-11-17. Retrieved 2023-03-09.
  8. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Ir545_11-0 "Jump up")** Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). "Bidirectional Language Models Are Also Few-shot Learners". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2209.14500](https://arxiv.org/abs/2209.14500) [[cs.LG](https://arxiv.org/archive/cs.LG)].
  9. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:02_12-0 "Jump up")** Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[1810.04805v2](https://arxiv.org/abs/1810.04805v2) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  10. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:6_13-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:6_13-1) Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J. (2020). ["Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"](http://jmlr.org/papers/v21/20-074.html). _Journal of Machine Learning Research_. **21** (140): 1‚Äì67. [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[1910.10683](https://arxiv.org/abs/1910.10683). [ISSN](https://en.wikipedia.org/wiki/ISSN_\(identifier\) "ISSN \(identifier\)") [1533-7928](https://search.worldcat.org/issn/1533-7928).
  11. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-14 "Jump up")** [_google-research/text-to-text-transfer-transformer_](https://github.com/google-research/text-to-text-transfer-transformer), Google Research, 2024-04-02, [archived](https://web.archive.org/web/20240329112957/https://github.com/google-research/text-to-text-transfer-transformer) from the original on 2024-03-29, retrieved 2024-04-04
  12. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-15 "Jump up")** ["Imagen: Text-to-Image Diffusion Models"](https://imagen.research.google/). _imagen.research.google_. [Archived](https://web.archive.org/web/20240327201713/https://imagen.research.google/) from the original on 2024-03-27. Retrieved 2024-04-04.
  13. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-16 "Jump up")** ["Pretrained models ‚Äî transformers 2.0.0 documentation"](https://huggingface.co/transformers/v2.0.0/pretrained_models.html). _huggingface.co_. [Archived](https://web.archive.org/web/20240805032110/https://huggingface.co/transformers/v2.0.0/pretrained_models.html) from the original on 2024-08-05. Retrieved 2024-08-05.
  14. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-xlnet_17-0 "Jump up")** ["xlnet"](https://github.com/zihangdai/xlnet/). _GitHub_. [Archived](https://web.archive.org/web/20240102191842/https://github.com/zihangdai/xlnet/) from the original on 2 January 2024. Retrieved 2 January 2024.
  15. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-LX3rI_18-0 "Jump up")** Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V. (2 January 2020). "XLNet: Generalized Autoregressive Pretraining for Language Understanding". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[1906.08237](https://arxiv.org/abs/1906.08237) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  16. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-15Brelease_19-0 "Jump up")** ["GPT-2: 1.5B Release"](https://openai.com/blog/gpt-2-1-5b-release/). _OpenAI_. 2019-11-05. [Archived](https://web.archive.org/web/20191114074358/https://openai.com/blog/gpt-2-1-5b-release/) from the original on 2019-11-14. Retrieved 2019-11-14.
  17. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-5T8u5_20-0 "Jump up")** ["Better language models and their implications"](https://openai.com/research/better-language-models). _openai.com_. [Archived](https://web.archive.org/web/20230316160730/https://openai.com/research/better-language-models) from the original on 2023-03-16. Retrieved 2023-03-13.
  18. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-LambdaLabs_21-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-LambdaLabs_21-1) ["OpenAI's GPT-3 Language Model: A Technical Overview"](https://lambdalabs.com/blog/demystifying-gpt-3). _lambdalabs.com_. 3 June 2020. [Archived](https://web.archive.org/web/20230327213811/https://lambdalabs.com/blog/demystifying-gpt-3) from the original on 27 March 2023. Retrieved 13 March 2023.
  19. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:10_22-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:10_22-1) ["openai-community/gpt2-xl ¬∑ Hugging Face"](https://huggingface.co/openai-community/gpt2-xl). _huggingface.co_. [Archived](https://web.archive.org/web/20240724041702/https://huggingface.co/openai-community/gpt2-xl) from the original on 2024-07-24. Retrieved 2024-07-24.
  20. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Sudbe_23-0 "Jump up")** ["gpt-2"](https://github.com/openai/gpt-2). _GitHub_. [Archived](https://web.archive.org/web/20230311154936/https://github.com/openai/gpt-2) from the original on 11 March 2023. Retrieved 13 March 2023.
  21. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Wiggers_24-0 "Jump up")** Wiggers, Kyle (28 April 2022). ["The emerging types of language models and why they matter"](https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/). _TechCrunch_. [Archived](https://web.archive.org/web/20230316072443/https://techcrunch.com/2022/04/28/the-emerging-types-of-language-models-and-why-they-matter/) from the original on 16 March 2023. Retrieved 9 March 2023.
  22. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:2_25-0 "Jump up")** Table D.1 in Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (May 28, 2020). "Language Models are Few-Shot Learners". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2005.14165v4](https://arxiv.org/abs/2005.14165v4) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  23. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-chatgpt-blog_26-0 "Jump up")** ["ChatGPT: Optimizing Language Models for Dialogue"](https://openai.com/blog/chatgpt/). _OpenAI_. 2022-11-30. [Archived](https://web.archive.org/web/20221130180912/https://openai.com/blog/chatgpt/) from the original on 2022-11-30. Retrieved 2023-01-13.
  24. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-gpt-neo_27-0 "Jump up")** ["GPT Neo"](https://github.com/EleutherAI/gpt-neo). March 15, 2023. [Archived](https://web.archive.org/web/20230312225202/https://github.com/EleutherAI/gpt-neo) from the original on March 12, 2023. Retrieved March 12, 2023 ‚Äì via GitHub.
  25. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Pile_28-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Pile_28-1) [_**c**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Pile_28-2) Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (31 December 2020). "The Pile: An 800GB Dataset of Diverse Text for Language Modeling". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2101.00027](https://arxiv.org/abs/2101.00027) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  26. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-vb-gpt-neo_29-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-vb-gpt-neo_29-1) Iyer, Abhishek (15 May 2021). ["GPT-3's free alternative GPT-Neo is something to be excited about"](https://venturebeat.com/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/). _VentureBeat_. [Archived](https://web.archive.org/web/20230309012717/https://venturebeat.com/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/) from the original on 9 March 2023. Retrieved 13 March 2023.
  27. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-JxohJ_30-0 "Jump up")** ["GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront"](https://web.archive.org/web/20230309205439/https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model). _www.forefront.ai_. Archived from [the original](https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model) on 2023-03-09. Retrieved 2023-02-28.
  28. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:3_31-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:3_31-1) [_**c**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:3_31-2) [_**d**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:3_31-3) Dey, Nolan; Gosal, Gurpreet; Zhiming; Chen; Khachane, Hemant; Marshall, William; Pathria, Ribhu; Tom, Marvin; Hestness, Joel (2023-04-01). "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2304.03208](https://arxiv.org/abs/2304.03208) [[cs.LG](https://arxiv.org/archive/cs.LG)].
  29. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-BwnW5_32-0 "Jump up")** Alvi, Ali; Kharya, Paresh (11 October 2021). ["Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model"](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/). _Microsoft Research_. [Archived](https://web.archive.org/web/20230313180531/https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) from the original on 13 March 2023. Retrieved 13 March 2023.
  30. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-mtnlg-preprint_33-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-mtnlg-preprint_33-1) Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022-02-04). "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2201.11990](https://arxiv.org/abs/2201.11990) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  31. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:11_34-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:11_34-1) Rajbhandari, Samyam; Li, Conglong; Yao, Zhewei; Zhang, Minjia; Aminabadi, Reza Yazdani; Awan, Ammar Ahmad; Rasley, Jeff; He, Yuxiong (2022-07-21), _DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale_ , [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2201.05596](https://arxiv.org/abs/2201.05596)
  32. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-qeOB8_35-0 "Jump up")** Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (December 23, 2021). "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2112.12731](https://arxiv.org/abs/2112.12731) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  33. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-i8jc4_36-0 "Jump up")** ["Product"](https://www.anthropic.com/product). _Anthropic_. [Archived](https://web.archive.org/web/20230316145444/https://www.anthropic.com/product) from the original on 16 March 2023. Retrieved 14 March 2023.
  34. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-AnthroArch_37-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-AnthroArch_37-1) Askell, Amanda; Bai, Yuntao; Chen, Anna; et al. (9 December 2021). "A General Language Assistant as a Laboratory for Alignment". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2112.00861](https://arxiv.org/abs/2112.00861) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  35. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-RZqhw_38-0 "Jump up")** Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et al. (15 December 2022). "Constitutional AI: Harmlessness from AI Feedback". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2212.08073](https://arxiv.org/abs/2212.08073) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  36. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-glam-blog_39-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-glam-blog_39-1) [_**c**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-glam-blog_39-2) Dai, Andrew M; Du, Nan (December 9, 2021). ["More Efficient In-Context Learning with GLaM"](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html). _ai.googleblog.com_. [Archived](https://web.archive.org/web/20230312072042/https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html) from the original on 2023-03-12. Retrieved 2023-03-09.
  37. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-mD5eE_40-0 "Jump up")** ["Language modelling at scale: Gopher, ethical considerations, and retrieval"](https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval). _www.deepmind.com_. 8 December 2021. [Archived](https://web.archive.org/web/20230320082323/https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval) from the original on 20 March 2023. Retrieved 20 March 2023.
  38. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-hoffman_41-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-hoffman_41-1) [_**c**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-hoffman_41-2) Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et al. (29 March 2022). "Training Compute-Optimal Large Language Models". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2203.15556](https://arxiv.org/abs/2203.15556) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  39. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:4_42-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:4_42-1) [_**c**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:4_42-2) [_**d**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:4_42-3) Table 20 and page 66 of _[PaLM: Scaling Language Modeling with Pathways](https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf) [Archived](https://web.archive.org/web/20230610040050/https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf) 2023-06-10 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine "Wayback Machine")_
  40. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-lamda-blog_43-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-lamda-blog_43-1) Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). ["LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything"](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html). _ai.googleblog.com_. [Archived](https://web.archive.org/web/20220325014118/https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html) from the original on 2022-03-25. Retrieved 2023-03-09.
  41. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-DMs9Z_44-0 "Jump up")** Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer, Noam; Kulshreshtha, Apoorv; Cheng, Heng-Tze; Jin, Alicia; Bos, Taylor; Baker, Leslie; Du, Yu; Li, YaGuang; Lee, Hongrae; Zheng, Huaixiu Steven; Ghafouri, Amin; Menegali, Marcelo (2022-01-01). "LaMDA: Language Models for Dialog Applications". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2201.08239](https://arxiv.org/abs/2201.08239) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  42. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-gpt-neox-20b_45-0 "Jump up")** Black, Sidney; Biderman, Stella; Hallahan, Eric; et al. (2022-05-01). [_GPT-NeoX-20B: An Open-Source Autoregressive Language Model_](https://aclanthology.org/2022.bigscience-1.9/). Proceedings of BigScience Episode #5 ‚Äì Workshop on Challenges & Perspectives in Creating Large Language Models. Vol. Proceedings of BigScience Episode #5 ‚Äì Workshop on Challenges & Perspectives in Creating Large Language Models. pp. 95‚Äì136. [Archived](https://web.archive.org/web/20221210082456/https://aclanthology.org/2022.bigscience-1.9/) from the original on 2022-12-10. Retrieved 2022-12-19.
  43. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-chinchilla-blog_46-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-chinchilla-blog_46-1) [_**c**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-chinchilla-blog_46-2) Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). ["An empirical analysis of compute-optimal large language model training"](https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training). _Deepmind Blog_. [Archived](https://web.archive.org/web/20220413014510/https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training) from the original on 13 April 2022. Retrieved 9 March 2023.
  44. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-palm-blog_47-0 "Jump up")** Narang, Sharan; Chowdhery, Aakanksha (April 4, 2022). ["Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance"](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html). _ai.googleblog.com_. [Archived](https://web.archive.org/web/20220404161447/https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) from the original on 2022-04-04. Retrieved 2023-03-09.
  45. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-jlof8_48-0 "Jump up")** Susan Zhang; Mona Diab; Luke Zettlemoyer. ["Democratizing access to large-scale language models with OPT-175B"](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/). _ai.facebook.com_. [Archived](https://web.archive.org/web/20230312231820/https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) from the original on 2023-03-12. Retrieved 2023-03-12.
  46. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-QjTIc_49-0 "Jump up")** Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (21 June 2022). "OPT: Open Pre-trained Transformer Language Models". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2205.01068](https://arxiv.org/abs/2205.01068) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  47. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-51 "Jump up")** ["metaseq/projects/OPT/chronicles at main ¬∑ facebookresearch/metaseq"](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles). _GitHub_. Retrieved 2024-10-18.
  48. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-yalm-repo_52-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-yalm-repo_52-1) Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022-06-22), [_YaLM 100B_](https://github.com/yandex/YaLM-100B), [archived](https://web.archive.org/web/20230616050056/https://github.com/yandex/YaLM-100B) from the original on 2023-06-16, retrieved 2023-03-18
  49. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-minerva-paper_53-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-minerva-paper_53-1) Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (30 June 2022). "Solving Quantitative Reasoning Problems with Language Models". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2206.14858](https://arxiv.org/abs/2206.14858) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  50. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-FfCNK_54-0 "Jump up")** ["Minerva: Solving Quantitative Reasoning Problems with Language Models"](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html). _ai.googleblog.com_. 30 June 2022. Retrieved 20 March 2023.
  51. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-bigger-better_55-0 "Jump up")** Ananthaswamy, Anil (8 March 2023). ["In AI, is bigger always better?"](https://www.nature.com/articles/d41586-023-00641-w). _Nature_. **615** (7951): 202‚Äì205. [Bibcode](https://en.wikipedia.org/wiki/Bibcode_\(identifier\) "Bibcode \(identifier\)"):[2023Natur.615..202A](https://ui.adsabs.harvard.edu/abs/2023Natur.615..202A). [doi](https://en.wikipedia.org/wiki/Doi_\(identifier\) "Doi \(identifier\)"):[10.1038/d41586-023-00641-w](https://doi.org/10.1038%2Fd41586-023-00641-w). [PMID](https://en.wikipedia.org/wiki/PMID_\(identifier\) "PMID \(identifier\)") [36890378](https://pubmed.ncbi.nlm.nih.gov/36890378). [S2CID](https://en.wikipedia.org/wiki/S2CID_\(identifier\) "S2CID \(identifier\)") [257380916](https://api.semanticscholar.org/CorpusID:257380916). [Archived](https://web.archive.org/web/20230316181013/https://www.nature.com/articles/d41586-023-00641-w) from the original on 16 March 2023. Retrieved 9 March 2023.
  52. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-B8wB2_56-0 "Jump up")** ["bigscience/bloom ¬∑ Hugging Face"](https://huggingface.co/bigscience/bloom). _huggingface.co_. [Archived](https://web.archive.org/web/20230412002547/https://huggingface.co/bigscience/bloom) from the original on 2023-04-12. Retrieved 2023-03-13.
  53. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-37sY6_57-0 "Jump up")** Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). "Galactica: A Large Language Model for Science". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2211.09085](https://arxiv.org/abs/2211.09085) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  54. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-u5szh_58-0 "Jump up")** ["20B-parameter Alexa model sets new marks in few-shot learning"](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning). _Amazon Science_. 2 August 2022. [Archived](https://web.archive.org/web/20230315190223/https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning) from the original on 15 March 2023. Retrieved 12 March 2023.
  55. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-HaA7l_59-0 "Jump up")** Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et al. (3 August 2022). "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2208.01448](https://arxiv.org/abs/2208.01448) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  56. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-rpehM_60-0 "Jump up")** ["AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog"](https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/). _aws.amazon.com_. 17 November 2022. [Archived](https://web.archive.org/web/20230313163933/https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/) from the original on 13 March 2023. Retrieved 13 March 2023.
  57. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-llama-blog_61-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-llama-blog_61-1) [_**c**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-llama-blog_61-2) ["Introducing LLaMA: A foundational, 65-billion-parameter large language model"](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/). _Meta AI_. 24 February 2023. [Archived](https://web.archive.org/web/20230303112302/https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) from the original on 3 March 2023. Retrieved 9 March 2023.
  58. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:5_62-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:5_62-1) [_**c**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:5_62-2) ["The Falcon has landed in the Hugging Face ecosystem"](https://huggingface.co/blog/falcon). _huggingface.co_. [Archived](https://web.archive.org/web/20230620002832/https://huggingface.co/blog/falcon) from the original on 2023-06-20. Retrieved 2023-06-20.
  59. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-GPT4Tech_64-0 "Jump up")** ["GPT-4 Technical Report"](https://cdn.openai.com/papers/gpt-4.pdf) (PDF). _[OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI")_. 2023. [Archived](https://web.archive.org/web/20230314190904/https://cdn.openai.com/papers/gpt-4.pdf) (PDF) from the original on March 14, 2023. Retrieved March 14, 2023.
  60. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-66 "Jump up")** Schreiner, Maximilian (2023-07-11). ["GPT-4 architecture, datasets, costs and more leaked"](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/). _THE DECODER_. [Archived](https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/) from the original on 2023-07-12. Retrieved 2024-07-26.
  61. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-D0k2a_67-0 "Jump up")** Dey, Nolan (March 28, 2023). ["Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models"](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/). _Cerebras_. [Archived](https://web.archive.org/web/20230328213339/https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) from the original on March 28, 2023. Retrieved March 28, 2023.
  62. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-falcon_68-0 "Jump up")** ["Abu Dhabi-based TII launches its own version of ChatGPT"](https://fastcompanyme.com/news/abu-dhabi-based-tii-launches-its-own-version-of-chatgpt/). _tii.ae_. [Archived](https://web.archive.org/web/20230403021729/https://fastcompanyme.com/news/abu-dhabi-based-tii-launches-its-own-version-of-chatgpt/) from the original on 2023-04-03. Retrieved 2023-04-03.
  63. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Xb1gq_69-0 "Jump up")** Penedo, Guilherme; Malartic, Quentin; Hesslow, Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza; Pannier, Baptiste; Almazrouei, Ebtesam; Launay, Julien (2023-06-01). "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2306.01116](https://arxiv.org/abs/2306.01116) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  64. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-gzTNw_70-0 "Jump up")** ["tiiuae/falcon-40b ¬∑ Hugging Face"](https://huggingface.co/tiiuae/falcon-40b). _huggingface.co_. 2023-06-09. Retrieved 2023-06-20.
  65. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Wmlcs_71-0 "Jump up")** [UAE's Falcon 40B, World's Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free](https://www.businesswire.com/news/home/20230531005608/en/UAE's-Falcon-40B-World's-Top-Ranked-AI-Model-from-Technology-Innovation-Institute-is-Now-Royalty-Free) [Archived](https://web.archive.org/web/20240208133040/https://www.businesswire.com/news/home/20230531005608/en/UAE%27s-Falcon-40B-World%27s-Top-Ranked-AI-Model-from-Technology-Innovation-Institute-is-Now-Royalty-Free) 2024-02-08 at the [Wayback Machine](https://en.wikipedia.org/wiki/Wayback_Machine "Wayback Machine"), 31 May 2023
  66. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-nGOSu_72-0 "Jump up")** Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). "BloombergGPT: A Large Language Model for Finance". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2303.17564](https://arxiv.org/abs/2303.17564) [[cs.LG](https://arxiv.org/archive/cs.LG)].
  67. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-9WSFw_73-0 "Jump up")** Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). "PanGu-Œ£: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2303.10845](https://arxiv.org/abs/2303.10845) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  68. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-JiOl8_74-0 "Jump up")** K√∂pf, Andreas; Kilcher, Yannic; von R√ºtte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Rich√°rd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). "OpenAssistant Conversations ‚Äì Democratizing Large Language Model Alignment". [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2304.07327](https://arxiv.org/abs/2304.07327) [[cs.CL](https://arxiv.org/archive/cs.CL)].
  69. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-75 "Jump up")** Wrobel, Sharon. ["Tel Aviv startup rolls out new advanced AI language model to rival OpenAI"](https://www.timesofisrael.com/ai21-labs-rolls-out-new-advanced-ai-language-model-to-rival-openai/). _[The Times of Israel](https://en.wikipedia.org/wiki/The_Times_of_Israel "The Times of Israel")_. [ISSN](https://en.wikipedia.org/wiki/ISSN_\(identifier\) "ISSN \(identifier\)") [0040-7909](https://search.worldcat.org/issn/0040-7909). [Archived](https://web.archive.org/web/20230724191823/https://www.timesofisrael.com/ai21-labs-rolls-out-new-advanced-ai-language-model-to-rival-openai/) from the original on 2023-07-24. Retrieved 2023-07-24.
  70. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-76 "Jump up")** Wiggers, Kyle (2023-04-13). ["With Bedrock, Amazon enters the generative AI race"](https://techcrunch.com/2023/04/13/with-bedrock-amazon-enters-the-generative-ai-race/). _TechCrunch_. [Archived](https://web.archive.org/web/20230724102458/https://techcrunch.com/2023/04/13/with-bedrock-amazon-enters-the-generative-ai-race/) from the original on 2023-07-24. Retrieved 2023-07-24.
  71. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-cnbc-20230516_77-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-cnbc-20230516_77-1) Elias, Jennifer (16 May 2023). ["Google's newest A.I. model uses nearly five times more text data for training than its predecessor"](https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html). _[CNBC](https://en.wikipedia.org/wiki/CNBC "CNBC")_. [Archived](https://web.archive.org/web/20230516225326/https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html) from the original on 16 May 2023. Retrieved 18 May 2023.
  72. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-pWyLA_78-0 "Jump up")** ["Introducing PaLM 2"](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/). _Google_. May 10, 2023. [Archived](https://web.archive.org/web/20230518213209/https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) from the original on May 18, 2023. Retrieved May 18, 2023.
  73. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-meta-20230719_79-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-meta-20230719_79-1) ["Introducing Llama 2: The Next Generation of Our Open Source Large Language Model"](https://ai.meta.com/llama/). _Meta AI_. 2023. [Archived](https://web.archive.org/web/20240105234629/https://ai.meta.com/llama/) from the original on 2024-01-05. Retrieved 2023-07-19.
  74. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-80 "Jump up")** ["llama/MODEL_CARD.md at main ¬∑ meta-llama/llama"](https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md). _GitHub_. [Archived](https://web.archive.org/web/20240528090541/https://github.com/meta-llama/llama/blob/main/MODEL_CARD.md) from the original on 2024-05-28. Retrieved 2024-05-28.
  75. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-81 "Jump up")** ["Claude 2"](https://www.anthropic.com/index/claude-2). _anthropic.com_. [Archived](https://web.archive.org/web/20231215212208/https://www.anthropic.com/index/claude-2) from the original on 15 December 2023. Retrieved 12 December 2023.
  76. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-82 "Jump up")** Nirmal, Dinesh (2023-09-07). ["Building AI for business: IBM's Granite foundation models"](https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models). _IBM Blog_. [Archived](https://web.archive.org/web/20240722083855/https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models/) from the original on 2024-07-22. Retrieved 2024-08-11.
  77. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-mistral-20230927_83-0 "Jump up")** ["Announcing Mistral 7B"](https://mistral.ai/news/announcing-mistral-7b/). _Mistral_. 2023. [Archived](https://web.archive.org/web/20240106051047/https://mistral.ai/news/announcing-mistral-7b/) from the original on 2024-01-06. Retrieved 2023-10-06.
  78. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-84 "Jump up")** ["Introducing Claude 2.1"](https://www.anthropic.com/index/claude-2-1). _anthropic.com_. [Archived](https://web.archive.org/web/20231215201726/https://www.anthropic.com/index/claude-2-1) from the original on 15 December 2023. Retrieved 12 December 2023.
  79. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-85 "Jump up")** [_xai-org/grok-1_](https://github.com/xai-org/grok-1), xai-org, 2024-03-19, [archived](https://web.archive.org/web/20240528170731/https://github.com/xai-org/grok-1) from the original on 2024-05-28, retrieved 2024-03-19
  80. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-86 "Jump up")** ["Grok-1 model card"](https://x.ai/model-card/). _x.ai_. Retrieved 12 December 2023.
  81. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-87 "Jump up")** ["Gemini ‚Äì Google DeepMind"](https://deepmind.google/technologies/gemini/#capabilities). _deepmind.google_. [Archived](https://web.archive.org/web/20231208015607/https://deepmind.google/technologies/gemini/#capabilities) from the original on 8 December 2023. Retrieved 12 December 2023.
  82. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-88 "Jump up")** Franzen, Carl (11 December 2023). ["Mistral shocks AI community as latest open source model eclipses GPT-3.5 performance"](https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/). _VentureBeat_. [Archived](https://web.archive.org/web/20231211213640/https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/) from the original on 11 December 2023. Retrieved 12 December 2023.
  83. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-89 "Jump up")** ["Mixtral of experts"](https://mistral.ai/news/mixtral-of-experts/). _mistral.ai_. 11 December 2023. [Archived](https://web.archive.org/web/20240213104049/https://mistral.ai/news/mixtral-of-experts/) from the original on 13 February 2024. Retrieved 12 December 2023.
  84. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:1_90-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:1_90-1) DeepSeek-AI; Bi, Xiao; Chen, Deli; Chen, Guanting; Chen, Shanhuang; Dai, Damai; Deng, Chengqi; Ding, Honghui; Dong, Kai (2024-01-05), _DeepSeek LLM: Scaling Open-Source Language Models with Longtermism_ , [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2401.02954](https://arxiv.org/abs/2401.02954)
  85. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:9_91-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:9_91-1) Hughes, Alyssa (12 December 2023). ["Phi-2: The surprising power of small language models"](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/). _Microsoft Research_. [Archived](https://web.archive.org/web/20231212232647/https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) from the original on 12 December 2023. Retrieved 13 December 2023.
  86. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-92 "Jump up")** ["Our next-generation model: Gemini 1.5"](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window). _Google_. 15 February 2024. [Archived](https://web.archive.org/web/20240216003052/https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#context-window) from the original on 16 February 2024. Retrieved 16 February 2024. "This means 1.5 Pro can process vast amounts of information in one go ‚Äî including 1 hour of video, 11 hours of audio, codebases with over 30,000 lines of code or over 700,000 words. In our research, we've also successfully tested up to 10 million tokens."
  87. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-gemma_93-0 "Jump up")** ["Gemma"](https://ai.google.dev/gemma/terms) ‚Äì via GitHub.
  88. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-94 "Jump up")** ["Introducing the next generation of Claude"](https://www.anthropic.com/news/claude-3-family). _www.anthropic.com_. [Archived](https://web.archive.org/web/20240304143650/https://www.anthropic.com/news/claude-3-family) from the original on 2024-03-04. Retrieved 2024-03-04.
  89. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-95 "Jump up")** ["Databricks Open Model License"](https://www.databricks.com/legal/open-model-license). _[Databricks](https://en.wikipedia.org/wiki/Databricks "Databricks")_. 27 March 2024. Retrieved 6 August 2025.
  90. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-96 "Jump up")** ["Databricks Open Model Acceptable Use Policy"](https://www.databricks.com/legal/acceptable-use-policy-open-model). _[Databricks](https://en.wikipedia.org/wiki/Databricks "Databricks")_. 27 March 2024. Retrieved 6 August 2025.
  91. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-97 "Jump up")** ["Fugaku-LLM Terms of Use"](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B/blob/main/LICENSE). 23 April 2024. Retrieved 6 August 2025 ‚Äì via [Hugging Face](https://en.wikipedia.org/wiki/Hugging_Face "Hugging Face").
  92. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-98 "Jump up")** ["Fugaku-LLM/Fugaku-LLM-13B ¬∑ Hugging Face"](https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B). _huggingface.co_. [Archived](https://web.archive.org/web/20240517135225/https://huggingface.co/Fugaku-LLM/Fugaku-LLM-13B) from the original on 2024-05-17. Retrieved 2024-05-17.
  93. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-99 "Jump up")** Dickson, Ben (22 May 2024). ["Meta introduces Chameleon, a state-of-the-art multimodal model"](https://venturebeat.com/ai/meta-introduces-chameleon-a-state-of-the-art-multimodal-model/). _VentureBeat_.
  94. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-100 "Jump up")** ["chameleon/LICENSE at e3b711ef63b0bb3a129cf0cf0918e36a32f26e2c ¬∑ facebookresearch/chameleon"](https://github.com/facebookresearch/chameleon/blob/e3b711ef63b0bb3a129cf0cf0918e36a32f26e2c/LICENSE). Meta Research. Retrieved 6 August 2025 ‚Äì via [GitHub](https://en.wikipedia.org/wiki/GitHub "GitHub").
  95. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-101 "Jump up")** AI, Mistral (2024-04-17). ["Cheaper, Better, Faster, Stronger"](https://mistral.ai/news/mixtral-8x22b/). _mistral.ai_. [Archived](https://web.archive.org/web/20240505023828/https://mistral.ai/news/mixtral-8x22b/) from the original on 2024-05-05. Retrieved 2024-05-05.
  96. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-102 "Jump up")** ["Phi-3"](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms). _azure.microsoft.com_. 23 April 2024. [Archived](https://web.archive.org/web/20240427043835/https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/) from the original on 2024-04-27. Retrieved 2024-04-28.
  97. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-103 "Jump up")** ["Phi-3 Model Documentation"](https://huggingface.co/docs/transformers/main/en/model_doc/phi3). _huggingface.co_. [Archived](https://web.archive.org/web/20240513141513/https://huggingface.co/docs/transformers/main/en/model_doc/phi3) from the original on 2024-05-13. Retrieved 2024-04-28.
  98. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-104 "Jump up")** ["Qwen2"](https://github.com/QwenLM/Qwen2?spm=a3c0i.28768018.7084722650.1.5cd35c10NEqBXm&file=Qwen1.5). _[GitHub](https://en.wikipedia.org/wiki/GitHub "GitHub")_. [Archived](https://web.archive.org/web/20240617072401/https://github.com/QwenLM/Qwen2?spm=a3c0i.28768018.7084722650.1.5cd35c10NEqBXm&file=Qwen1.5) from the original on 2024-06-17. Retrieved 2024-06-17.
  99. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-105 "Jump up")** DeepSeek-AI; Liu, Aixin; Feng, Bei; Wang, Bin; Wang, Bingxuan; Liu, Bo; Zhao, Chenggang; Dengr, Chengqi; Ruan, Chong (2024-06-19), _DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model_ , [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2405.04434](https://arxiv.org/abs/2405.04434)
  100. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-106 "Jump up")** ["NVIDIA Open Models License"](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/). _[Nvidia](https://en.wikipedia.org/wiki/Nvidia "Nvidia")_. 16 June 2025. Retrieved 6 August 2025.
  101. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-107 "Jump up")** ["Trustworthy AI"](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/). _[Nvidia](https://en.wikipedia.org/wiki/Nvidia "Nvidia")_. 27 June 2024. Retrieved 6 August 2025.
  102. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-108 "Jump up")** ["nvidia/Nemotron-4-340B-Base ¬∑ Hugging Face"](https://huggingface.co/nvidia/Nemotron-4-340B-Base). _huggingface.co_. 2024-06-14. [Archived](https://web.archive.org/web/20240615010323/https://huggingface.co/nvidia/Nemotron-4-340B-Base) from the original on 2024-06-15. Retrieved 2024-06-15.
  103. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-109 "Jump up")** ["Nemotron-4 340B | Research"](https://research.nvidia.com/publication/2024-06_nemotron-4-340b). _research.nvidia.com_. [Archived](https://web.archive.org/web/20240615010323/https://research.nvidia.com/publication/2024-06_nemotron-4-340b) from the original on 2024-06-15. Retrieved 2024-06-15.
  104. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-110 "Jump up")** ["Introducing Claude 3.5 Sonnet"](https://www.anthropic.com/news/claude-3-5-sonnet). _www.anthropic.com_. Retrieved 8 August 2025.
  105. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-111 "Jump up")** ["Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku"](https://www.anthropic.com/news/3-5-models-and-computer-use). _www.anthropic.com_. Retrieved 8 August 2025.
  106. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-112 "Jump up")** ["The Llama 3 Herd of Models" (July 23, 2024) Llama Team, AI @ Meta](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
  107. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-113 "Jump up")** ["llama-models/models/llama3_1/MODEL_CARD.md at main ¬∑ meta-llama/llama-models"](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md). _GitHub_. [Archived](https://web.archive.org/web/20240723151851/https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) from the original on 2024-07-23. Retrieved 2024-07-23.
  108. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-114 "Jump up")** ["LICENSE ¬∑ xai-org/grok-2 at main"](https://huggingface.co/xai-org/grok-2/blob/main/LICENSE). 5 November 2025. Retrieved 18 November 2025 ‚Äì via [Hugging Face](https://en.wikipedia.org/wiki/Hugging_Face "Hugging Face").
  109. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-115 "Jump up")** ["xAI Acceptable Use Policy"](https://x.ai/legal/acceptable-use-policy). _[xAI](https://en.wikipedia.org/wiki/XAI_\(company\) "XAI \(company\)")_. 2 January 2025. Retrieved 18 November 2025.
  110. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-116 "Jump up")** Weatherbed, Jess (14 August 2024). ["xAI's new Grok-2 chatbots bring AI image generation to X"](https://www.theverge.com/2024/8/14/24220127/grok-ai-chatbot-beta-image-generation-x-xai-update). _[The Verge](https://en.wikipedia.org/wiki/The_Verge "The Verge")_. Retrieved 18 November 2025.
  111. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-117 "Jump up")** Ha, Anthony (24 August 2025). ["Elon Musk says xAI has open sourced Grok 2.5"](https://techcrunch.com/2025/08/24/elon-musk-says-xai-has-open-sourced-grok-2-5/). _[TechCrunch](https://en.wikipedia.org/wiki/TechCrunch "TechCrunch")_. Retrieved 18 November 2025.
  112. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-118 "Jump up")** ["Introducing OpenAI o1"](https://openai.com/o1/). _openai.com_. Retrieved 8 August 2025.
  113. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Mistral_models_overview_119-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-Mistral_models_overview_119-1) ["Models Overview"](https://docs.mistral.ai/getting-started/models/models_overview/). _mistral.ai_. Retrieved 2025-03-03.
  114. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-120 "Jump up")** ["Phi-4 Model Card"](https://huggingface.co/microsoft/phi-4). _huggingface.co_. Retrieved 2025-11-11.`{{cite web[](https://en.wikipedia.org/wiki/Template:Cite_web "Template:Cite web")}}`: CS1 maint: url-status ([link](https://en.wikipedia.org/wiki/Category:CS1_maint:_url-status "Category:CS1 maint: url-status"))
  115. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-121 "Jump up")** ["Introducing Phi-4: Microsoft's Newest Small Language Model Specializing in Complex Reasoning"](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090). _techcommunity.microsoft.com_. Retrieved 2025-11-11.`{{cite web[](https://en.wikipedia.org/wiki/Template:Cite_web "Template:Cite web")}}`: CS1 maint: url-status ([link](https://en.wikipedia.org/wiki/Category:CS1_maint:_url-status "Category:CS1 maint: url-status"))
  116. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-122 "Jump up")** [_deepseek-ai/DeepSeek-V3_](https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file), DeepSeek, 2024-12-26, retrieved 2024-12-26
  117. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-123 "Jump up")** Feng, Coco (25 March 2025). ["DeepSeek wows coders with more powerful open-source V3 model"](https://www.scmp.com/tech/big-tech/article/3303798/deepseeks-upgraded-foundational-model-excels-coding-and-maths). _[South China Morning Post](https://en.wikipedia.org/wiki/South_China_Morning_Post "South China Morning Post")_. Retrieved 6 April 2025.
  118. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-124 "Jump up")** [_Amazon Nova Micro, Lite, and Pro - AWS AI Service Cards3_](https://docs.aws.amazon.com/ai/responsible-ai/nova-micro-lite-pro/overview.html), Amazon, 2024-12-27, retrieved 2024-12-27
  119. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-125 "Jump up")** [_deepseek-ai/DeepSeek-R1_](https://github.com/deepseek-ai/DeepSeek-R1), DeepSeek, 2025-01-21, retrieved 2025-01-21
  120. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-126 "Jump up")** DeepSeek-AI; Guo, Daya; Yang, Dejian; Zhang, Haowei; Song, Junxiao; Zhang, Ruoyu; Xu, Runxin; Zhu, Qihao; Ma, Shirong (2025-01-22), _DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning_ , [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2501.12948](https://arxiv.org/abs/2501.12948)
  121. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-127 "Jump up")** Qwen; Yang, An; Yang, Baosong; Zhang, Beichen; Hui, Binyuan; Zheng, Bo; Yu, Bowen; Li, Chengyuan; Liu, Dayiheng (2025-01-03), _Qwen2.5 Technical Report_ , [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2412.15115](https://arxiv.org/abs/2412.15115)
  122. ^ [Jump up to: _**a**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:0_128-0) [_**b**_](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-:0_128-1) MiniMax; Li, Aonian; Gong, Bangwei; Yang, Bo; Shan, Boji; Liu, Chang; Zhu, Cheng; Zhang, Chunhao; Guo, Congchao (2025-01-14), _MiniMax-01: Scaling Foundation Models with Lightning Attention_ , [arXiv](https://en.wikipedia.org/wiki/ArXiv_\(identifier\) "ArXiv \(identifier\)"):[2501.08313](https://arxiv.org/abs/2501.08313)
  123. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-129 "Jump up")** [_MiniMax-AI/MiniMax-01_](https://github.com/MiniMax-AI/MiniMax-01?tab=readme-ov-file), MiniMax, 2025-01-26, retrieved 2025-01-26
  124. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-130 "Jump up")** Kavukcuoglu, Koray (5 February 2025). ["Gemini 2.0 is now available to everyone"](https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/). _Google_. Retrieved 6 February 2025.
  125. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-131 "Jump up")** ["Gemini 2.0: Flash, Flash-Lite and Pro"](https://developers.googleblog.com/en/gemini-2-family-expands/). _Google for Developers_. Retrieved 6 February 2025.
  126. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-132 "Jump up")** Franzen, Carl (5 February 2025). ["Google launches Gemini 2.0 Pro, Flash-Lite and connects reasoning model Flash Thinking to YouTube, Maps and Search"](https://venturebeat.com/ai/google-launches-gemini-2-0-pro-flash-lite-and-connects-reasoning-model-flash-thinking-to-youtube-maps-and-search/). _VentureBeat_. Retrieved 6 February 2025.
  127. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-133 "Jump up")** ["Claude 3.7 Sonnet and Claude Code"](https://www.anthropic.com/news/claude-3-7-sonnet). _www.anthropic.com_. Retrieved 8 August 2025.
  128. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-134 "Jump up")** ["Introducing GPT-4.5"](https://openai.com/index/introducing-gpt-4-5/). _openai.com_. Retrieved 8 August 2025.
  129. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-135 "Jump up")** ["Grok 3 Beta ‚Äî The Age of Reasoning Agents"](https://x.ai/blog/grok-3). _x.ai_. Retrieved 2025-02-22.
  130. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-136 "Jump up")** Kavukcuoglu, Koray (25 March 2025). ["Gemini 2.5: Our most intelligent AI model"](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/). _Google_. Retrieved 23 September 2025.
  131. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-137 "Jump up")** ["meta-llama/Llama-4-Maverick-17B-128E ¬∑ Hugging Face"](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E). _huggingface.co_. 2025-04-05. Retrieved 2025-04-06.
  132. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-138 "Jump up")** ["The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation"](http://web.archive.org/web/20250405185132/https://ai.meta.com/blog/llama-4-multimodal-intelligence/). _ai.meta.com_. Archived from [the original](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) on 2025-04-05. Retrieved 2025-04-05.
  133. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-139 "Jump up")** ["Introducing OpenAI o3 and o4-mini"](https://openai.com/index/introducing-o3-and-o4-mini/). _openai.com_. Retrieved 8 August 2025.
  134. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-140 "Jump up")** Team, Qwen (2025-04-29). ["Qwen3: Think Deeper, Act Faster"](https://qwenlm.github.io/blog/qwen3/). _Qwen_. Retrieved 2025-04-29.
  135. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-141 "Jump up")** ["Introducing Claude 4"](https://www.anthropic.com/news/claude-4). _www.anthropic.com_. Retrieved 8 August 2025.
  136. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-142 "Jump up")** ["zai-org/GLM-4.5 ¬∑ Hugging Face"](https://huggingface.co/zai-org/GLM-4.5). _huggingface.co_. 2025-08-04. Retrieved 2025-08-06.
  137. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-143 "Jump up")** ["GLM-4.5: Reasoning, Coding, and Agentic Abililties"](https://z.ai/blog/glm-4.5). _z.ai_. Retrieved 2025-08-06.
  138. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-144 "Jump up")** Whitwam, Ryan (5 August 2025). ["OpenAI announces two "gpt-oss" open AI models, and you can download them today"](https://arstechnica.com/ai/2025/08/openai-releases-its-first-open-source-models-since-2019/). _[Ars Technica](https://en.wikipedia.org/wiki/Ars_Technica "Ars Technica")_. Retrieved 6 August 2025.
  139. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-145 "Jump up")** ["Claude Opus 4.1"](https://www.anthropic.com/news/claude-opus-4-1). _www.anthropic.com_. Retrieved 8 August 2025.
  140. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-146 "Jump up")** ["Introducing GPT-5"](https://openai.com/index/introducing-gpt-5/). _openai.com_. 7 August 2025. Retrieved 8 August 2025.
  141. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-147 "Jump up")** ["OpenAI Platform: GPT-5 Model Documentation"](https://platform.openai.com/docs/models/gpt-5). _openai.com_. Retrieved 18 August 2025.
  142. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-148 "Jump up")** ["deepseek-ai/DeepSeek-V3.1 ¬∑ Hugging Face"](https://huggingface.co/deepseek-ai/DeepSeek-V3.1). _huggingface.co_. 2025-08-21. Retrieved 2025-08-25.
  143. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-149 "Jump up")** ["DeepSeek-V3.1 Release | DeepSeek API Docs"](https://api-docs.deepseek.com/news/news250821). _api-docs.deepseek.com_. Retrieved 2025-08-25.
  144. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-150 "Jump up")** ["Apertus: Ein vollst√§ndig offenes, transparentes und mehrsprachiges Sprachmodell"](https://ethz.ch/de/news-und-veranstaltungen/eth-news/news/2025/09/medienmitteilung-apertus-ein-vollstaendig-offenes-transparentes-und-mehrsprachiges-sprachmodell.html) (in German). Z√ºrich: ETH Z√ºrich. 2025-09-02. Retrieved 2025-11-07.
  145. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-151 "Jump up")** Kirchner, Malte (2025-09-02). ["Apertus: Schweiz stellt erstes offenes und mehrsprachiges KI-Modell vor"](https://www.heise.de/news/Apertus-Schweiz-stellt-erstes-offenes-und-mehrsprachiges-KI-Modell-vor-10629412.html). _heise online_ (in German). Retrieved 2025-11-07.
  146. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-152 "Jump up")** ["Introducing Claude Sonnet 4.5"](https://www.anthropic.com/news/claude-sonnet-4-5). _www.anthropic.com_. Retrieved 29 September 2025.
  147. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-153 "Jump up")** ["Introducing DeepSeek-V3.2-Exp | DeepSeek API Docs"](https://api-docs.deepseek.com/news/news250929). _api-docs.deepseek.com_. Retrieved 2025-10-01.
  148. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-154 "Jump up")** ["deepseek-ai/DeepSeek-V3.2-Exp ¬∑ Hugging Face"](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp). _huggingface.co_. 2025-09-29. Retrieved 2025-10-01.
  149. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-155 "Jump up")** ["DeepSeek-V3.2-Exp/DeepSeek_V3_2.pdf at main ¬∑ deepseek-ai/DeepSeek-V3.2-Exp"](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf) (PDF). _GitHub_. Retrieved 2025-10-01.
  150. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-156 "Jump up")** ["GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities"](https://z.ai/blog/glm-4.6). _z.ai_. Retrieved 2025-10-01.
  151. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-157 "Jump up")** ["zai-org/GLM-4.6 ¬∑ Hugging Face"](https://huggingface.co/zai-org/GLM-4.6). _huggingface.co_. 2025-09-30. Retrieved 2025-10-01.
  152. **[^](https://en.wikipedia.org/wiki/List_of_large_language_models#cite_ref-158 "Jump up")** ["GLM-4.6"](https://modelscope.cn/models/ZhipuAI/GLM-4.6). _modelscope.cn_. Retrieved 2025-10-01.


hide
  * [v](https://en.wikipedia.org/wiki/Template:Natural_language_processing "Template:Natural language processing")
  * [t](https://en.wikipedia.org/wiki/Template_talk:Natural_language_processing "Template talk:Natural language processing")
  * [e](https://en.wikipedia.org/wiki/Special:EditPage/Template:Natural_language_processing "Special:EditPage/Template:Natural language processing")

[Natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing "Natural language processing")  
---  
General terms | 
  * [AI-complete](https://en.wikipedia.org/wiki/AI-complete "AI-complete")
  * [Bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model "Bag-of-words model")
  * [_n_ -gram](https://en.wikipedia.org/wiki/N-gram "N-gram")
    * [Bigram](https://en.wikipedia.org/wiki/Bigram "Bigram")
    * [Trigram](https://en.wikipedia.org/wiki/Trigram "Trigram")
  * [Computational linguistics](https://en.wikipedia.org/wiki/Computational_linguistics "Computational linguistics")
  * [Natural language understanding](https://en.wikipedia.org/wiki/Natural_language_understanding "Natural language understanding")
  * [Stop words](https://en.wikipedia.org/wiki/Stop_word "Stop word")
  * [Text processing](https://en.wikipedia.org/wiki/Text_processing "Text processing")

  
[Text analysis](https://en.wikipedia.org/wiki/Text_mining "Text mining") | 
  * [Argument mining](https://en.wikipedia.org/wiki/Argument_mining "Argument mining")
  * [Collocation extraction](https://en.wikipedia.org/wiki/Collocation_extraction "Collocation extraction")
  * [Concept mining](https://en.wikipedia.org/wiki/Concept_mining "Concept mining")
  * [Coreference resolution](https://en.wikipedia.org/wiki/Coreference#Coreference_resolution "Coreference")
  * [Deep linguistic processing](https://en.wikipedia.org/wiki/Deep_linguistic_processing "Deep linguistic processing")
  * [Distant reading](https://en.wikipedia.org/wiki/Distant_reading "Distant reading")
  * [Information extraction](https://en.wikipedia.org/wiki/Information_extraction "Information extraction")
  * [Named-entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition "Named-entity recognition")
  * [Ontology learning](https://en.wikipedia.org/wiki/Ontology_learning "Ontology learning")
  * [Parsing](https://en.wikipedia.org/wiki/Parsing "Parsing")
    * [semantic](https://en.wikipedia.org/wiki/Semantic_parsing "Semantic parsing")
    * [syntactic](https://en.wikipedia.org/wiki/Syntactic_parsing_\(computational_linguistics\) "Syntactic parsing \(computational linguistics\)")
  * [Part-of-speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging "Part-of-speech tagging")
  * [Semantic analysis](https://en.wikipedia.org/wiki/Semantic_analysis_\(machine_learning\) "Semantic analysis \(machine learning\)")
  * [Semantic role labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling "Semantic role labeling")
  * [Semantic decomposition](https://en.wikipedia.org/wiki/Semantic_decomposition_\(natural_language_processing\) "Semantic decomposition \(natural language processing\)")
  * [Semantic similarity](https://en.wikipedia.org/wiki/Semantic_similarity "Semantic similarity")
  * [Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis "Sentiment analysis")


  * [Terminology extraction](https://en.wikipedia.org/wiki/Terminology_extraction "Terminology extraction")
  * [Text mining](https://en.wikipedia.org/wiki/Text_mining "Text mining")
  * [Textual entailment](https://en.wikipedia.org/wiki/Textual_entailment "Textual entailment")
  * [Truecasing](https://en.wikipedia.org/wiki/Truecasing "Truecasing")
  * [Word-sense disambiguation](https://en.wikipedia.org/wiki/Word-sense_disambiguation "Word-sense disambiguation")
  * [Word-sense induction](https://en.wikipedia.org/wiki/Word-sense_induction "Word-sense induction")

| [Text segmentation](https://en.wikipedia.org/wiki/Text_segmentation "Text segmentation") | 
  * [Compound-term processing](https://en.wikipedia.org/wiki/Compound-term_processing "Compound-term processing")
  * [Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation "Lemmatisation")
  * [Lexical analysis](https://en.wikipedia.org/wiki/Lexical_analysis "Lexical analysis")
  * [Text chunking](https://en.wikipedia.org/wiki/Shallow_parsing "Shallow parsing")
  * [Stemming](https://en.wikipedia.org/wiki/Stemming "Stemming")
  * [Sentence segmentation](https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation "Sentence boundary disambiguation")
  * [Word segmentation](https://en.wikipedia.org/wiki/Word#Word_boundaries "Word")

  
---|---  
[Automatic summarization](https://en.wikipedia.org/wiki/Automatic_summarization "Automatic summarization") | 
  * [Multi-document summarization](https://en.wikipedia.org/wiki/Multi-document_summarization "Multi-document summarization")
  * [Sentence extraction](https://en.wikipedia.org/wiki/Sentence_extraction "Sentence extraction")
  * [Text simplification](https://en.wikipedia.org/wiki/Text_simplification "Text simplification")

  
[Machine translation](https://en.wikipedia.org/wiki/Machine_translation "Machine translation") | 
  * [Computer-assisted](https://en.wikipedia.org/wiki/Computer-assisted_translation "Computer-assisted translation")
  * [Example-based](https://en.wikipedia.org/wiki/Example-based_machine_translation "Example-based machine translation")
  * [Rule-based](https://en.wikipedia.org/wiki/Rule-based_machine_translation "Rule-based machine translation")
  * [Statistical](https://en.wikipedia.org/wiki/Statistical_machine_translation "Statistical machine translation")
  * [Transfer-based](https://en.wikipedia.org/wiki/Transfer-based_machine_translation "Transfer-based machine translation")
  * [Neural](https://en.wikipedia.org/wiki/Neural_machine_translation "Neural machine translation")

  
[Distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics "Distributional semantics") models | 
  * [BERT](https://en.wikipedia.org/wiki/BERT_\(language_model\) "BERT \(language model\)")
  * [Document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix "Document-term matrix")
  * [Explicit semantic analysis](https://en.wikipedia.org/wiki/Explicit_semantic_analysis "Explicit semantic analysis")
  * [fastText](https://en.wikipedia.org/wiki/FastText "FastText")
  * [GloVe](https://en.wikipedia.org/wiki/GloVe "GloVe")
  * [Language model](https://en.wikipedia.org/wiki/Language_model "Language model")
    * [large](https://en.wikipedia.org/wiki/Large_language_model "Large language model")
    * [small](https://en.wikipedia.org/wiki/Small_language_model "Small language model")
  * [Latent semantic analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis "Latent semantic analysis")
  * [Long short-term memory](https://en.wikipedia.org/wiki/Long_short-term_memory "Long short-term memory")
  * [Seq2seq](https://en.wikipedia.org/wiki/Seq2seq "Seq2seq")
  * [Transformer](https://en.wikipedia.org/wiki/Transformer_\(deep_learning_architecture\) "Transformer \(deep learning architecture\)")
  * [Word embedding](https://en.wikipedia.org/wiki/Word_embedding "Word embedding")
  * [Word2vec](https://en.wikipedia.org/wiki/Word2vec "Word2vec")

  
[Language resources](https://en.wikipedia.org/wiki/Language_resource "Language resource"),  
datasets and corpora |  | Types and  
standards | 
  * [Corpus linguistics](https://en.wikipedia.org/wiki/Corpus_linguistics "Corpus linguistics")
  * [Lexical resource](https://en.wikipedia.org/wiki/Lexical_resource "Lexical resource")
  * [Linguistic Linked Open Data](https://en.wikipedia.org/wiki/Linguistic_Linked_Open_Data "Linguistic Linked Open Data")
  * [Machine-readable dictionary](https://en.wikipedia.org/wiki/Machine-readable_dictionary "Machine-readable dictionary")
  * [Parallel text](https://en.wikipedia.org/wiki/Parallel_text "Parallel text")
  * [PropBank](https://en.wikipedia.org/wiki/PropBank "PropBank")
  * [Semantic network](https://en.wikipedia.org/wiki/Semantic_network "Semantic network")
  * [Simple Knowledge Organization System](https://en.wikipedia.org/wiki/Simple_Knowledge_Organization_System "Simple Knowledge Organization System")
  * [Speech corpus](https://en.wikipedia.org/wiki/Speech_corpus "Speech corpus")
  * [Text corpus](https://en.wikipedia.org/wiki/Text_corpus "Text corpus")
  * [Thesaurus (information retrieval)](https://en.wikipedia.org/wiki/Thesaurus_\(information_retrieval\) "Thesaurus \(information retrieval\)")
  * [Treebank](https://en.wikipedia.org/wiki/Treebank "Treebank")
  * [Universal Dependencies](https://en.wikipedia.org/wiki/Universal_Dependencies "Universal Dependencies")

  
---|---  
Data | 
  * [BabelNet](https://en.wikipedia.org/wiki/BabelNet "BabelNet")
  * [Bank of English](https://en.wikipedia.org/wiki/Bank_of_English "Bank of English")
  * [DBpedia](https://en.wikipedia.org/wiki/DBpedia "DBpedia")
  * [FrameNet](https://en.wikipedia.org/wiki/FrameNet "FrameNet")
  * [Google Ngram Viewer](https://en.wikipedia.org/wiki/Google_Ngram_Viewer "Google Ngram Viewer")
  * [UBY](https://en.wikipedia.org/wiki/UBY "UBY")
  * [WordNet](https://en.wikipedia.org/wiki/WordNet "WordNet")
  * [Wikidata](https://en.wikipedia.org/wiki/Wikidata "Wikidata")

  
[Automatic identification  
and data capture](https://en.wikipedia.org/wiki/Automatic_identification_and_data_capture "Automatic identification and data capture") | 
  * [Speech recognition](https://en.wikipedia.org/wiki/Speech_recognition "Speech recognition")
  * [Speech segmentation](https://en.wikipedia.org/wiki/Speech_segmentation "Speech segmentation")
  * [Speech synthesis](https://en.wikipedia.org/wiki/Speech_synthesis "Speech synthesis")
  * [Natural language generation](https://en.wikipedia.org/wiki/Natural_language_generation "Natural language generation")
  * [Optical character recognition](https://en.wikipedia.org/wiki/Optical_character_recognition "Optical character recognition")

  
[Topic model](https://en.wikipedia.org/wiki/Topic_model "Topic model") | 
  * [Document classification](https://en.wikipedia.org/wiki/Document_classification "Document classification")
  * [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation "Latent Dirichlet allocation")
  * [Pachinko allocation](https://en.wikipedia.org/wiki/Pachinko_allocation "Pachinko allocation")

  
[Computer-assisted  
reviewing](https://en.wikipedia.org/wiki/Computer-assisted_reviewing "Computer-assisted reviewing") | 
  * [Automated essay scoring](https://en.wikipedia.org/wiki/Automated_essay_scoring "Automated essay scoring")
  * [Concordancer](https://en.wikipedia.org/wiki/Concordancer "Concordancer")
  * [Grammar checker](https://en.wikipedia.org/wiki/Grammar_checker "Grammar checker")
  * [Predictive text](https://en.wikipedia.org/wiki/Predictive_text "Predictive text")
  * [Pronunciation assessment](https://en.wikipedia.org/wiki/Pronunciation_assessment "Pronunciation assessment")
  * [Spell checker](https://en.wikipedia.org/wiki/Spell_checker "Spell checker")

  
[Natural language  
user interface](https://en.wikipedia.org/wiki/Natural-language_user_interface "Natural-language user interface") | 
  * [Chatbot](https://en.wikipedia.org/wiki/Chatbot "Chatbot")
  * [Interactive fiction](https://en.wikipedia.org/wiki/Interactive_fiction "Interactive fiction")
  * [Question answering](https://en.wikipedia.org/wiki/Question_answering "Question answering")
  * [Virtual assistant](https://en.wikipedia.org/wiki/Virtual_assistant "Virtual assistant")
  * [Voice user interface](https://en.wikipedia.org/wiki/Voice_user_interface "Voice user interface")

  
Related | 
  * [Formal semantics](https://en.wikipedia.org/wiki/Formal_semantics_\(natural_language\) "Formal semantics \(natural language\)")
  * [Hallucination](https://en.wikipedia.org/wiki/Hallucination_\(artificial_intelligence\) "Hallucination \(artificial intelligence\)")
  * [Natural Language Toolkit](https://en.wikipedia.org/wiki/Natural_Language_Toolkit "Natural Language Toolkit")
  * [spaCy](https://en.wikipedia.org/wiki/SpaCy "SpaCy")

  
[Portal](https://en.wikipedia.org/wiki/Wikipedia:Contents/Portals "Wikipedia:Contents/Portals"):
  * [![icon](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Globe_of_letters.svg/20px-Globe_of_letters.svg.png)](https://en.wikipedia.org/wiki/File:Globe_of_letters.svg) [Language](https://en.wikipedia.org/wiki/Portal:Language "Portal:Language")


Retrieved from "[https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&oldid=1325091032](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&oldid=1325091032)"
[Categories](https://en.wikipedia.org/wiki/Help:Category "Help:Category"): 
  * [Software comparisons](https://en.wikipedia.org/wiki/Category:Software_comparisons "Category:Software comparisons")
  * [Large language models](https://en.wikipedia.org/wiki/Category:Large_language_models "Category:Large language models")


Hidden categories: 
  * [Pages using the EasyTimeline extension](https://en.wikipedia.org/wiki/Category:Pages_using_the_EasyTimeline_extension "Category:Pages using the EasyTimeline extension")
  * [Webarchive template wayback links](https://en.wikipedia.org/wiki/Category:Webarchive_template_wayback_links "Category:Webarchive template wayback links")
  * [CS1: long volume value](https://en.wikipedia.org/wiki/Category:CS1:_long_volume_value "Category:CS1: long volume value")
  * [CS1 maint: url-status](https://en.wikipedia.org/wiki/Category:CS1_maint:_url-status "Category:CS1 maint: url-status")
  * [CS1 German-language sources (de)](https://en.wikipedia.org/wiki/Category:CS1_German-language_sources_\(de\) "Category:CS1 German-language sources \(de\)")
  * [Articles with short description](https://en.wikipedia.org/wiki/Category:Articles_with_short_description "Category:Articles with short description")
  * [Short description is different from Wikidata](https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata "Category:Short description is different from Wikidata")
  * [Dynamic lists](https://en.wikipedia.org/wiki/Category:Dynamic_lists "Category:Dynamic lists")


  * This page was last edited on 1 December 2025, at 03:30 (UTC).
  * Text is available under the [Creative Commons Attribution-ShareAlike 4.0 License](https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License "Wikipedia:Text of the Creative Commons Attribution-ShareAlike 4.0 International License"); additional terms may apply. By using this site, you agree to the [Terms of Use](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use "foundation:Special:MyLanguage/Policy:Terms of Use") and [Privacy Policy](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy "foundation:Special:MyLanguage/Policy:Privacy policy"). Wikipedia¬Æ is a registered trademark of the [Wikimedia Foundation, Inc.](https://wikimediafoundation.org/), a non-profit organization.


  * [Privacy policy](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy)
  * [About Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:About)
  * [Disclaimers](https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer)
  * [Contact Wikipedia](https://en.wikipedia.org/wiki/Wikipedia:Contact_us)
  * [Code of Conduct](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct)
  * [Developers](https://developer.wikimedia.org)
  * [Statistics](https://stats.wikimedia.org/#/en.wikipedia.org)
  * [Cookie statement](https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement)
  * [Mobile view](https://en.wikipedia.org/w/index.php?title=List_of_large_language_models&mobileaction=toggle_view_mobile)
  * [Edit preview settings](https://en.wikipedia.org/wiki/List_of_large_language_models)


  * [![Wikimedia Foundation](https://en.wikipedia.org/static/images/footer/wikimedia.svg)](https://www.wikimedia.org/)
  * [![Powered by MediaWiki](https://en.wikipedia.org/w/resources/assets/mediawiki_compact.svg)](https://www.mediawiki.org/)


Search
Search
Toggle the table of contents
List of large language models
[](https://en.wikipedia.org/wiki/List_of_large_language_models) [](https://en.wikipedia.org/wiki/List_of_large_language_models) [](https://en.wikipedia.org/wiki/List_of_large_language_models) [](https://en.wikipedia.org/wiki/List_of_large_language_models) [](https://en.wikipedia.org/wiki/List_of_large_language_models) [](https://en.wikipedia.org/wiki/List_of_large_language_models) [](https://en.wikipedia.org/wiki/List_of_large_language_models)
4 languages [Add topic ](https://en.wikipedia.org/wiki/List_of_large_language_models)
[](https://en.wikipedia.org/wiki/List_of_large_language_models?action=edit)
  *[v]: View this template
  *[t]: Discuss this template
  *[e]: Edit this template
